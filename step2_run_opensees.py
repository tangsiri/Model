"""
File name      : step2_run_opensees.py
Author         : pc22
Created on     : Sat Dec 27 13:11:43 2025
Last modified  : Wed Jan 08 2026
------------------------------------------------------------
Purpose:
    Parallelized, automated, and resumable Time History Analysis (THA)
    in OpenSeesPy for generating structural response datasets under
    multiple column heights and multiple earthquake records, in both
    training and prediction workflows, for both linear and nonlinear
    structural models, with an online global ETA for the entire batch.

    New in this version:
      - Multi-core / parallel execution across records using a
        process-based worker pool (ProcessPoolExecutor) to utilize
        multiple CPU cores and reduce total batch runtime.
------------------------------------------------------------
Description:
    This script is the main batch runner for dynamic time history
    analysis using OpenSeesPy, with built-in support for safe
    interruption/resume and progress time estimation.

    It supports two execution modes:
      1) train  : runs THA for a set of ground motion records intended
                 to produce training datasets.
      2) predict: runs THA for a separate set of records used for model
                 evaluation / comparison.

    It also supports two structural model types:
      - linear   : executes model_linear.py
      - nonlinear: executes model_nonlinear.py

    For each selected column height (H) and each earthquake record
    (.AT2), the script:
      - sets the height via environment variable (H_COL)
      - creates a dedicated output folder: H*/<record_name>/
      - builds the OpenSees model (linear/nonlinear)
      - defines damping and recorders
      - reads/transforms the ground motion record
      - applies the excitation using UniformExcitation
      - runs dynamic analysis via doDynamicAnalysis()
      - saves recorder outputs to the record-specific folder

    Parallel execution (multi-core):
      - Instead of running records sequentially, remaining (not-DONE)
        record-runs are dispatched to multiple independent processes
        using ProcessPoolExecutor.
      - Each process runs one (H, record) job in isolation (separate
        OpenSees state), improving stability and CPU utilization.

    Resume mechanism (robust long batch runs):
      - After successful execution of each record, a completion marker
        file (__DONE__.txt) is written to the corresponding record output
        folder.
      - Upon restart (after power loss, crash, or manual stop), records
        that already contain this marker are automatically skipped.
      - Records that were interrupted before completion are safely re-run,
        with a per-record cleanup to prevent mixing partial outputs with
        new results.

    Global ETA (entire project across all selected heights):
      - Before starting the run, the script scans all selected heights
        and counts how many record-runs remain (i.e., do not have
        __DONE__.txt).
      - During execution, it measures per-record wall time and updates an
        Exponential Moving Average (EMA).
      - After each successful record-run, it prints a concise global status:
           "üåç ⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá: OK/Total | ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá | ETA(ALL) ‚Üí finish time"
        representing the estimated time remaining and the approximate
        completion timestamp for the entire batch (all heights).
------------------------------------------------------------
Inputs:
    - User inputs at runtime:
        * RUN_MODE: train (0) / predict (1)
        * Model type: linear (1) / nonlinear (0)
        * Column height(s): one or multiple values (e.g., 3 4 5)
        * Parallel workers: number of processes (optional; suggested default)

    - Earthquake records:
        Output/1_IDA_Records_train/*.AT2   (for train mode)
        Output/1_IDA_Records_predict/*.AT2 (for predict mode)

    - Dependent scripts (called via exec/import):
        * model_linear.py / model_nonlinear.py
        * defineDamping.py
        * defineRecorders.py
        * ReadRecord.py        (ReadRecord function)
        * doDynamicAnalysis.py (doDynamicAnalysis function)

    - Key environment variables set per job:
        * RUN_MODE  : 'train' or 'predict'
        * IS_LINEAR : '1' or '0'
        * H_COL     : column height value for model scripts
------------------------------------------------------------
Outputs:
    - THA results saved per height and per record:
        Output/2_THA_train_linear/H*/<record_name>/
        Output/2_THA_train_nonlinear/H*/<record_name>/
        Output/2_THA_predict_linear/H*/<record_name>/
        Output/2_THA_predict_nonlinear/H*/<record_name>/

      (Exact files depend on defineRecorders.py, typically time-history
       outputs such as displacement, acceleration, internal forces, etc.)

    - Completion marker per successfully executed record:
        __DONE__.txt  (stored inside each record folder)

    - Failed record logs per height:
        failed_records_<mode>_<linear/nonlinear>_<Htag>.txt

    - Error diagnostics for failed records (optional):
        __ERROR__.txt inside the corresponding record folder

    - Console progress output:
        * Global ETA line after each successful run
        * End-of-run summary (OK/FAIL/SKIP + elapsed time)
------------------------------------------------------------
Changes since previous version:
    - Added parallel multi-process execution across (height, record)
      jobs using ProcessPoolExecutor to increase CPU utilization.
    - Preserved resume-safe execution using per-record completion markers
      (__DONE__.txt) and safe cleanup of partial outputs.
    - Kept global ETA across all selected heights using EMA of observed
      per-record wall time.
    - Added optional user input to select the number of parallel workers.
------------------------------------------------------------
Impact of changes:
    - Significantly reduces total runtime on multi-core CPUs by processing
      multiple records concurrently.
    - Maintains reliability for long batch runs (resume after interruption)
      without losing completed results.
    - Preserves reproducible folder hierarchy and traceability using DONE
      markers and per-record error logs.
------------------------------------------------------------
Status:
    Stable (Parallel batch processing / Resumable dataset generation with global ETA)
------------------------------------------------------------
Notes:
    - Parallel execution is process-based (not threads) to keep OpenSees
      state isolated per job and avoid cross-interference.
    - By default, output folders are preserved to support resume
      functionality. A clean re-run per height can be enforced via the
      runtime prompt (clean=1), which deletes the height folder and
      recomputes that height.
    - Scaling factor is set as: scaleFac = 10 * 9.81.
    - Ground motions are applied using UniformExcitation with a Path
      timeSeries generated from transformed record data.
    - Recorder definitions fully control which response quantities are saved.
"""




# # -*- coding: utf-8 -*-
# import sys, io

# # ‚úÖ ÿ¨ŸÑŸà⁄Ø€åÿ±€å ÿßÿ≤ ÿÆÿ∑ÿß€å UnicodeEncodeError ÿØÿ± ŸÖÿ≠€åÿ∑‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ (Spyderÿå CMDÿå Run-Codes)
# if hasattr(sys.stdout, "buffer"):
#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')

# import os
# import shutil
# from openseespy.opensees import *
# from ReadRecord import ReadRecord
# from analyzeAndAnimate import analyzeAndAnimateTHA
# import vfo.vfo as vfo
# import opsvis as opsv
# from doDynamicAnalysis import doDynamicAnalysis

# # ------------------------------------------------------------
# # üîß ÿßŸÜÿ™ÿÆÿßÿ® ÿ≠ÿßŸÑÿ™ train / predict
# # ------------------------------------------------------------
# choice = input("ÿ®ÿ±ÿß€å train ÿπÿØÿØ 0 Ÿà ÿ®ÿ±ÿß€å predict ÿπÿØÿØ 1 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
# if choice == "0":
#     RUN_MODE = "train"
# elif choice == "1":
#     RUN_MODE = "predict"
# else:
#     print("‚ùå ŸÅŸÇÿ∑ ÿπÿØÿØ 0 €åÿß 1 ŸÖÿ¨ÿßÿ≤ ÿßÿ≥ÿ™.")
#     sys.exit(1)

# # ------------------------------------------------------------
# # üîß ÿßŸÜÿ™ÿÆÿßÿ® ŸÖÿØŸÑ ÿÆÿ∑€å / ÿ∫€åÿ±ÿÆÿ∑€å
# # ------------------------------------------------------------
# lin_choice = input("ŸÖÿØŸÑ ÿÆÿ∑€å ÿ®ÿßÿ¥ÿØ €åÿß ÿ∫€åÿ±ÿÆÿ∑€åÿü ÿ®ÿ±ÿß€å ŸÖÿØŸÑ ÿÆÿ∑€å ÿπÿØÿØ 1 Ÿà ÿ®ÿ±ÿß€å ÿ∫€åÿ±ÿÆÿ∑€å ÿπÿØÿØ 0 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
# IS_LINEAR = (lin_choice == "1")

# print(f"üìå ÿ≠ÿßŸÑÿ™ ÿßÿ¨ÿ±ÿß: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")

# # ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿ®Ÿá ŸÖÿØŸÑ ÿÆÿ®ÿ± ŸÖ€å‚ÄåÿØŸá€åŸÖ (ÿØÿ± ÿµŸàÿ±ÿ™ ŸÜ€åÿßÿ≤)
# os.environ["RUN_MODE"] = RUN_MODE
# os.environ["IS_LINEAR"] = "1" if IS_LINEAR else "0"

# # ------------------------------------------------------------
# # üìè ÿØÿ±€åÿßŸÅÿ™ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ (€å⁄© €åÿß ⁄ÜŸÜÿØ ŸÖŸÇÿØÿßÿ±)
# # ------------------------------------------------------------
# heights_raw = input("ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ (ŸÖÿ´ŸÑÿßŸã: 3 €åÿß 3 4 5): ").strip()

# if not heights_raw:
#     print("‚ö†Ô∏è Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØÿõ ŸÖŸÇÿØÿßÿ± Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂ 3 ŸÖÿ™ÿ± ÿØÿ± ŸÜÿ∏ÿ± ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
#     heights = [3.0]
# else:
#     heights = []
#     for token in heights_raw.replace(',', ' ').split():
#         try:
#             h_val = float(token)
#             heights.append(h_val)
#         except ValueError:
#             print(f"‚ö†Ô∏è ŸÖŸÇÿØÿßÿ± ¬´{token}¬ª ÿπÿØÿØ ŸÖÿπÿ™ÿ®ÿ±€å ŸÜ€åÿ≥ÿ™ Ÿà ŸÜÿßÿØ€åÿØŸá ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")

#     if not heights:
#         print("‚ùå Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿ™ÿ®ÿ±€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØ. ÿßÿ¨ÿ±ÿß€å ÿ®ÿ±ŸÜÿßŸÖŸá ŸÖÿ™ŸàŸÇŸÅ ÿ¥ÿØ.")
#         sys.exit(1)

# print("üìè ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß€å ÿßŸÜÿ™ÿÆÿßÿ®‚Äåÿ¥ÿØŸá ÿ®ÿ±ÿß€å ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß:", ", ".join(str(h) for h in heights))

# # ------------------------------------------------------------
# # ‚öôÔ∏è ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿßŸàŸÑ€åŸá ÿ™ÿ≠ŸÑ€åŸÑ
# # ------------------------------------------------------------
# scaleFac = 10 * 9.81
# TFree = 0
# dataDirRoot = '../../'

# # ---------------------- ŸæŸàÿ¥ŸáŸî Ÿàÿ±ŸàÿØ€å GM ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÑÿ™ ----------------------
# if RUN_MODE == 'train':
#     GMFolder = os.path.join(dataDirRoot, 'Output', '1_IDA_Records_train')
# else:
#     GMFolder = os.path.join(dataDirRoot, 'Output', '1_IDA_Records_predict')

# # ---------------------- ŸæŸàÿ¥ŸáŸî ÿÆÿ±Ÿàÿ¨€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÑÿ™ Ÿà ÿÆÿ∑€å/ÿ∫€åÿ±ÿÆÿ∑€å ----------------------
# if RUN_MODE == 'train':
#     if IS_LINEAR:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_train_linear')
#     else:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_train_nonlinear')
# else:  # predict
#     if IS_LINEAR:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_predict_linear')
#     else:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_predict_nonlinear')

# showAnimationDeform = 0

# print(f"üì• ŸæŸàÿ¥Ÿá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿàÿ±ŸàÿØ€å: {GMFolder}")
# print(f"üìÇ ŸæŸàÿ¥Ÿá Ÿæÿß€åŸá‚Äå€å ÿÆÿ±Ÿàÿ¨€å THA: {dataDirBase}\n")

# # ------------------------------------------------------------
# # ‚ú≥Ô∏è Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å .AT2
# # ------------------------------------------------------------
# if not os.path.isdir(GMFolder):
#     raise FileNotFoundError(f"‚ùå ŸæŸàÿ¥Ÿá Ÿàÿ±ŸàÿØ€å ÿ±⁄©Ÿàÿ±ÿØŸáÿß Ÿæ€åÿØÿß ŸÜÿ¥ÿØ: {GMFolder}")

# all_records = [f for f in os.listdir(GMFolder) if f.endswith('.AT2')]
# print(f"üîç ÿ™ÿπÿØÿßÿØ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿæ€åÿØÿß ÿ¥ÿØŸá: {len(all_records)}")

# # ------------------------------------------------------------
# # üöÄ ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπ Ÿà Ÿáÿ± ÿ±⁄©Ÿàÿ±ÿØ
# # ------------------------------------------------------------
# for h_val in heights:
    # ÿ™ŸÜÿ∏€åŸÖ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ ÿ®ÿ±ÿß€å ŸÖÿØŸÑ (ŸÖÿ™ÿ∫€åÿ± ŸÖÿ≠€åÿ∑€å ÿ®ÿ±ÿß€å model_linear.py Ÿà model_nonlinear.py)
#     os.environ["H_COL"] = str(h_val)

    # ÿ≥ÿßÿÆÿ™ ŸÜÿßŸÖ ŸæŸàÿ¥Ÿá ÿ®ÿ±ÿß€å ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     if float(h_val).is_integer():
#         h_tag = f"H{int(h_val)}"          # ŸÖÿ´ÿßŸÑ: H3
#     else:
#         h_tag = "H" + str(h_val).replace('.', 'p')   # ŸÖÿ´ÿßŸÑ: H3p5

    # ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     dataDirOut = os.path.join(dataDirBase, h_tag)

    # ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     failed_records = []

#     print(f"üèóÔ∏è ÿ¥ÿ±Ÿàÿπ ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ {h_val} ŸÖÿ™ÿ± ÿØÿ± ŸæŸàÿ¥Ÿá: {dataDirOut}")

    # üßπ Ÿæÿß⁄©ÿ≥ÿßÿ≤€å ŸæŸàÿ¥Ÿá ÿÆÿ±Ÿàÿ¨€å ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     if os.path.exists(dataDirOut):
#         print(f"üßπ ÿ≠ÿ∞ŸÅ ŸÖÿ≠ÿ™Ÿàÿß€å ŸÇÿ®ŸÑ€å ŸæŸàÿ¥Ÿá: {dataDirOut}")
#         shutil.rmtree(dataDirOut)
#     os.makedirs(dataDirOut, exist_ok=True)

    # ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å Ÿáÿ± ÿ±⁄©Ÿàÿ±ÿØ
#     for i, rec_file in enumerate(all_records, start=1):
#         try:
#             record_name = os.path.splitext(rec_file)[0]  # ŸÖÿ´ŸÑÿßŸã: RSN4_..._x1_0
#             inFileName = os.path.join(GMFolder, rec_file)
#             GMPath = os.path.join(GMFolder, record_name + ".txt")

            # ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿ±⁄©Ÿàÿ±ÿØ (ÿ≤€åÿ±ŸæŸàÿ¥Ÿá‚Äåÿß€å ÿØÿßÿÆŸÑ ŸæŸàÿ¥Ÿá ÿßÿ±ÿ™ŸÅÿßÿπ)
#             dataDir_rec = os.path.join(dataDirOut, record_name)
#             os.makedirs(dataDir_rec, exist_ok=True)

#             # ‚ö° RecorderŸáÿß ÿßÿ≤ ÿß€åŸÜ ŸÖÿ™ÿ∫€åÿ± ÿ®ÿ±ÿß€å ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ
#             dataDir = dataDir_rec

            # üîπ ÿßÿ¨ÿ±ÿß€å ŸÖÿØŸÑ (ÿÆÿ∑€å €åÿß ÿ∫€åÿ±ÿÆÿ∑€å) Ÿà ŸÖ€åÿ±ÿß€å€å
#             if IS_LINEAR:
#                 exec(io.open("model_linear.py", "r", encoding="utf-8").read())
#             else:
#                 exec(open("model_nonlinear.py").read())

#             exec(open("defineDamping.py").read())
#             exec(open("defineRecorders.py").read())

            # üîπ ÿÆŸàÿßŸÜÿØŸÜ ÿ±⁄©Ÿàÿ±ÿØ ÿ≤ŸÑÿ≤ŸÑŸá Ÿà ÿßÿπŸÖÿßŸÑ ÿ®Ÿá ÿ≥ÿ±€å ÿ≤ŸÖÿßŸÜ€å
#             transformed_path = os.path.join(GMFolder, "transformed")
#             os.makedirs(transformed_path, exist_ok=True)

#             dtInput, numPoints = ReadRecord(inFileName, GMPath)
#             seriesTag = 2
#             timeSeries('Path', seriesTag, '-dt', dtInput, '-filePath', GMPath, '-factor', scaleFac)
#             GMDir = 1
#             pattern('UniformExcitation', 2, GMDir, '-accel', seriesTag)

            # üîπ ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿØ€åŸÜÿßŸÖ€å⁄©€å
#             Tmax = numPoints * dtInput + TFree
#             dtAnalysis = dtInput

#             mode_str = "train" if RUN_MODE == "train" else "predict"
#             lin_str = "ÿÆÿ∑€å" if IS_LINEAR else "ÿ∫€åÿ±ÿÆÿ∑€å"
#             print(f"‚öôÔ∏è ÿßÿ±ÿ™ŸÅÿßÿπ H = {h_val} m | ÿßÿ¨ÿ±ÿß€å ÿ±⁄©Ÿàÿ±ÿØ {i}/{len(all_records)}: {record_name}  ({mode_str}, {lin_str})")

#             doDynamicAnalysis(Tmax, dtInput)
#             wipe()

#             print(f"‚úÖ ÿßÿ±ÿ™ŸÅÿßÿπ H = {h_val} m | ÿ±⁄©Ÿàÿ±ÿØ {record_name} ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿßÿ¨ÿ±ÿß ÿ¥ÿØ. ({mode_str}, {lin_str})\n")

#         except Exception as e:
#             print(f'‚ùå ÿßÿ±ÿ™ŸÅÿßÿπ H = {h_val} m | ÿÆÿ∑ÿß ÿØÿ± ÿ±⁄©Ÿàÿ±ÿØ {rec_file}: {e}')
#             failed_records.append(rec_file)

    # ‚úèÔ∏è ÿ∞ÿÆ€åÿ±Ÿá ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ÿ®ÿ±ÿß€å ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     failed_suffix = f"_{RUN_MODE}_{'linear' if IS_LINEAR else 'nonlinear'}_{h_tag}"
#     failed_file_name = f"failed_records{failed_suffix}.txt"

#     with open(failed_file_name, "w", encoding="utf-8") as f:
#         for rec in failed_records:
#             f.write(f"{rec}\n")

#     print(f"üìÑ ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ÿ®ÿ±ÿß€å ÿßÿ±ÿ™ŸÅÿßÿπ {h_val} ŸÖÿ™ÿ± ÿØÿ± ŸÅÿß€åŸÑ {failed_file_name} ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ.")

# print(f"üèÅ ÿßÿ¨ÿ±ÿß€å ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß ÿ®ÿ±ÿß€å ŸáŸÖŸá ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß ÿ™ŸÖÿßŸÖ ÿ¥ÿØ. ÿ≠ÿßŸÑÿ™ ÿßÿ¨ÿ±ÿß: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")


# =============================================================================
# 
# 
# # -*- coding: utf-8 -*-
# import sys, io
# 
# # ‚úÖ ÿ¨ŸÑŸà⁄Ø€åÿ±€å ÿßÿ≤ ÿÆÿ∑ÿß€å UnicodeEncodeError ÿØÿ± ŸÖÿ≠€åÿ∑‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ (Spyderÿå CMDÿå Run-Codes)
# if hasattr(sys.stdout, "buffer"):
#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')
# 
# import os
# import shutil
# 
# # ============================================================
# # ‚úÖ ÿ®ÿÆÿ¥ ÿ¨ÿØ€åÿØ: ÿ≥ÿßÿ≤⁄Øÿßÿ±€å ÿ®ÿß ÿ¨ÿßÿ®Ÿá‚Äåÿ¨ÿß€å€å ŸÅÿß€åŸÑ ÿ®€åŸÜ:
# #   Model\Time History Analysis (THA)  ‚Üî  Model
# # ============================================================
# 
# # ŸÖÿ≥€åÿ± ŸÅŸàŸÑÿØÿ± ŸáŸÖ€åŸÜ ŸÅÿß€åŸÑ (Ÿáÿ±ÿ¨ÿß ÿ®ÿßÿ¥ÿØ)
# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# 
# # ÿß⁄Øÿ± ÿßÿ≥⁄©ÿ±€åŸæÿ™ ÿ±ÿß ÿ®Ÿá Model ŸÖŸÜÿ™ŸÇŸÑ ⁄©ŸÜ€åÿå ŸÅÿß€åŸÑ‚ÄåŸáÿß€å THA ŸÖÿπŸÖŸàŸÑÿßŸã ÿß€åŸÜÿ¨ÿß Ÿáÿ≥ÿ™ŸÜÿØ:
# THA_DIR = os.path.join(BASE_DIR, "Time History Analysis (THA)")
# 
# # ÿ®ÿ±ÿß€å ÿß€åŸÜ⁄©Ÿá importŸáÿß€å ÿ≤€åÿ± (ReadRecord, doDynamicAnalysis, ...) ŸáŸÖ€åÿ¥Ÿá ⁄©ÿßÿ± ⁄©ŸÜŸÜÿØ:
# # - ÿß⁄Øÿ± ŸÅÿß€åŸÑ ÿØÿßÿÆŸÑ THA ÿ®ÿßÿ¥ÿØ: BASE_DIR ŸáŸÖÿßŸÜ THA_DIR ŸàÿßŸÇÿπ€å ÿßÿ≥ÿ™
# # - ÿß⁄Øÿ± ŸÅÿß€åŸÑ ÿØÿßÿÆŸÑ Model ÿ®ÿßÿ¥ÿØ: THA_DIR Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ
# if os.path.isdir(THA_DIR) and THA_DIR not in sys.path:
#     sys.path.insert(0, THA_DIR)
# 
# # ÿÆŸàÿØ BASE_DIR ÿ±ÿß ŸáŸÖ ÿØÿ± sys.path ÿ®⁄Øÿ∞ÿßÿ± (ÿß€åŸÖŸÜ)
# if BASE_DIR not in sys.path:
#     sys.path.insert(0, BASE_DIR)
# 
# def find_project_root(start_dir: str, max_up: int = 6) -> str:
#     """
#     ÿ±€åÿ¥Ÿá Ÿæÿ±Ÿà⁄òŸá ÿ±ÿß Ÿæ€åÿØÿß ŸÖ€å‚Äå⁄©ŸÜÿØ: ÿ¨ÿß€å€å ⁄©Ÿá ŸæŸàÿ¥Ÿá Output Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ.
#     """
#     cur = os.path.abspath(start_dir)
#     for _ in range(max_up):
#         if os.path.isdir(os.path.join(cur, "Output")):
#             return cur
#         parent = os.path.dirname(cur)
#         if parent == cur:
#             break
#         cur = parent
#     # ÿß⁄Øÿ± Ÿæ€åÿØÿß ŸÜÿ¥ÿØÿå ŸáŸÖÿßŸÜ ÿ±ŸÅÿ™ÿßÿ± ŸÇÿ®ŸÑ€å ÿ±ÿß ÿ™ÿß ÿ≠ÿØ ŸÖŸÖ⁄©ŸÜ ÿ≠ŸÅÿ∏ ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ
#     # (ÿßŸÖÿß ÿßÿ≠ÿ™ŸÖÿßŸÑÿßŸã Ÿæÿ±Ÿà⁄òŸá ÿ≥ÿßÿÆÿ™ÿßÿ± ÿØ€å⁄Øÿ±€å ÿØÿßÿ±ÿØ)
#     return os.path.abspath(start_dir)
# 
# PROJECT_ROOT = find_project_root(BASE_DIR)
# 
# def locate_dep(filename: str) -> str:
#     """
#     ŸÅÿß€åŸÑ‚ÄåŸáÿß€å Ÿàÿßÿ®ÿ≥ÿ™Ÿá ÿ±ÿß ÿØÿ± ÿß€åŸÜ ŸÖÿ≥€åÿ±Ÿáÿß Ÿæ€åÿØÿß ŸÖ€å‚Äå⁄©ŸÜÿØ:
#       1) ⁄©ŸÜÿßÿ± ŸáŸÖ€åŸÜ ÿßÿ≥⁄©ÿ±€åŸæÿ™
#       2) ÿØÿßÿÆŸÑ Time History Analysis (THA) (ÿß⁄Øÿ± ÿßÿ≥⁄©ÿ±€åŸæÿ™ ÿØÿßÿÆŸÑ Model ÿ®ÿßÿ¥ÿØ)
#     """
#     c1 = os.path.join(BASE_DIR, filename)
#     if os.path.exists(c1):
#         return c1
#     c2 = os.path.join(THA_DIR, filename)
#     if os.path.exists(c2):
#         return c2
#     # ÿß⁄Øÿ± Ÿæ€åÿØÿß ŸÜÿ¥ÿØÿå ŸáŸÖÿßŸÜ ÿßÿ≥ŸÖ ÿ±ÿß ÿ®ÿ±ŸÖ€å‚Äå⁄Øÿ±ÿØÿßŸÜ€åŸÖ ÿ™ÿß Ÿæ€åÿßŸÖ ÿÆÿ∑ÿß€å ÿ∑ÿ®€åÿπ€å ÿ®ÿØŸáÿØ
#     return filename
# 
# # ============================================================
# # ÿ≠ÿßŸÑÿß importŸáÿß (ÿ®ÿπÿØ ÿßÿ≤ sys.path ÿ™ŸÜÿ∏€åŸÖ ÿ¥ÿØŸá)
# # ============================================================
# from openseespy.opensees import *
# from ReadRecord import ReadRecord
# from analyzeAndAnimate import analyzeAndAnimateTHA
# import vfo.vfo as vfo
# import opsvis as opsv
# from doDynamicAnalysis import doDynamicAnalysis
# 
# # ------------------------------------------------------------
# # üîß ÿßŸÜÿ™ÿÆÿßÿ® ÿ≠ÿßŸÑÿ™ train / predict
# # ------------------------------------------------------------
# choice = input("ÿ®ÿ±ÿß€å train ÿπÿØÿØ 0 Ÿà ÿ®ÿ±ÿß€å predict ÿπÿØÿØ 1 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
# if choice == "0":
#     RUN_MODE = "train"
# elif choice == "1":
#     RUN_MODE = "predict"
# else:
#     print("‚ùå ŸÅŸÇÿ∑ ÿπÿØÿØ 0 €åÿß 1 ŸÖÿ¨ÿßÿ≤ ÿßÿ≥ÿ™.")
#     sys.exit(1)
# 
# # ------------------------------------------------------------
# # üîß ÿßŸÜÿ™ÿÆÿßÿ® ŸÖÿØŸÑ ÿÆÿ∑€å / ÿ∫€åÿ±ÿÆÿ∑€å
# # ------------------------------------------------------------
# lin_choice = input("ŸÖÿØŸÑ ÿÆÿ∑€å ÿ®ÿßÿ¥ÿØ €åÿß ÿ∫€åÿ±ÿÆÿ∑€åÿü ÿ®ÿ±ÿß€å ŸÖÿØŸÑ ÿÆÿ∑€å ÿπÿØÿØ 1 Ÿà ÿ®ÿ±ÿß€å ÿ∫€åÿ±ÿÆÿ∑€å ÿπÿØÿØ 0 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
# IS_LINEAR = (lin_choice == "1")
# 
# print(f"üìå ÿ≠ÿßŸÑÿ™ ÿßÿ¨ÿ±ÿß: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")
# 
# # ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿ®Ÿá ŸÖÿØŸÑ ÿÆÿ®ÿ± ŸÖ€å‚ÄåÿØŸá€åŸÖ (ÿØÿ± ÿµŸàÿ±ÿ™ ŸÜ€åÿßÿ≤)
# os.environ["RUN_MODE"] = RUN_MODE
# os.environ["IS_LINEAR"] = "1" if IS_LINEAR else "0"
# 
# # ------------------------------------------------------------
# # üìè ÿØÿ±€åÿßŸÅÿ™ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ (€å⁄© €åÿß ⁄ÜŸÜÿØ ŸÖŸÇÿØÿßÿ±)
# # ------------------------------------------------------------
# heights_raw = input("ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ (ŸÖÿ´ŸÑÿßŸã: 3 €åÿß 3 4 5): ").strip()
# 
# if not heights_raw:
#     print("‚ö†Ô∏è Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØÿõ ŸÖŸÇÿØÿßÿ± Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂ 3 ŸÖÿ™ÿ± ÿØÿ± ŸÜÿ∏ÿ± ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
#     heights = [3.0]
# else:
#     heights = []
#     for token in heights_raw.replace(',', ' ').split():
#         try:
#             h_val = float(token)
#             heights.append(h_val)
#         except ValueError:
#             print(f"‚ö†Ô∏è ŸÖŸÇÿØÿßÿ± ¬´{token}¬ª ÿπÿØÿØ ŸÖÿπÿ™ÿ®ÿ±€å ŸÜ€åÿ≥ÿ™ Ÿà ŸÜÿßÿØ€åÿØŸá ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
# 
#     if not heights:
#         print("‚ùå Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿ™ÿ®ÿ±€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØ. ÿßÿ¨ÿ±ÿß€å ÿ®ÿ±ŸÜÿßŸÖŸá ŸÖÿ™ŸàŸÇŸÅ ÿ¥ÿØ.")
#         sys.exit(1)
# 
# print("üìè ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß€å ÿßŸÜÿ™ÿÆÿßÿ®‚Äåÿ¥ÿØŸá ÿ®ÿ±ÿß€å ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß:", ", ".join(str(h) for h in heights))
# 
# # ------------------------------------------------------------
# # ‚öôÔ∏è ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿßŸàŸÑ€åŸá ÿ™ÿ≠ŸÑ€åŸÑ
# # ------------------------------------------------------------
# scaleFac = 10 * 9.81
# TFree = 0
# 
# # ‚úÖ ŸÇÿ®ŸÑÿßŸã: dataDirRoot = '../../'
# # ‚úÖ ÿßŸÑÿßŸÜ: ÿ±€åÿ¥Ÿá Ÿæÿ±Ÿà⁄òŸá ÿ±ÿß ÿÆŸàÿØ⁄©ÿßÿ± Ÿæ€åÿØÿß ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ
# dataDirRoot = PROJECT_ROOT
# 
# # ---------------------- ŸæŸàÿ¥ŸáŸî Ÿàÿ±ŸàÿØ€å GM ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÑÿ™ ----------------------
# if RUN_MODE == 'train':
#     GMFolder = os.path.join(dataDirRoot, 'Output', '1_IDA_Records_train')
# else:
#     GMFolder = os.path.join(dataDirRoot, 'Output', '1_IDA_Records_predict')
# 
# # ---------------------- ŸæŸàÿ¥ŸáŸî ÿÆÿ±Ÿàÿ¨€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÑÿ™ Ÿà ÿÆÿ∑€å/ÿ∫€åÿ±ÿÆÿ∑€å ----------------------
# if RUN_MODE == 'train':
#     if IS_LINEAR:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_train_linear')
#     else:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_train_nonlinear')
# else:  # predict
#     if IS_LINEAR:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_predict_linear')
#     else:
#         dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_predict_nonlinear')
# 
# showAnimationDeform = 0
# 
# print(f"üì• ŸæŸàÿ¥Ÿá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿàÿ±ŸàÿØ€å: {GMFolder}")
# print(f"üìÇ ŸæŸàÿ¥Ÿá Ÿæÿß€åŸá‚Äå€å ÿÆÿ±Ÿàÿ¨€å THA: {dataDirBase}\n")
# 
# # ------------------------------------------------------------
# # ‚ú≥Ô∏è Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å .AT2
# # ------------------------------------------------------------
# if not os.path.isdir(GMFolder):
#     raise FileNotFoundError(f"‚ùå ŸæŸàÿ¥Ÿá Ÿàÿ±ŸàÿØ€å ÿ±⁄©Ÿàÿ±ÿØŸáÿß Ÿæ€åÿØÿß ŸÜÿ¥ÿØ: {GMFolder}")
# 
# all_records = [f for f in os.listdir(GMFolder) if f.endswith('.AT2')]
# print(f"üîç ÿ™ÿπÿØÿßÿØ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿæ€åÿØÿß ÿ¥ÿØŸá: {len(all_records)}")
# 
# # ------------------------------------------------------------
# # üöÄ ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπ Ÿà Ÿáÿ± ÿ±⁄©Ÿàÿ±ÿØ
# # ------------------------------------------------------------
# for h_val in heights:
#     # ÿ™ŸÜÿ∏€åŸÖ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ ÿ®ÿ±ÿß€å ŸÖÿØŸÑ (ŸÖÿ™ÿ∫€åÿ± ŸÖÿ≠€åÿ∑€å ÿ®ÿ±ÿß€å model_linear.py Ÿà model_nonlinear.py)
#     os.environ["H_COL"] = str(h_val)
# 
#     # ÿ≥ÿßÿÆÿ™ ŸÜÿßŸÖ ŸæŸàÿ¥Ÿá ÿ®ÿ±ÿß€å ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     if float(h_val).is_integer():
#         h_tag = f"H{int(h_val)}"          # ŸÖÿ´ÿßŸÑ: H3
#     else:
#         h_tag = "H" + str(h_val).replace('.', 'p')   # ŸÖÿ´ÿßŸÑ: H3p5
# 
#     # ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     dataDirOut = os.path.join(dataDirBase, h_tag)
# 
#     # ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     failed_records = []
# 
#     print(f"üèóÔ∏è ÿ¥ÿ±Ÿàÿπ ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ {h_val} ŸÖÿ™ÿ± ÿØÿ± ŸæŸàÿ¥Ÿá: {dataDirOut}")
# 
#     # üßπ Ÿæÿß⁄©ÿ≥ÿßÿ≤€å ŸæŸàÿ¥Ÿá ÿÆÿ±Ÿàÿ¨€å ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     if os.path.exists(dataDirOut):
#         print(f"üßπ ÿ≠ÿ∞ŸÅ ŸÖÿ≠ÿ™Ÿàÿß€å ŸÇÿ®ŸÑ€å ŸæŸàÿ¥Ÿá: {dataDirOut}")
#         shutil.rmtree(dataDirOut)
#     os.makedirs(dataDirOut, exist_ok=True)
# 
#     # ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å Ÿáÿ± ÿ±⁄©Ÿàÿ±ÿØ
#     for i, rec_file in enumerate(all_records, start=1):
#         try:
#             record_name = os.path.splitext(rec_file)[0]  # ŸÖÿ´ŸÑÿßŸã: RSN4_..._x1_0
#             inFileName = os.path.join(GMFolder, rec_file)
#             GMPath = os.path.join(GMFolder, record_name + ".txt")
# 
#             # ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ÿ±⁄©Ÿàÿ±ÿØ (ÿ≤€åÿ±ŸæŸàÿ¥Ÿá‚Äåÿß€å ÿØÿßÿÆŸÑ ŸæŸàÿ¥Ÿá ÿßÿ±ÿ™ŸÅÿßÿπ)
#             dataDir_rec = os.path.join(dataDirOut, record_name)
#             os.makedirs(dataDir_rec, exist_ok=True)
# 
#             # ‚ö° RecorderŸáÿß ÿßÿ≤ ÿß€åŸÜ ŸÖÿ™ÿ∫€åÿ± ÿ®ÿ±ÿß€å ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ
#             dataDir = dataDir_rec
# 
#             # üîπ ÿßÿ¨ÿ±ÿß€å ŸÖÿØŸÑ (ÿÆÿ∑€å €åÿß ÿ∫€åÿ±ÿÆÿ∑€å) Ÿà ŸÖ€åÿ±ÿß€å€å
#             if IS_LINEAR:
#                 exec(io.open(locate_dep("model_linear.py"), "r", encoding="utf-8").read())
#             else:
#                 exec(open(locate_dep("model_nonlinear.py"), "r", encoding="utf-8").read())
# 
#             exec(open(locate_dep("defineDamping.py"), "r", encoding="utf-8").read())
#             exec(open(locate_dep("defineRecorders.py"), "r", encoding="utf-8").read())
# 
#             # üîπ ÿÆŸàÿßŸÜÿØŸÜ ÿ±⁄©Ÿàÿ±ÿØ ÿ≤ŸÑÿ≤ŸÑŸá Ÿà ÿßÿπŸÖÿßŸÑ ÿ®Ÿá ÿ≥ÿ±€å ÿ≤ŸÖÿßŸÜ€å
#             transformed_path = os.path.join(GMFolder, "transformed")
#             os.makedirs(transformed_path, exist_ok=True)
# 
#             dtInput, numPoints = ReadRecord(inFileName, GMPath)
#             seriesTag = 2
#             timeSeries('Path', seriesTag, '-dt', dtInput, '-filePath', GMPath, '-factor', scaleFac)
#             GMDir = 1
#             pattern('UniformExcitation', 2, GMDir, '-accel', seriesTag)
# 
#             # üîπ ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿØ€åŸÜÿßŸÖ€å⁄©€å
#             Tmax = numPoints * dtInput + TFree
#             dtAnalysis = dtInput
# 
#             mode_str = "train" if RUN_MODE == "train" else "predict"
#             lin_str = "ÿÆÿ∑€å" if IS_LINEAR else "ÿ∫€åÿ±ÿÆÿ∑€å"
#             print(f"‚öôÔ∏è ÿßÿ±ÿ™ŸÅÿßÿπ H = {h_val} m | ÿßÿ¨ÿ±ÿß€å ÿ±⁄©Ÿàÿ±ÿØ {i}/{len(all_records)}: {record_name}  ({mode_str}, {lin_str})")
# 
#             doDynamicAnalysis(Tmax, dtInput)
#             wipe()
# 
#             print(f"‚úÖ ÿßÿ±ÿ™ŸÅÿßÿπ H = {h_val} m | ÿ±⁄©Ÿàÿ±ÿØ {record_name} ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿßÿ¨ÿ±ÿß ÿ¥ÿØ. ({mode_str}, {lin_str})\n")
# 
#         except Exception as e:
#             print(f'‚ùå ÿßÿ±ÿ™ŸÅÿßÿπ H = {h_val} m | ÿÆÿ∑ÿß ÿØÿ± ÿ±⁄©Ÿàÿ±ÿØ {rec_file}: {e}')
#             failed_records.append(rec_file)
# 
#     # ‚úèÔ∏è ÿ∞ÿÆ€åÿ±Ÿá ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ÿ®ÿ±ÿß€å ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     failed_suffix = f"_{RUN_MODE}_{'linear' if IS_LINEAR else 'nonlinear'}_{h_tag}"
#     failed_file_name = f"failed_records{failed_suffix}.txt"
# 
#     with open(failed_file_name, "w", encoding="utf-8") as f:
#         for rec in failed_records:
#             f.write(f"{rec}\n")
# 
#     print(f"üìÑ ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ÿ®ÿ±ÿß€å ÿßÿ±ÿ™ŸÅÿßÿπ {h_val} ŸÖÿ™ÿ± ÿØÿ± ŸÅÿß€åŸÑ {failed_file_name} ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ.")
# 
# print(f"üèÅ ÿßÿ¨ÿ±ÿß€å ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß ÿ®ÿ±ÿß€å ŸáŸÖŸá ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß ÿ™ŸÖÿßŸÖ ÿ¥ÿØ. ÿ≠ÿßŸÑÿ™ ÿßÿ¨ÿ±ÿß: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")
# 
# =============================================================================







# =============================================================================
# 
# 
# 
# import sys, io
# import os
# import shutil
# import time
# import traceback
# 
# # ‚úÖ NEW (ETA helpers)
# from datetime import datetime, timedelta
# from collections import deque
# 
# # ‚úÖ ÿ¨ŸÑŸà⁄Ø€åÿ±€å ÿßÿ≤ ÿÆÿ∑ÿß€å UnicodeEncodeError ÿØÿ± ŸÖÿ≠€åÿ∑‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ (Spyderÿå CMDÿå Run-Codes)
# if hasattr(sys.stdout, "buffer"):
#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')
# 
# # ============================================================
# # ‚úÖ ETA formatting / EMA
# # ============================================================
# def fmt_seconds(sec: float) -> str:
#     sec = max(0.0, float(sec))
#     h = int(sec // 3600)
#     m = int((sec % 3600) // 60)
#     s = int(sec % 60)
#     return f"{h:02d}:{m:02d}:{s:02d}"
# 
# def ema_update(prev: float, x: float, alpha: float = 0.2) -> float:
#     return x if prev is None else (alpha * x + (1 - alpha) * prev)
# 
# # ============================================================
# # ‚úÖ ÿ≥ÿßÿ≤⁄Øÿßÿ±€å ÿ®ÿß ÿ¨ÿßÿ®Ÿá‚Äåÿ¨ÿß€å€å ŸÅÿß€åŸÑ ÿ®€åŸÜ:
# #   Model\Time History Analysis (THA)  ‚Üî  Model
# # ============================================================
# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# THA_DIR = os.path.join(BASE_DIR, "Time History Analysis (THA)")
# 
# if os.path.isdir(THA_DIR) and THA_DIR not in sys.path:
#     sys.path.insert(0, THA_DIR)
# 
# if BASE_DIR not in sys.path:
#     sys.path.insert(0, BASE_DIR)
# 
# def find_project_root(start_dir: str, max_up: int = 6) -> str:
#     """Root = ÿ¨ÿß€å€å ⁄©Ÿá ŸæŸàÿ¥Ÿá Output Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ."""
#     cur = os.path.abspath(start_dir)
#     for _ in range(max_up):
#         if os.path.isdir(os.path.join(cur, "Output")):
#             return cur
#         parent = os.path.dirname(cur)
#         if parent == cur:
#             break
#         cur = parent
#     return os.path.abspath(start_dir)
# 
# PROJECT_ROOT = find_project_root(BASE_DIR)
# 
# def locate_dep(filename: str) -> str:
#     """
#     ŸÅÿß€åŸÑ‚ÄåŸáÿß€å Ÿàÿßÿ®ÿ≥ÿ™Ÿá ÿ±ÿß ÿØÿ± ÿß€åŸÜ ŸÖÿ≥€åÿ±Ÿáÿß Ÿæ€åÿØÿß ŸÖ€å‚Äå⁄©ŸÜÿØ:
#       1) ⁄©ŸÜÿßÿ± ŸáŸÖ€åŸÜ ÿßÿ≥⁄©ÿ±€åŸæÿ™
#       2) ÿØÿßÿÆŸÑ Time History Analysis (THA)
#     """
#     c1 = os.path.join(BASE_DIR, filename)
#     if os.path.exists(c1):
#         return c1
#     c2 = os.path.join(THA_DIR, filename)
#     if os.path.exists(c2):
#         return c2
#     return filename
# 
# # ============================================================
# # ImportŸáÿß ÿ®ÿπÿØ ÿßÿ≤ ÿ™ŸÜÿ∏€åŸÖ sys.path
# # ============================================================
# from openseespy.opensees import *
# from ReadRecord import ReadRecord
# from analyzeAndAnimate import analyzeAndAnimateTHA
# import vfo.vfo as vfo
# import opsvis as opsv
# from doDynamicAnalysis import doDynamicAnalysis
# 
# # ============================================================
# # ‚úÖ ÿ™ŸÜÿ∏€åŸÖÿßÿ™ RESUME
# # ============================================================
# # ÿß⁄Øÿ± True ÿ®ÿßÿ¥ÿØÿå ŸÖÿ´ŸÑ ŸÇÿ®ŸÑ ŸæŸàÿ¥Ÿá Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπ ÿ±ÿß Ÿæÿß⁄© ŸÖ€å‚Äå⁄©ŸÜÿØ (ÿ®ÿ±ÿß€å ÿßÿ¨ÿ±ÿß€å ÿ™ŸÖ€åÿ≤)
# CLEAN_START_PER_HEIGHT = False
# 
# # ÿßÿ≥ŸÖ ŸÅÿß€åŸÑ ŸÖÿßÿ±⁄©ÿ± Ÿæÿß€åÿßŸÜ ŸÖŸàŸÅŸÇ Ÿáÿ± ÿ±⁄©Ÿàÿ±ÿØ
# DONE_MARKER_NAME = "__DONE__.txt"
# 
# def done_marker_path(record_out_dir: str) -> str:
#     return os.path.join(record_out_dir, DONE_MARKER_NAME)
# 
# def is_record_done(record_out_dir: str) -> bool:
#     return os.path.isfile(done_marker_path(record_out_dir))
# 
# def write_done_marker(record_out_dir: str, extra_text: str = ""):
#     p = done_marker_path(record_out_dir)
#     with open(p, "w", encoding="utf-8") as f:
#         f.write("DONE\n")
#         f.write(f"timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
#         if extra_text:
#             f.write(extra_text.strip() + "\n")
# 
# # ------------------------------------------------------------
# # üîß ÿßŸÜÿ™ÿÆÿßÿ® ÿ≠ÿßŸÑÿ™ train / predict
# # ------------------------------------------------------------
# choice = input("ÿ®ÿ±ÿß€å train ÿπÿØÿØ 0 Ÿà ÿ®ÿ±ÿß€å predict ÿπÿØÿØ 1 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
# if choice == "0":
#     RUN_MODE = "train"
# elif choice == "1":
#     RUN_MODE = "predict"
# else:
#     print("‚ùå ŸÅŸÇÿ∑ ÿπÿØÿØ 0 €åÿß 1 ŸÖÿ¨ÿßÿ≤ ÿßÿ≥ÿ™.")
#     sys.exit(1)
# 
# # ------------------------------------------------------------
# # üîß ÿßŸÜÿ™ÿÆÿßÿ® ŸÖÿØŸÑ ÿÆÿ∑€å / ÿ∫€åÿ±ÿÆÿ∑€å
# # ------------------------------------------------------------
# lin_choice = input("ŸÖÿØŸÑ ÿÆÿ∑€å ÿ®ÿßÿ¥ÿØ €åÿß ÿ∫€åÿ±ÿÆÿ∑€åÿü ÿ®ÿ±ÿß€å ŸÖÿØŸÑ ÿÆÿ∑€å ÿπÿØÿØ 1 Ÿà ÿ®ÿ±ÿß€å ÿ∫€åÿ±ÿÆÿ∑€å ÿπÿØÿØ 0 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
# IS_LINEAR = (lin_choice == "1")
# 
# print(f"üìå ÿ≠ÿßŸÑÿ™ ÿßÿ¨ÿ±ÿß: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")
# 
# os.environ["RUN_MODE"] = RUN_MODE
# os.environ["IS_LINEAR"] = "1" if IS_LINEAR else "0"
# 
# # ------------------------------------------------------------
# # üìè ÿØÿ±€åÿßŸÅÿ™ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ (€å⁄© €åÿß ⁄ÜŸÜÿØ ŸÖŸÇÿØÿßÿ±)
# # ------------------------------------------------------------
# heights_raw = input("ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ (ŸÖÿ´ŸÑÿßŸã: 3 €åÿß 3 4 5): ").strip()
# 
# if not heights_raw:
#     print("‚ö†Ô∏è Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØÿõ ŸÖŸÇÿØÿßÿ± Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂ 3 ŸÖÿ™ÿ± ÿØÿ± ŸÜÿ∏ÿ± ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
#     heights = [3.0]
# else:
#     heights = []
#     for token in heights_raw.replace(',', ' ').split():
#         try:
#             heights.append(float(token))
#         except ValueError:
#             print(f"‚ö†Ô∏è ŸÖŸÇÿØÿßÿ± ¬´{token}¬ª ÿπÿØÿØ ŸÖÿπÿ™ÿ®ÿ±€å ŸÜ€åÿ≥ÿ™ Ÿà ŸÜÿßÿØ€åÿØŸá ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
#     if not heights:
#         print("‚ùå Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿ™ÿ®ÿ±€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØ. ÿßÿ¨ÿ±ÿß€å ÿ®ÿ±ŸÜÿßŸÖŸá ŸÖÿ™ŸàŸÇŸÅ ÿ¥ÿØ.")
#         sys.exit(1)
# 
# print("üìè ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß€å ÿßŸÜÿ™ÿÆÿßÿ®‚Äåÿ¥ÿØŸá ÿ®ÿ±ÿß€å ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß:", ", ".join(str(h) for h in heights))
# 
# # ------------------------------------------------------------
# # ‚öôÔ∏è ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿßŸàŸÑ€åŸá ÿ™ÿ≠ŸÑ€åŸÑ
# # ------------------------------------------------------------
# scaleFac = 10 * 9.81
# TFree = 0
# dataDirRoot = PROJECT_ROOT
# 
# # ---------------------- ŸæŸàÿ¥ŸáŸî Ÿàÿ±ŸàÿØ€å GM ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÑÿ™ ----------------------
# if RUN_MODE == 'train':
#     GMFolder = os.path.join(dataDirRoot, 'Output', '1_IDA_Records_train')
# else:
#     GMFolder = os.path.join(dataDirRoot, 'Output', '1_IDA_Records_predict')
# 
# # ---------------------- ŸæŸàÿ¥ŸáŸî ÿÆÿ±Ÿàÿ¨€å THA ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÑÿ™ Ÿà ÿÆÿ∑€å/ÿ∫€åÿ±ÿÆÿ∑€å ----------------------
# if RUN_MODE == 'train':
#     dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_train_linear' if IS_LINEAR else '2_THA_train_nonlinear')
# else:
#     dataDirBase = os.path.join(dataDirRoot, 'Output', '2_THA_predict_linear' if IS_LINEAR else '2_THA_predict_nonlinear')
# 
# print(f"üì• ŸæŸàÿ¥Ÿá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿàÿ±ŸàÿØ€å: {GMFolder}")
# print(f"üìÇ ŸæŸàÿ¥Ÿá Ÿæÿß€åŸá‚Äå€å ÿÆÿ±Ÿàÿ¨€å THA: {dataDirBase}\n")
# 
# # ------------------------------------------------------------
# # ‚ú≥Ô∏è Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å .AT2
# # ------------------------------------------------------------
# if not os.path.isdir(GMFolder):
#     raise FileNotFoundError(f"‚ùå ŸæŸàÿ¥Ÿá Ÿàÿ±ŸàÿØ€å ÿ±⁄©Ÿàÿ±ÿØŸáÿß Ÿæ€åÿØÿß ŸÜÿ¥ÿØ: {GMFolder}")
# 
# all_records = [f for f in os.listdir(GMFolder) if f.endswith('.AT2')]
# all_records.sort()
# print(f"üîç ÿ™ÿπÿØÿßÿØ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿæ€åÿØÿß ÿ¥ÿØŸá: {len(all_records)}")
# 
# # ============================================================
# # ‚úÖ NEW: GLOBAL ETA across ALL selected heights
# #   - Count remaining runs (not DONE) for all heights upfront
# #   - Track progress across heights while running
# # ============================================================
# EMA_ALPHA_GLOBAL = 0.2
# ema_total_global = None
# ema_model_global = None
# global_last100_total = deque(maxlen=100)
# global_last100_model = deque(maxlen=100)
# 
# global_total_to_run = 0
# for h_val in heights:
#     if float(h_val).is_integer():
#         h_tag_tmp = f"H{int(h_val)}"
#     else:
#         h_tag_tmp = "H" + str(h_val).replace('.', 'p')
# 
#     dataDirOut_tmp = os.path.join(dataDirBase, h_tag_tmp)
#     # ÿß⁄Øÿ± ŸæŸàÿ¥Ÿá Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ±ÿØÿå €åÿπŸÜ€å Ÿá€å⁄Ü DONE ÿß€å ŸáŸÖ ŸÜ€åÿ≥ÿ™ => ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá‚ÄåÿßŸÜÿØ
#     for rf in all_records:
#         rn = os.path.splitext(rf)[0]
#         rec_dir = os.path.join(dataDirOut_tmp, rn)
#         if not os.path.isfile(os.path.join(rec_dir, DONE_MARKER_NAME)):
#             global_total_to_run += 1
# 
# global_executed = 0
# global_start_perf = time.perf_counter()
# 
# print("============================================================")
# print(f"üåç ⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá (ŸáŸÖŸá ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß): ⁄©ŸÑ ÿ±⁄©Ÿàÿ±ÿØŸáÿß={len(all_records)} | ÿ™ÿπÿØÿßÿØ ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß={len(heights)}")
# print(f"üåç ŸÇÿßÿ®ŸÑ ÿßÿ¨ÿ±ÿß (ÿ®ÿØŸàŸÜ DONE) ÿØÿ± ŸáŸÖŸá ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß: {global_total_to_run}")
# print("============================================================")
# 
# # ------------------------------------------------------------
# # üöÄ ÿßÿ¨ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿ®ÿ±ÿß€å Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπ Ÿà Ÿáÿ± ÿ±⁄©Ÿàÿ±ÿØ (ÿ®ÿß RESUME)
# # ------------------------------------------------------------
# for h_val in heights:
#     os.environ["H_COL"] = str(h_val)
# 
#     if float(h_val).is_integer():
#         h_tag = f"H{int(h_val)}"
#     else:
#         h_tag = "H" + str(h_val).replace('.', 'p')
# 
#     dataDirOut = os.path.join(dataDirBase, h_tag)
#     os.makedirs(dataDirOut, exist_ok=True)
# 
#     print(f"\nüèóÔ∏è ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ = {h_val} ŸÖÿ™ÿ± | ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ÿßÿ±ÿ™ŸÅÿßÿπ: {dataDirOut}")
# 
#     # ‚úÖ ŸÅŸÇÿ∑ ÿß⁄Øÿ± ⁄©ÿßÿ±ÿ®ÿ± ÿ®ÿÆŸàÿßŸáÿØ ÿßÿ≤ ÿµŸÅÿ± ÿ®ÿ±ÿß€å ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ¥ÿ±Ÿàÿπ ⁄©ŸÜÿØ
#     if CLEAN_START_PER_HEIGHT:
#         print(f"üßπ CLEAN_START_PER_HEIGHT=True ‚Üí ÿ≠ÿ∞ŸÅ ⁄©ÿßŸÖŸÑ ŸæŸàÿ¥Ÿá ÿßÿ±ÿ™ŸÅÿßÿπ: {dataDirOut}")
#         shutil.rmtree(dataDirOut, ignore_errors=True)
#         os.makedirs(dataDirOut, exist_ok=True)
# 
#     failed_records = []
#     skipped = 0
#     executed = 0
# 
#     # ============================================================
#     # ‚úÖ ETA stats per height + last100 reporting (per-height)
#     # ============================================================
#     ema_total = None
#     ema_model = None
#     last100_total = deque(maxlen=100)
#     last100_model = deque(maxlen=100)
# 
#     # ÿ™ÿπÿØÿßÿØ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å€å ⁄©Ÿá ŸàÿßŸÇÿπÿßŸã ÿ®ÿß€åÿØ ÿßÿ¨ÿ±ÿß ÿ¥ŸàŸÜÿØ (DONE Ÿáÿß ÿ≠ÿ∞ŸÅ ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ) ÿ®ÿ±ÿß€å ŸáŸÖ€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     to_run_total = 0
#     for rf in all_records:
#         rn = os.path.splitext(rf)[0]
#         rec_dir = os.path.join(dataDirOut, rn)
#         if not os.path.isfile(os.path.join(rec_dir, DONE_MARKER_NAME)):
#             to_run_total += 1
# 
#     print(f"üìå ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ: ⁄©ŸÑ ÿ±⁄©Ÿàÿ±ÿØŸáÿß={len(all_records)} | ŸÇÿßÿ®ŸÑ ÿßÿ¨ÿ±ÿß (ÿ®ÿØŸàŸÜ DONE)={to_run_total}")
#     height_start_perf = time.perf_counter()
# 
#     for i, rec_file in enumerate(all_records, start=1):
#         record_name = os.path.splitext(rec_file)[0]
#         inFileName = os.path.join(GMFolder, rec_file)
#         GMPath = os.path.join(GMFolder, record_name + ".txt")
# 
#         # ŸæŸàÿ¥Ÿá ÿÆÿ±Ÿàÿ¨€å ÿ±⁄©Ÿàÿ±ÿØ
#         dataDir_rec = os.path.join(dataDirOut, record_name)
#         os.makedirs(dataDir_rec, exist_ok=True)
# 
#         # ‚úÖ ÿß⁄Øÿ± ŸÇÿ®ŸÑÿßŸã DONE ÿ¥ÿØŸáÿå skip
#         if is_record_done(dataDir_rec):
#             skipped += 1
#             print(f"‚è≠Ô∏è  SKIP (DONE) | {i}/{len(all_records)} | H={h_val} | {record_name}")
#             continue
# 
#         # timer for total record duration
#         t_rec0 = time.perf_counter()
# 
#         try:
#             # Ÿæÿß⁄©ÿ≥ÿßÿ≤€å ÿßŸÖŸÜ ŸæŸàÿ¥Ÿá ÿ±⁄©Ÿàÿ±ÿØ
#             for fname in os.listdir(dataDir_rec):
#                 fpath = os.path.join(dataDir_rec, fname)
#                 try:
#                     if os.path.isfile(fpath) or os.path.islink(fpath):
#                         os.remove(fpath)
#                     elif os.path.isdir(fpath):
#                         shutil.rmtree(fpath)
#                 except Exception:
#                     pass
# 
#             # ‚ö° RecorderŸáÿß ÿßÿ≤ ÿß€åŸÜ ŸÖÿ™ÿ∫€åÿ± ÿ®ÿ±ÿß€å ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ
#             dataDir = dataDir_rec
# 
#             # ŸáŸÖ€åÿ¥Ÿá ŸÇÿ®ŸÑ ÿßÿ≤ ÿ≥ÿßÿÆÿ™ ŸÖÿØŸÑÿå wipe ÿß€åŸÖŸÜ
#             wipe()
# 
#             # üîπ ÿßÿ¨ÿ±ÿß€å ŸÖÿØŸÑ (ÿÆÿ∑€å €åÿß ÿ∫€åÿ±ÿÆÿ∑€å) Ÿà ŸÖ€åÿ±ÿß€å€å Ÿà ÿ±⁄©Ÿàÿ±ÿØÿ±Ÿáÿß
#             if IS_LINEAR:
#                 exec(io.open(locate_dep("model_linear.py"), "r", encoding="utf-8").read())
#             else:
#                 exec(open(locate_dep("model_nonlinear.py"), "r", encoding="utf-8").read())
# 
#             exec(open(locate_dep("defineDamping.py"), "r", encoding="utf-8").read())
#             exec(open(locate_dep("defineRecorders.py"), "r", encoding="utf-8").read())
# 
#             # üîπ ÿÆŸàÿßŸÜÿØŸÜ ÿ±⁄©Ÿàÿ±ÿØ Ÿà ÿ™ÿπÿ±€åŸÅ ÿ™ÿ≠ÿ±€å⁄©
#             transformed_path = os.path.join(GMFolder, "transformed")
#             os.makedirs(transformed_path, exist_ok=True)
# 
#             dtInput, numPoints = ReadRecord(inFileName, GMPath)
#             seriesTag = 2
#             timeSeries('Path', seriesTag, '-dt', dtInput, '-filePath', GMPath, '-factor', scaleFac)
#             GMDir = 1
#             pattern('UniformExcitation', 2, GMDir, '-accel', seriesTag)
# 
#             Tmax = numPoints * dtInput + TFree
# 
#             mode_str = "train" if RUN_MODE == "train" else "predict"
#             lin_str = "ÿÆÿ∑€å" if IS_LINEAR else "ÿ∫€åÿ±ÿÆÿ∑€å"
#             print(f"‚öôÔ∏è RUN | {i}/{len(all_records)} | H={h_val} | {record_name}  ({mode_str}, {lin_str})")
# 
#             # timer for model/analysis duration only
#             t_model0 = time.perf_counter()
#             doDynamicAnalysis(Tmax, dtInput)
#             t_model1 = time.perf_counter()
#             model_sec = t_model1 - t_model0
# 
#             # total record time
#             t_rec1 = time.perf_counter()
#             total_sec = t_rec1 - t_rec0
# 
#             # ‚úÖ ŸÖÿßÿ±⁄©ÿ± DONE
#             write_done_marker(
#                 dataDir_rec,
#                 extra_text=f"mode={RUN_MODE}, model={'linear' if IS_LINEAR else 'nonlinear'}, H={h_val}, rec={record_name}"
#             )
# 
#             executed += 1
#             global_executed += 1
#             print(f"‚úÖ DONE | H={h_val} | {record_name}\n")
# 
#             # wipe ÿ®ÿπÿØ ÿßÿ≤ Ÿæÿß€åÿßŸÜ ÿ±⁄©Ÿàÿ±ÿØ
#             wipe()
# 
#             # ---------------- per-height stats ----------------
#             last100_total.append(total_sec)
#             last100_model.append(model_sec)
#             ema_total = ema_update(ema_total, total_sec, alpha=EMA_ALPHA_GLOBAL)
#             ema_model = ema_update(ema_model, model_sec, alpha=EMA_ALPHA_GLOBAL)
# 
#             remain_h = max(0, to_run_total - executed)
#             avg_total_h = ema_total if ema_total is not None else total_sec
#             eta_h_sec = remain_h * avg_total_h
#             finish_h = datetime.now() + timedelta(seconds=eta_h_sec)
#             elapsed_h = time.perf_counter() - height_start_perf
# 
#             # ---------------- global stats ----------------
#             global_last100_total.append(total_sec)
#             global_last100_model.append(model_sec)
#             ema_total_global = ema_update(ema_total_global, total_sec, alpha=EMA_ALPHA_GLOBAL)
#             ema_model_global = ema_update(ema_model_global, model_sec, alpha=EMA_ALPHA_GLOBAL)
# 
#             remain_g = max(0, global_total_to_run - global_executed)
#             avg_total_g = ema_total_global if ema_total_global is not None else total_sec
#             eta_g_sec = remain_g * avg_total_g
#             finish_g = datetime.now() + timedelta(seconds=eta_g_sec)
#             elapsed_g = time.perf_counter() - global_start_perf
# 
#             # ---------------- prints ----------------
#             # print(f"‚è±Ô∏è ÿ≤ŸÖÿßŸÜ ÿ±⁄©Ÿàÿ±ÿØ (⁄©ŸÑ): {fmt_seconds(total_sec)} | ÿ≤ŸÖÿßŸÜ ŸÖÿØŸÑ/ÿ™ÿ≠ŸÑ€åŸÑ: {fmt_seconds(model_sec)}")
#             # print(f"üìà ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ: {executed}/{to_run_total} | ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá: {remain_h} | ETA(H): {fmt_seconds(eta_h_sec)} ‚Üí {finish_h.strftime('%Y-%m-%d %H:%M:%S')}")
#             print(f"üåç ⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá: {global_executed}/{global_total_to_run} | ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá: {remain_g} | ETA(ALL): {fmt_seconds(eta_g_sec)} ‚Üí {finish_g.strftime('%Y-%m-%d %H:%M:%S')}")
#             # print(f"üïí ÿ≥Ÿæÿ±€å‚Äåÿ¥ÿØŸá: ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ={fmt_seconds(elapsed_h)} | ⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá={fmt_seconds(elapsed_g)}")
# 
#             # 100-run reports (only if we have 100 successes IN THIS SESSION)
#             if len(last100_total) == 100:
#                 print(f"üìä €±€∞€∞ ÿßÿ¨ÿ±ÿß€å ŸÇÿ®ŸÑ€å (ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ): ⁄©ŸÑ={fmt_seconds(sum(last100_total))} | ŸÖÿØŸÑ/ÿ™ÿ≠ŸÑ€åŸÑ={fmt_seconds(sum(last100_model))}")
#             if len(global_last100_total) == 100:
#                 print(f"üìä €±€∞€∞ ÿßÿ¨ÿ±ÿß€å ŸÇÿ®ŸÑ€å (⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá): ⁄©ŸÑ={fmt_seconds(sum(global_last100_total))} | ŸÖÿØŸÑ/ÿ™ÿ≠ŸÑ€åŸÑ={fmt_seconds(sum(global_last100_model))}")
# 
#         except KeyboardInterrupt:
#             print("\nüõë ÿßÿ¨ÿ±ÿß€å ÿ®ÿ±ŸÜÿßŸÖŸá ÿ™Ÿàÿ≥ÿ∑ ⁄©ÿßÿ±ÿ®ÿ± ŸÖÿ™ŸàŸÇŸÅ ÿ¥ÿØ (KeyboardInterrupt).")
#             print("‚úÖ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å DONE ÿ¥ÿØŸá ÿ≠ŸÅÿ∏ ŸÖ€å‚Äåÿ¥ŸàŸÜÿØÿõ ÿØŸÅÿπŸá ÿ®ÿπÿØ ÿßÿØÿßŸÖŸá ŸÖ€å‚ÄåÿØŸáÿØ.")
#             raise
# 
#         except Exception as e:
#             failed_records.append(rec_file)
#             print(f"‚ùå FAIL | H={h_val} | {rec_file}: {e}")
# 
#             # ŸÅÿß€åŸÑ ÿÆÿ∑ÿß
#             try:
#                 err_path = os.path.join(dataDir_rec, "__ERROR__.txt")
#                 with open(err_path, "w", encoding="utf-8") as f:
#                     f.write(f"ERROR for record: {rec_file}\n")
#                     f.write(f"H={h_val}, mode={RUN_MODE}, model={'linear' if IS_LINEAR else 'nonlinear'}\n\n")
#                     f.write("Exception:\n")
#                     f.write(str(e) + "\n\n")
#                     f.write("Traceback:\n")
#                     f.write(traceback.format_exc())
#             except Exception:
#                 pass
# 
#             try:
#                 wipe()
#             except Exception:
#                 pass
# 
#     # ÿ∞ÿÆ€åÿ±Ÿá ŸÑ€åÿ≥ÿ™ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å ŸÜÿßŸÖŸàŸÅŸÇ ÿ®ÿ±ÿß€å ÿß€åŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ
#     failed_suffix = f"_{RUN_MODE}_{'linear' if IS_LINEAR else 'nonlinear'}_{h_tag}"
#     failed_file_name = os.path.join(dataDirOut, f"failed_records{failed_suffix}.txt")
#     with open(failed_file_name, "w", encoding="utf-8") as f:
#         for rec in failed_records:
#             f.write(f"{rec}\n")
# 
#     print("------------------------------------------------------------")
#     print(f"üìå ÿÆŸÑÿßÿµŸá ÿßÿ±ÿ™ŸÅÿßÿπ {h_tag}:")
#     print(f"   ‚úÖ ÿßÿ¨ÿ±ÿß ÿ¥ÿØŸá ÿ¨ÿØ€åÿØ: {executed}")
#     print(f"   ‚è≠Ô∏è  ÿßÿ≥⁄©€åŸæ (ŸÇÿ®ŸÑÿßŸã DONE): {skipped}")
#     print(f"   ‚ùå ŸÜÿßŸÖŸàŸÅŸÇ: {len(failed_records)}")
#     print(f"üìÑ ŸÑÿß⁄Ø ŸÜÿßŸÖŸàŸÅŸÇ‚ÄåŸáÿß: {failed_file_name}")
#     print("------------------------------------------------------------")
# 
# print(f"\nüèÅ Ÿæÿß€åÿßŸÜ ÿßÿ¨ÿ±ÿß€å ŸáŸÖŸá ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß. ÿ≠ÿßŸÑÿ™: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")
# 
# =============================================================================







# -*- coding: utf-8 -*-
import sys, io
import os
import shutil
import time
import traceback
from datetime import datetime, timedelta
from collections import deque
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

# ‚úÖ ÿ¨ŸÑŸà⁄Ø€åÿ±€å ÿßÿ≤ ÿÆÿ∑ÿß€å UnicodeEncodeError ÿØÿ± ŸÖÿ≠€åÿ∑‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ (Spyderÿå CMDÿå Run-Codes)
if hasattr(sys.stdout, "buffer"):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="ignore")

# ============================================================
# ‚úÖ ETA formatting / EMA
# ============================================================
def fmt_seconds(sec: float) -> str:
    sec = max(0.0, float(sec))
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def ema_update(prev: float, x: float, alpha: float = 0.2) -> float:
    return x if prev is None else (alpha * x + (1 - alpha) * prev)

# ============================================================
# ‚úÖ Root finder: ÿ¨ÿß€å€å ⁄©Ÿá ŸæŸàÿ¥Ÿá Output Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ
# ============================================================
def find_project_root(start_dir: str, max_up: int = 6) -> str:
    cur = os.path.abspath(start_dir)
    for _ in range(max_up):
        if os.path.isdir(os.path.join(cur, "Output")):
            return cur
        parent = os.path.dirname(cur)
        if parent == cur:
            break
        cur = parent
    return os.path.abspath(start_dir)

# ============================================================
# ‚úÖ RESUME markers
# ============================================================
DONE_MARKER_NAME = "__DONE__.txt"

def done_marker_path(record_out_dir: str) -> str:
    return os.path.join(record_out_dir, DONE_MARKER_NAME)

def is_record_done(record_out_dir: str) -> bool:
    return os.path.isfile(done_marker_path(record_out_dir))

def write_done_marker(record_out_dir: str, extra_text: str = ""):
    p = done_marker_path(record_out_dir)
    with open(p, "w", encoding="utf-8") as f:
        f.write("DONE\n")
        f.write(f"timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        if extra_text:
            f.write(extra_text.strip() + "\n")

def safe_clean_dir(folder: str):
    """ÿ≠ÿ∞ŸÅ ÿßŸÖŸÜ ŸÖÿ≠ÿ™Ÿàÿß€å ŸæŸàÿ¥Ÿá (ÿ®ÿØŸàŸÜ ÿ≠ÿ∞ŸÅ ÿÆŸàÿØ ŸæŸàÿ¥Ÿá)."""
    if not os.path.isdir(folder):
        return
    for fname in os.listdir(folder):
        fpath = os.path.join(folder, fname)
        try:
            if os.path.isfile(fpath) or os.path.islink(fpath):
                os.remove(fpath)
            elif os.path.isdir(fpath):
                shutil.rmtree(fpath, ignore_errors=True)
        except Exception:
            pass

def height_tag(h_val: float) -> str:
    if float(h_val).is_integer():
        return f"H{int(h_val)}"
    return "H" + str(h_val).replace(".", "p")

# ============================================================
# ‚úÖ Worker: ÿßÿ¨ÿ±ÿß€å €å⁄© ÿ±⁄©Ÿàÿ±ÿØ ÿ®ÿ±ÿß€å €å⁄© ÿßÿ±ÿ™ŸÅÿßÿπ (ÿØÿ± €å⁄© Ÿæÿ±ÿØÿßÿ≤Ÿá ŸÖÿ≥ÿ™ŸÇŸÑ)
# ============================================================
def run_one_job(job: dict) -> dict:
    """
    job keys:
      - h_val, rec_file, GMFolder, dataDirBase, PROJECT_ROOT, BASE_DIR, THA_DIR
      - RUN_MODE, IS_LINEAR
      - scaleFac, TFree
    """
    h_val      = job["h_val"]
    rec_file   = job["rec_file"]
    GMFolder   = job["GMFolder"]
    dataDirBase= job["dataDirBase"]
    RUN_MODE   = job["RUN_MODE"]
    IS_LINEAR  = job["IS_LINEAR"]
    scaleFac   = job["scaleFac"]
    TFree      = job["TFree"]
    BASE_DIR   = job["BASE_DIR"]
    THA_DIR    = job["THA_DIR"]

    # ÿ®ÿ±ÿß€å ŸÖÿß⁄òŸàŸÑ‚ÄåŸáÿß€å Ÿàÿßÿ®ÿ≥ÿ™Ÿá (ReadRecord, doDynamicAnalysis, ...)
    if os.path.isdir(THA_DIR) and THA_DIR not in sys.path:
        sys.path.insert(0, THA_DIR)
    if BASE_DIR not in sys.path:
        sys.path.insert(0, BASE_DIR)

    def locate_dep(filename: str) -> str:
        c1 = os.path.join(BASE_DIR, filename)
        if os.path.exists(c1):
            return c1
        c2 = os.path.join(THA_DIR, filename)
        if os.path.exists(c2):
            return c2
        return filename

    # ÿÆÿ±Ÿàÿ¨€å‚ÄåŸáÿß
    record_name = os.path.splitext(rec_file)[0]
    h_tag = height_tag(h_val)
    dataDirOut = os.path.join(dataDirBase, h_tag)
    dataDir_rec = os.path.join(dataDirOut, record_name)
    os.makedirs(dataDir_rec, exist_ok=True)

    # ÿß⁄Øÿ± DONE ÿßÿ≥ÿ™ÿå ÿ≥ÿ±€åÿπ ÿ®ÿ±⁄Øÿ±ÿØ
    if is_record_done(dataDir_rec):
        return {
            "status": "SKIP",
            "h_val": h_val,
            "rec_file": rec_file,
            "record_name": record_name,
            "dataDir_rec": dataDir_rec,
            "total_sec": 0.0,
            "model_sec": 0.0,
            "error": ""
        }

    # envŸáÿß (ŸÖÿØŸÑ‚ÄåŸáÿß€å ÿ¥ŸÖÿß ÿßÿ≤ ÿß€åŸÜ‚ÄåŸáÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ)
    os.environ["RUN_MODE"] = RUN_MODE
    os.environ["IS_LINEAR"] = "1" if IS_LINEAR else "0"
    os.environ["H_COL"] = str(h_val)

    inFileName = os.path.join(GMFolder, rec_file)
    GMPath = os.path.join(GMFolder, record_name + ".txt")

    # ÿ™ÿß€åŸÖÿ± ⁄©ŸÑ ÿ±⁄©Ÿàÿ±ÿØ
    t_rec0 = time.perf_counter()

    try:
        # Ÿæÿß⁄©ÿ≥ÿßÿ≤€å ÿßŸÖŸÜ ŸæŸàÿ¥Ÿá ÿ±⁄©Ÿàÿ±ÿØ
        safe_clean_dir(dataDir_rec)

        # ImportŸáÿß€å OpenSees ÿØÿßÿÆŸÑ Ÿæÿ±ÿØÿßÿ≤Ÿá (ÿÆ€åŸÑ€å ŸÖŸáŸÖ)
        from openseespy.opensees import wipe, timeSeries, pattern
        from ReadRecord import ReadRecord
        from doDynamicAnalysis import doDynamicAnalysis

        # ŸÖÿØŸÑ‚ÄåŸáÿß ÿ®Ÿá dataDir ŸÜ€åÿßÿ≤ ÿØÿßÿ±ŸÜÿØ (RecorderŸáÿß)
        dataDir = dataDir_rec  # noqa: F841  (ÿ®ÿ±ÿß€å ŸÅÿß€åŸÑ‚ÄåŸáÿß€å exec)

        # wipe ÿß€åŸÖŸÜ
        wipe()

        # ÿ≥ÿßÿÆÿ™ ŸÖÿØŸÑ
        if IS_LINEAR:
            exec(io.open(locate_dep("model_linear.py"), "r", encoding="utf-8").read(), globals(), locals())
        else:
            exec(io.open(locate_dep("model_nonlinear.py"), "r", encoding="utf-8").read(), globals(), locals())

        exec(io.open(locate_dep("defineDamping.py"), "r", encoding="utf-8").read(), globals(), locals())
        exec(io.open(locate_dep("defineRecorders.py"), "r", encoding="utf-8").read(), globals(), locals())

        # ÿÆŸàÿßŸÜÿØŸÜ ÿ±⁄©Ÿàÿ±ÿØ Ÿà ÿ™ÿπÿ±€åŸÅ ÿ™ÿ≠ÿ±€å⁄©
        transformed_path = os.path.join(GMFolder, "transformed")
        os.makedirs(transformed_path, exist_ok=True)

        dtInput, numPoints = ReadRecord(inFileName, GMPath)
        seriesTag = 2
        timeSeries("Path", seriesTag, "-dt", dtInput, "-filePath", GMPath, "-factor", scaleFac)
        GMDir = 1
        pattern("UniformExcitation", 2, GMDir, "-accel", seriesTag)

        Tmax = numPoints * dtInput + TFree

        # ÿ™ÿß€åŸÖÿ± ÿ™ÿ≠ŸÑ€åŸÑ (ŸÖÿØŸÑ)
        t_model0 = time.perf_counter()
        doDynamicAnalysis(Tmax, dtInput)
        t_model1 = time.perf_counter()
        model_sec = t_model1 - t_model0

        # ÿ≤ŸÖÿßŸÜ ⁄©ŸÑ ÿ±⁄©Ÿàÿ±ÿØ
        t_rec1 = time.perf_counter()
        total_sec = t_rec1 - t_rec0

        # ŸÖÿßÿ±⁄©ÿ± DONE
        write_done_marker(
            dataDir_rec,
            extra_text=f"mode={RUN_MODE}, model={'linear' if IS_LINEAR else 'nonlinear'}, H={h_val}, rec={record_name}"
        )

        # wipe Ÿæÿß€åÿßŸÜ ÿ±⁄©Ÿàÿ±ÿØ
        wipe()

        return {
            "status": "OK",
            "h_val": h_val,
            "rec_file": rec_file,
            "record_name": record_name,
            "dataDir_rec": dataDir_rec,
            "total_sec": float(total_sec),
            "model_sec": float(model_sec),
            "error": ""
        }

    except KeyboardInterrupt:
        # ÿØÿ± Ÿæÿ±ÿØÿßÿ≤Ÿá‚ÄåŸáÿß ÿ®Ÿáÿ™ÿ± ÿßÿ≥ÿ™ ŸáŸÖ€åŸÜ ÿ±ÿß Ÿæÿßÿ≥ ÿ®ÿØŸá€åŸÖ
        raise

    except Exception as e:
        # ŸÅÿß€åŸÑ ÿÆÿ∑ÿß
        try:
            err_path = os.path.join(dataDir_rec, "__ERROR__.txt")
            with open(err_path, "w", encoding="utf-8") as f:
                f.write(f"ERROR for record: {rec_file}\n")
                f.write(f"H={h_val}, mode={RUN_MODE}, model={'linear' if IS_LINEAR else 'nonlinear'}\n\n")
                f.write("Exception:\n")
                f.write(str(e) + "\n\n")
                f.write("Traceback:\n")
                f.write(traceback.format_exc())
        except Exception:
            pass

        return {
            "status": "FAIL",
            "h_val": h_val,
            "rec_file": rec_file,
            "record_name": record_name,
            "dataDir_rec": dataDir_rec,
            "total_sec": float(time.perf_counter() - t_rec0),
            "model_sec": 0.0,
            "error": f"{e}"
        }

# ============================================================
# ‚úÖ MAIN
# ============================================================
def main():
    # ÿ≥ÿßÿ≤⁄Øÿßÿ±€å ÿ®ÿß ÿ¨ÿßÿ®Ÿá‚Äåÿ¨ÿß€å€å ŸÅÿß€åŸÑ ÿ®€åŸÜ:
    #   Model\Time History Analysis (THA)  ‚Üî  Model
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    THA_DIR  = os.path.join(BASE_DIR, "Time History Analysis (THA)")
    PROJECT_ROOT = find_project_root(BASE_DIR)

    # ------------------------------------------------------------
    # üîß ÿßŸÜÿ™ÿÆÿßÿ® ÿ≠ÿßŸÑÿ™ train / predict
    # ------------------------------------------------------------
    choice = input("ÿ®ÿ±ÿß€å train ÿπÿØÿØ 0 Ÿà ÿ®ÿ±ÿß€å predict ÿπÿØÿØ 1 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
    if choice == "0":
        RUN_MODE = "train"
    elif choice == "1":
        RUN_MODE = "predict"
    else:
        print("‚ùå ŸÅŸÇÿ∑ ÿπÿØÿØ 0 €åÿß 1 ŸÖÿ¨ÿßÿ≤ ÿßÿ≥ÿ™.")
        sys.exit(1)

    # ------------------------------------------------------------
    # üîß ÿßŸÜÿ™ÿÆÿßÿ® ŸÖÿØŸÑ ÿÆÿ∑€å / ÿ∫€åÿ±ÿÆÿ∑€å
    # ------------------------------------------------------------
    lin_choice = input("ŸÖÿØŸÑ ÿÆÿ∑€å ÿ®ÿßÿ¥ÿØ €åÿß ÿ∫€åÿ±ÿÆÿ∑€åÿü ÿ®ÿ±ÿß€å ŸÖÿØŸÑ ÿÆÿ∑€å ÿπÿØÿØ 1 Ÿà ÿ®ÿ±ÿß€å ÿ∫€åÿ±ÿÆÿ∑€å ÿπÿØÿØ 0 ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ: ").strip()
    IS_LINEAR = (lin_choice == "1")

    print(f"üìå ÿ≠ÿßŸÑÿ™ ÿßÿ¨ÿ±ÿß: {RUN_MODE} | ŸÖÿØŸÑ: {'ÿÆÿ∑€å' if IS_LINEAR else 'ÿ∫€åÿ±ÿÆÿ∑€å'}")

    # ------------------------------------------------------------
    # üìè ÿØÿ±€åÿßŸÅÿ™ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ (€å⁄© €åÿß ⁄ÜŸÜÿØ ŸÖŸÇÿØÿßÿ±)
    # ------------------------------------------------------------
    heights_raw = input("ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß ÿ±ÿß Ÿàÿßÿ±ÿØ ⁄©ŸÜ (ŸÖÿ´ŸÑÿßŸã: 3 €åÿß 3 4 5): ").strip()

    if not heights_raw:
        print("‚ö†Ô∏è Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØÿõ ŸÖŸÇÿØÿßÿ± Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂ 3 ŸÖÿ™ÿ± ÿØÿ± ŸÜÿ∏ÿ± ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
        heights = [3.0]
    else:
        heights = []
        for token in heights_raw.replace(",", " ").split():
            try:
                heights.append(float(token))
            except ValueError:
                print(f"‚ö†Ô∏è ŸÖŸÇÿØÿßÿ± ¬´{token}¬ª ÿπÿØÿØ ŸÖÿπÿ™ÿ®ÿ±€å ŸÜ€åÿ≥ÿ™ Ÿà ŸÜÿßÿØ€åÿØŸá ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.")
        if not heights:
            print("‚ùå Ÿá€å⁄Ü ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿ™ÿ®ÿ±€å Ÿàÿßÿ±ÿØ ŸÜÿ¥ÿØ. ÿßÿ¨ÿ±ÿß€å ÿ®ÿ±ŸÜÿßŸÖŸá ŸÖÿ™ŸàŸÇŸÅ ÿ¥ÿØ.")
            sys.exit(1)

    print("üìè ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß€å ÿßŸÜÿ™ÿÆÿßÿ®‚Äåÿ¥ÿØŸá:", ", ".join(str(h) for h in heights))

    # ------------------------------------------------------------
    # ‚öôÔ∏è ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿßŸàŸÑ€åŸá ÿ™ÿ≠ŸÑ€åŸÑ
    # ------------------------------------------------------------
    scaleFac = 10 * 9.81
    TFree = 0
    dataDirRoot = PROJECT_ROOT

    # ---------------------- ŸæŸàÿ¥ŸáŸî Ÿàÿ±ŸàÿØ€å GM ----------------------
    if RUN_MODE == "train":
        GMFolder = os.path.join(dataDirRoot, "Output", "1_IDA_Records_train")
    else:
        GMFolder = os.path.join(dataDirRoot, "Output", "1_IDA_Records_predict")

    # ---------------------- ŸæŸàÿ¥ŸáŸî ÿÆÿ±Ÿàÿ¨€å THA ----------------------
    if RUN_MODE == "train":
        dataDirBase = os.path.join(
            dataDirRoot,
            "Output",
            "2_THA_train_linear" if IS_LINEAR else "2_THA_train_nonlinear"
        )
    else:
        dataDirBase = os.path.join(
            dataDirRoot,
            "Output",
            "2_THA_predict_linear" if IS_LINEAR else "2_THA_predict_nonlinear"
        )

    print(f"üì• ŸæŸàÿ¥Ÿá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿàÿ±ŸàÿØ€å: {GMFolder}")
    print(f"üìÇ ŸæŸàÿ¥Ÿá Ÿæÿß€åŸá‚Äå€å ÿÆÿ±Ÿàÿ¨€å THA: {dataDirBase}")

    # ------------------------------------------------------------
    # ‚ú≥Ô∏è Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å .AT2
    # ------------------------------------------------------------
    if not os.path.isdir(GMFolder):
        raise FileNotFoundError(f"‚ùå ŸæŸàÿ¥Ÿá Ÿàÿ±ŸàÿØ€å ÿ±⁄©Ÿàÿ±ÿØŸáÿß Ÿæ€åÿØÿß ŸÜÿ¥ÿØ: {GMFolder}")

    all_records = [f for f in os.listdir(GMFolder) if f.endswith(".AT2")]
    all_records.sort()
    print(f"üîç ÿ™ÿπÿØÿßÿØ ÿ±⁄©Ÿàÿ±ÿØŸáÿß€å Ÿæ€åÿØÿß ÿ¥ÿØŸá: {len(all_records)}")

    # ------------------------------------------------------------
    # üßπ ÿß⁄Øÿ± ŸÖ€å‚ÄåÿÆŸàÿßŸá€åÿØ ÿßÿ≤ ÿµŸÅÿ± ÿ¥ÿ±Ÿàÿπ ÿ¥ŸàÿØ (ÿßÿÆÿ™€åÿßÿ±€å)
    # ------------------------------------------------------------
    CLEAN_START_PER_HEIGHT = False
    clean_choice = input("ÿß⁄Øÿ± ŸÖ€å‚ÄåÿÆŸàÿßŸá€å ÿ®ÿ±ÿß€å Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπ ÿÆÿ±Ÿàÿ¨€å‚ÄåŸáÿß ⁄©ÿßŸÖŸÑ Ÿæÿß⁄© ÿ¥ŸàÿØ ÿπÿØÿØ 1 Ÿà⁄Øÿ±ŸÜŸá 0: ").strip()
    if clean_choice == "1":
        CLEAN_START_PER_HEIGHT = True

    if CLEAN_START_PER_HEIGHT:
        print("üßπ ÿ≠ÿßŸÑÿ™ Ÿæÿß⁄©ÿ≥ÿßÿ≤€å ŸÅÿπÿßŸÑ ÿ¥ÿØ: ÿ®ÿ±ÿß€å Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπÿå ŸæŸàÿ¥Ÿá ÿÆÿ±Ÿàÿ¨€å ÿ¢ŸÜ ÿßÿ±ÿ™ŸÅÿßÿπ Ÿæÿß⁄© ŸÖ€å‚Äåÿ¥ŸàÿØ.")

    # ------------------------------------------------------------
    # üë∑ ÿ™ÿπÿØÿßÿØ Ÿæÿ±ÿØÿßÿ≤Ÿá‚ÄåŸáÿß€å ŸÖŸàÿßÿ≤€å (ÿ®Ÿá€åŸÜŸá ÿ®ÿ±ÿß€å ÿ≥€åÿ≥ÿ™ŸÖ ÿ¥ŸÖÿß: 6 Ÿæ€åÿ¥ŸÜŸáÿßÿØ ŸÖ€å‚Äåÿ¥ŸàÿØ)
    # ------------------------------------------------------------
    cpu_count = os.cpu_count() or 8
    default_workers = min(max(2, cpu_count - 2), 8)  # ŸÖÿπŸÖŸàŸÑÿßŸã 6 ÿ™ÿß 8 ÿÆŸàÿ® ÿßÿ≥ÿ™
    w_in = input(f"ÿ™ÿπÿØÿßÿØ Ÿæÿ±ÿØÿßÿ≤Ÿá‚ÄåŸáÿß€å ŸáŸÖÿ≤ŸÖÿßŸÜ (Ÿæ€åÿ¥ŸÜŸáÿßÿØ€å {default_workers}): ").strip()
    if not w_in:
        max_workers = default_workers
    else:
        try:
            max_workers = int(w_in)
            max_workers = max(1, max_workers)
        except ValueError:
            max_workers = default_workers

    print(f"üß† CPU Threads: {cpu_count} | Ÿæÿ±ÿØÿßÿ≤Ÿá‚ÄåŸáÿß€å ŸáŸÖÿ≤ŸÖÿßŸÜ: {max_workers}")
    print("------------------------------------------------------------")

    # ------------------------------------------------------------
    # ‚úÖ ÿß⁄Øÿ± CLEAN_START_PER_HEIGHT=Trueÿå ŸÇÿ®ŸÑ ÿßÿ≤ ÿ≥ÿßÿÆÿ™ JobŸáÿß Ÿæÿß⁄© ⁄©ŸÜ
    # ------------------------------------------------------------
    if CLEAN_START_PER_HEIGHT:
        for h_val in heights:
            h_tag = height_tag(h_val)
            h_dir = os.path.join(dataDirBase, h_tag)
            if os.path.isdir(h_dir):
                print(f"üßπ ÿ≠ÿ∞ŸÅ ŸæŸàÿ¥Ÿá ÿßÿ±ÿ™ŸÅÿßÿπ {h_tag}: {h_dir}")
                shutil.rmtree(h_dir, ignore_errors=True)
            os.makedirs(h_dir, exist_ok=True)

    # ------------------------------------------------------------
    # ‚úÖ ÿ≥ÿßÿÆÿ™ ŸÑ€åÿ≥ÿ™ JobŸáÿß (ŸÅŸÇÿ∑ ÿ¢ŸÜ‚ÄåŸáÿß€å€å ⁄©Ÿá DONE ŸÜ€åÿ≥ÿ™ŸÜÿØ)
    # ------------------------------------------------------------
    jobs = []
    for h_val in heights:
        h_tag = height_tag(h_val)
        h_dir = os.path.join(dataDirBase, h_tag)
        os.makedirs(h_dir, exist_ok=True)

        for rec_file in all_records:
            record_name = os.path.splitext(rec_file)[0]
            rec_dir = os.path.join(h_dir, record_name)
            if not os.path.isfile(os.path.join(rec_dir, DONE_MARKER_NAME)):
                jobs.append({
                    "h_val": h_val,
                    "rec_file": rec_file,
                    "GMFolder": GMFolder,
                    "dataDirBase": dataDirBase,
                    "PROJECT_ROOT": PROJECT_ROOT,
                    "BASE_DIR": BASE_DIR,
                    "THA_DIR": THA_DIR,
                    "RUN_MODE": RUN_MODE,
                    "IS_LINEAR": IS_LINEAR,
                    "scaleFac": scaleFac,
                    "TFree": TFree
                })

    total_to_run = len(jobs)
    print("============================================================")
    print(f"üåç ⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá: ÿ±⁄©Ÿàÿ±ÿØŸáÿß={len(all_records)} | ÿßÿ±ÿ™ŸÅÿßÿπ‚ÄåŸáÿß={len(heights)}")
    print(f"üåç ŸÇÿßÿ®ŸÑ ÿßÿ¨ÿ±ÿß (ÿ®ÿØŸàŸÜ DONE): {total_to_run}")
    print("============================================================")

    if total_to_run == 0:
        print("‚úÖ ŸáŸÖŸá ÿ±⁄©Ÿàÿ±ÿØŸáÿß ŸÇÿ®ŸÑÿßŸã DONE ÿ¥ÿØŸá‚ÄåÿßŸÜÿØ. ⁄Ü€åÿ≤€å ÿ®ÿ±ÿß€å ÿßÿ¨ÿ±ÿß ÿ®ÿßŸÇ€å ŸÜŸÖÿßŸÜÿØŸá ÿßÿ≥ÿ™.")
        return

    # ------------------------------------------------------------
    # ‚úÖ ETA ÿ≥ÿ±ÿßÿ≥ÿ±€å
    # ------------------------------------------------------------
    EMA_ALPHA_GLOBAL = 0.2
    ema_total_global = None
    ema_model_global = None
    last100_total = deque(maxlen=100)
    last100_model = deque(maxlen=100)

    executed_ok = 0
    executed_fail = 0
    executed_skip = 0

    global_start = time.perf_counter()

    # ÿ®ÿ±ÿß€å ⁄Øÿ≤ÿßÿ±ÿ¥ failed per height ŸáŸÖ ŸÅÿß€åŸÑ ŸÖ€å‚Äåÿ≥ÿßÿ≤€åŸÖ:
    failed_map = {height_tag(h): [] for h in heights}

    # ------------------------------------------------------------
    # üöÄ ÿßÿ¨ÿ±ÿß€å ŸÖŸàÿßÿ≤€å
    # ------------------------------------------------------------
    print("üöÄ ÿ¥ÿ±Ÿàÿπ ÿßÿ¨ÿ±ÿß€å ŸÖŸàÿßÿ≤€å...")
    with ProcessPoolExecutor(max_workers=max_workers) as ex:
        futures = [ex.submit(run_one_job, j) for j in jobs]

        for idx, fu in enumerate(as_completed(futures), start=1):
            res = fu.result()

            st = res["status"]
            h_val = res["h_val"]
            rec_file = res["rec_file"]
            rec_name = res["record_name"]

            if st == "OK":
                executed_ok += 1

                total_sec = res["total_sec"]
                model_sec = res["model_sec"]

                last100_total.append(total_sec)
                last100_model.append(model_sec)

                ema_total_global = ema_update(ema_total_global, total_sec, alpha=EMA_ALPHA_GLOBAL)
                ema_model_global = ema_update(ema_model_global, model_sec, alpha=EMA_ALPHA_GLOBAL)

                remain = max(0, total_to_run - (executed_ok + executed_fail))
                avg_total = ema_total_global if ema_total_global is not None else total_sec
                eta_sec = remain * avg_total
                finish_time = datetime.now() + timedelta(seconds=eta_sec)

                print(f"‚úÖ DONE | {idx}/{total_to_run} | H={h_val} | {rec_name}")
                print(f"üåç ⁄©ŸÑ Ÿæÿ±Ÿà⁄òŸá: OK={executed_ok} | FAIL={executed_fail} | ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá‚âà{remain} | ETA(ALL): {fmt_seconds(eta_sec)} ‚Üí {finish_time.strftime('%Y-%m-%d %H:%M:%S')}")
                if len(last100_total) == 100:
                    print(f"üìä €±€∞€∞ ÿßÿ¨ÿ±ÿß€å ŸÇÿ®ŸÑ€å: ⁄©ŸÑ={fmt_seconds(sum(last100_total))} | ŸÖÿØŸÑ/ÿ™ÿ≠ŸÑ€åŸÑ={fmt_seconds(sum(last100_model))}")

            elif st == "FAIL":
                executed_fail += 1
                ht = height_tag(h_val)
                failed_map[ht].append(rec_file)
                print(f"‚ùå FAIL | {idx}/{total_to_run} | H={h_val} | {rec_file} | {res.get('error','')}")

            elif st == "SKIP":
                executed_skip += 1
                print(f"‚è≠Ô∏è  SKIP (DONE) | H={h_val} | {rec_name}")

    elapsed = time.perf_counter() - global_start
    print("============================================================")
    print("üèÅ Ÿæÿß€åÿßŸÜ ÿßÿ¨ÿ±ÿß€å ŸÖŸàÿßÿ≤€å")
    print(f"‚è±Ô∏è ÿ≤ŸÖÿßŸÜ ÿ≥Ÿæÿ±€å‚Äåÿ¥ÿØŸá: {fmt_seconds(elapsed)}")
    print(f"‚úÖ ŸÖŸàŸÅŸÇ: {executed_ok} | ‚ùå ŸÜÿßŸÖŸàŸÅŸÇ: {executed_fail} | ‚è≠Ô∏è ÿßÿ≥⁄©€åŸæ: {executed_skip}")
    print("============================================================")

    # ------------------------------------------------------------
    # ÿ∞ÿÆ€åÿ±Ÿá failed_records ÿ®ÿ±ÿß€å Ÿáÿ± ÿßÿ±ÿ™ŸÅÿßÿπ
    # ------------------------------------------------------------
    for h_val in heights:
        h_tag = height_tag(h_val)
        dataDirOut = os.path.join(dataDirBase, h_tag)
        failed_suffix = f"_{RUN_MODE}_{'linear' if IS_LINEAR else 'nonlinear'}_{h_tag}"
        failed_file_name = os.path.join(dataDirOut, f"failed_records{failed_suffix}.txt")
        with open(failed_file_name, "w", encoding="utf-8") as f:
            for rec in failed_map[h_tag]:
                f.write(f"{rec}\n")

        print(f"üìå ÿßÿ±ÿ™ŸÅÿßÿπ {h_tag}: ‚ùå ŸÜÿßŸÖŸàŸÅŸÇ={len(failed_map[h_tag])} | üìÑ ŸÑÿß⁄Ø: {failed_file_name}")

if __name__ == "__main__":
    # ÿ®ÿ±ÿß€å Ÿà€åŸÜÿØŸàÿ≤
    mp.freeze_support()
    main()




