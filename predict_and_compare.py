


# -*- coding: utf-8 -*-
"""
File name      : predict_and_compare.py
Author         : pc22
Created on     : Sat Dec 27 13:09:18 2025
Last modified  : Sat Dec 27 13:09:18 2025
------------------------------------------------------------
Purpose:
    Prediction and post-processing of structural response
    time histories using trained LSTM models, with support
    for linear and nonlinear responses, clustered and
    non-clustered training configurations, and multi-height
    global models.

    The script is intended to evaluate trained models under
    unseen earthquake records and to generate quantitative
    and visual performance metrics.
------------------------------------------------------------
Description:
    This script performs time-history prediction of structural
    responses using previously trained LSTM models.

    The script automatically detects available training
    configurations (clustered / non-clustered) and allows
    the user to select the desired model set at runtime.

    Two prediction modes are supported:
      1) Height-specific prediction:
         - Each structural height uses its own trained model.
         - Input features: ground motion only (X = [GM]).
      2) Global multi-height prediction:
         - A single model trained on all heights is used.
         - Structural height is added as an explicit feature
           (X = [GM, H]).

    For each selected height and training scenario, the script:
      - Loads trained models and corresponding scalers
      - Predicts response time histories
      - Applies bias correction
      - Computes performance metrics (RMSE, correlation
        coefficient, peak error)
      - Generates response comparison plots
      - Produces normalized error PDFs
      - Exports a summary of metrics to Excel files

    The script is fully non-interactive in terms of plotting
    (no GUI backend) and is suitable for batch execution.
------------------------------------------------------------
Inputs:
    - Trained LSTM models:
        LSTM.keras
      Stored under:
        Progress_of_LSTM_linear/
        Progress_of_LSTM_nonlinear/
        (clustered or noCluster subfolders)

    - Scalers associated with training:
        scaler_X_*.pkl
        scaler_Y_*.pkl

    - Prediction datasets:
        X_data_H*.npy   (GM inputs)
        Y_data_H*.npy   (reference responses)

    - Raw ground motion records for plotting:
        Output/1_IDA_Records_predict/

    - User inputs at runtime:
        * Linear vs. nonlinear prediction
        * Training configuration (cluster / noCluster)
        * Global vs. per-height model usage
        * Heights to be predicted
        * Training scenarios to evaluate
------------------------------------------------------------
Outputs:
    - Predicted vs. true response plots (PNG) for each:
        height Ã— scenario Ã— earthquake

    - Error probability density functions (PDFs):
        error_pdf_all_scenarios.png

    - Excel summary of performance metrics:
        metrics_summary.xlsx
      Including:
        RMSE, correlation coefficient (CC),
        and peak response error (%)

    - Organized output directory structure:
        Output/predict_linear/
        Output/predict_nonlinear/
        (mirroring training configuration)
------------------------------------------------------------
Changes since previous version:
    - Added automatic detection of clustered and non-clustered
      training directories.
    - Enabled prediction using global multi-height models.
    - Improved output directory safety (no deletion of
      previous prediction results).
    - Added unified Excel reporting for quantitative metrics.
------------------------------------------------------------
Impact of changes:
    - Enables systematic and reproducible comparison of
      different training scenarios and architectures.
    - Simplifies post-processing and result interpretation
      for thesis and publication purposes.
    - Improves robustness when running large-scale
      prediction studies across multiple heights and models.
------------------------------------------------------------
Status:
    Stable (Research / Evaluation phase)

------------------------------------------------------------
Notes:
    - Bias correction is applied to predicted responses
      before error evaluation.
    - All plots are generated using a non-GUI backend and
      saved directly to disk.
    - The script assumes that all required training and
      preprocessing steps have been completed beforehand.
"""


# -*- coding: utf-8 -*-
import os
import shutil
import numpy as np
import matplotlib
matplotlib.use("Agg")  # âœ… backend Ø¨Ø¯ÙˆÙ† GUI
import matplotlib.pyplot as plt
plt.ioff()             # âœ… Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ø­Ø§Ù„Øª ØªØ¹Ø§Ù…Ù„ÛŒ

import tensorflow as tf
import joblib
import pandas as pd    # Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ xlsx

# ============================================================== #
# ğŸ“ Ù…Ø³ÛŒØ±Ù‡Ø§ + Ø³ÙˆØ§Ù„ Ø®Ø·ÛŒ / ØºÛŒØ±Ø®Ø·ÛŒ
# ============================================================== #
base_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.abspath(os.path.join(base_dir, os.pardir))

choice = input("Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø®Ø·ÛŒØŸ (1=Ø®Ø·ÛŒ / 0=ØºÛŒØ±Ø®Ø·ÛŒ): ").strip()
is_linear = (choice == "1")

if is_linear:
    print("ğŸ“Œ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„ Ø®Ø·ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

    # âœ… ÙÙ‚Ø· Ø§ÛŒÙ† Ø®Ø· ØªØºÛŒÛŒØ± Ú©Ø±Ø¯: Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² Output Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
    base_model_root = os.path.join(root_dir, "Output", "Progress_of_LSTM_linear")

    gm_root_dir     = os.path.join(root_dir, "Output", "3_GM_Fixed_predict_linear")
    tha_root_dir    = os.path.join(root_dir, "Output", "3_THA_Fixed_predict_linear")
    gm_raw_dir      = os.path.join(root_dir, "Output", "1_IDA_Records_predict", "zire ham")
    output_base_root = os.path.join(root_dir, "Output", "predict_linear")
else:
    print("ğŸ“Œ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„ ØºÛŒØ±Ø®Ø·ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

    # âœ… ÙÙ‚Ø· Ø§ÛŒÙ† Ø®Ø· ØªØºÛŒÛŒØ± Ú©Ø±Ø¯: Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² Output Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
    base_model_root = os.path.join(root_dir, "Output", "Progress_of_LSTM_nonlinear")

    gm_root_dir     = os.path.join(root_dir, "Output", "3_GM_Fixed_predict_nonlinear")
    tha_root_dir    = os.path.join(root_dir, "Output", "3_THA_Fixed_predict_nonlinear")
    gm_raw_dir      = os.path.join(root_dir, "Output", "1_IDA_Records_predict", "zire ham")
    output_base_root = os.path.join(root_dir, "Output", "predict_nonlinear")

if not os.path.isdir(base_model_root):
    raise FileNotFoundError(f"âŒ Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {base_model_root}")

# ============================================================== #
# ğŸ›ï¸ Ø§Ù†ØªØ®Ø§Ø¨ Ù¾ÙˆØ´Ù‡Ù” Ø¢Ù…ÙˆØ²Ø´ (cluster / noCluster / Ù‚Ø¯ÛŒÙ…ÛŒ)
# ============================================================== #
train_config_dirs = [
    d for d in os.listdir(base_model_root)
    if os.path.isdir(os.path.join(base_model_root, d))
    and ("cluster" in d.lower() or "nocluster" in d.lower())
]

if train_config_dirs:
    print("\nğŸ“‚ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…ÙˆØ¬ÙˆØ¯ (cluster / noCluster) Ø²ÛŒØ±:")
    print("   ", base_model_root)
    for i, d in enumerate(sorted(train_config_dirs)):
        print(f"  [{i}] {d}")

    train_config_dirs = sorted(train_config_dirs)

    if len(train_config_dirs) == 1:
        selected_train_dir = train_config_dirs[0]
        print(f"\nâœ… ÙÙ‚Ø· ÛŒÚ© Ù¾ÙˆØ´Ù‡ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ØŒ Ù‡Ù…Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: {selected_train_dir}")
    else:
        sel = input(
            "\nÙ†Ø§Ù… ÛŒØ§ Ø´Ù…Ø§Ø±Ù‡Ù” Ù¾ÙˆØ´Ù‡Ù” Ø¢Ù…ÙˆØ²Ø´ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† "
            "(Ù…Ø«Ø§Ù„: 0 ÛŒØ§ clusterK4_allHeights ØŒ Ø®Ø§Ù„ÛŒ = 0): "
        ).strip()

        if sel == "":
            idx = 0
        elif sel.isdigit():
            idx = int(sel)
            if idx < 0 or idx >= len(train_config_dirs):
                print("âš ï¸ Ø´Ù…Ø§Ø±Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯ØŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 0 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
                idx = 0
        else:
            if sel in train_config_dirs:
                idx = train_config_dirs.index(sel)
            else:
                print("âš ï¸ Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 0 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
                idx = 0

        selected_train_dir = train_config_dirs[idx]

    model_root_dir = os.path.join(base_model_root, selected_train_dir)
    output_root    = os.path.join(output_base_root, selected_train_dir)

    print("\nğŸ“‚ Ù¾ÙˆØ´Ù‡Ù” Ø¢Ù…ÙˆØ²Ø´ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡:")
    print("   ", model_root_dir)
    print("ğŸ“‚ Ù¾ÙˆØ´Ù‡Ù” Ø®Ø±ÙˆØ¬ÛŒ Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§:")
    print("   ", output_root)
else:
    model_root_dir = base_model_root
    output_root    = output_base_root

    print("\nğŸ“‚ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡Ù” cluster/noCluster Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
    print("   Ø§Ø² Ù‡Ù…ÛŒÙ† Ù…Ø³ÛŒØ± Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† root Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:")
    print("   ", model_root_dir)
    print("ğŸ“‚ Ù¾ÙˆØ´Ù‡Ù” Ø®Ø±ÙˆØ¬ÛŒ:")
    print("   ", output_root)

# ğŸ“‚ Ø±ÛŒØ´Ù‡â€ŒÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒ (Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ ÙÛŒÚ†Ø± H)
global_multi_root = os.path.join(model_root_dir, "Global_training_with_height")

# â“ Ø§Ù†ØªØ®Ø§Ø¨ Ù†ÙˆØ¹ Ù…Ø¯Ù„: Ú©Ù„ÛŒ (multi-height) ÛŒØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹
use_global_model = False
if os.path.isdir(global_multi_root):
    ans = input("Ø§Ø² Ù…Ø¯Ù„ Ú©Ù„ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ø§ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯ØŸ (1=Ù…Ø¯Ù„ Ú©Ù„ÛŒ / 0=Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡): ").strip()
    use_global_model = (ans == "1")
    if use_global_model:
        print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ú©Ù„ÛŒ (Global_training_with_height) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ.")
    else:
        print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡â€ŒÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹.")
else:
    print("âš ï¸ Ù¾ÙˆØ´Ù‡ Global_training_with_height ÛŒØ§ÙØª Ù†Ø´Ø¯Ø› ÙÙ‚Ø· Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡â€ŒÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù‡Ø³ØªÙ†Ø¯.")
    use_global_model = False

os.makedirs(output_root, exist_ok=True)

if use_global_model:
    os.makedirs(os.path.join(output_root, "Global_training_with_height"), exist_ok=True)

# ============================================================== #
# ğŸ” ØªØ¹ÛŒÛŒÙ† Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø§Ø² Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (GM)
# ============================================================== #
if not os.path.isdir(gm_root_dir):
    raise FileNotFoundError(f"âŒ Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ GM Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {gm_root_dir}")

height_tags = sorted(
    d for d in os.listdir(gm_root_dir)
    if os.path.isdir(os.path.join(gm_root_dir, d)) and d.startswith("H")
)

if not height_tags:
    raise RuntimeError(f"âŒ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡ Ø§Ø±ØªÙØ§Ø¹ (H*) Ø²ÛŒØ± {gm_root_dir} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")

print("\nğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (GM):")
for h in height_tags:
    print("  -", h)

def height_value_from_tag(h_tag: str) -> float:
    s = h_tag[1:]
    s = s.replace('p', '.')
    return float(s)

use_all_heights = input("\nØ¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŸ (y/n): ").strip().lower() == "y"

if not use_all_heights:
    print("Ù…Ø«Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ:  H3 H4  ÛŒØ§  H3")
    h_items = input("Ù†Ø§Ù… Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†: ").strip().split()
    selected_heights = []
    for h in h_items:
        if h in height_tags:
            selected_heights.append(h)
        else:
            print(f"âš ï¸ Ø§Ø±ØªÙØ§Ø¹ {h} Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
    selected_heights = list(dict.fromkeys(selected_heights))
    if not selected_heights:
        raise ValueError("âŒ Ù‡ÛŒÚ† Ø§Ø±ØªÙØ§Ø¹ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯.")
else:
    selected_heights = height_tags

print("\nâœ… Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ:")
for h in selected_heights:
    print("   â†’", h)
print()

# ============================================================== #
# ğŸ“¥ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ GM Ø®Ø§Ù… (Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… Ù¾Ø§Ø³Ø® Ø²Ù…Ø§Ù†ÛŒ)
# ============================================================== #
gm_files = sorted(os.listdir(gm_raw_dir))
print(f"ğŸ“Œ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ GM Ø®Ø§Ù…: {len(gm_files)}\n")

# ==============================================================
# âœ… CHANGE 1: Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø´ÙˆÙ†Ø¯ (Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹)
#   - Ù…Ù†Ø¨Ø¹ Ù„ÛŒØ³Øª Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§:
#       * Ø§Ú¯Ø± global model: Ø§Ø² global_multi_root
#       * Ø§Ú¯Ø± per-height: Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡
# ============================================================== #
if use_global_model:
    scenario_base_dir_global = global_multi_root
else:
    scenario_base_dir_global = os.path.join(model_root_dir, selected_heights[0])

scenario_dirs_global = sorted(
    d for d in os.listdir(scenario_base_dir_global)
    if os.path.isdir(os.path.join(scenario_base_dir_global, d)) and d.startswith("ep")
)

if not scenario_dirs_global:
    raise RuntimeError("âŒ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ (Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ÛŒÙ‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§).")

print("\nğŸ“‚ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ (ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯):")
for i, nm in enumerate(scenario_dirs_global):
    print(f"  {i}. {nm}")

run_all_scen_global = input("\nÙ‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ Ø§Ø¬Ø±Ø§ Ø´ÙˆÙ†Ø¯ØŸ (y/n): ").strip().lower() == "y"

if not run_all_scen_global:
    print("\nÙ…Ø«Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§:   0 3 6 9")
    print("Ù…Ø«Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù…â€ŒÙ‡Ø§:     ep100_A1.0_T0.50 ep60_A0.5_T0.20")
    print("ÛŒØ§ ØªØ±Ú©ÛŒØ¨ÛŒ:             0 ep60_A0.5_T0.20 7")

    scen_items = input("Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ ÛŒØ§ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†: ").strip().split()

    selected_scen_global = []
    invalid_scen = []

    for item in scen_items:
        if item.isdigit():
            idx = int(item)
            if 0 <= idx < len(scenario_dirs_global):
                selected_scen_global.append(scenario_dirs_global[idx])
            else:
                invalid_scen.append(item)
        else:
            if item in scenario_dirs_global:
                selected_scen_global.append(item)
            else:
                invalid_scen.append(item)

    selected_scen_global = list(dict.fromkeys(selected_scen_global))

    if not selected_scen_global:
        raise ValueError("âŒ Ù‡ÛŒÚ† Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ ÙˆØ§Ø±Ø¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

    if invalid_scen:
        print("\nâš ï¸ Ù…ÙˆØ§Ø±Ø¯ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù†Ø¯:")
        for itm in invalid_scen:
            print("  -", itm)
else:
    selected_scen_global = scenario_dirs_global

print("\nâœ… Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ (Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§):")
for s in selected_scen_global:
    print("   -", s)
print()

# ============================================================== #
# ğŸ” Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§
# ============================================================== #
for h_tag in selected_heights:

    print("\n" + "#" * 80)
    print(f"ğŸ—ï¸ Ø´Ø±ÙˆØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹ Ø³ØªÙˆÙ†: {h_tag}")
    print("#" * 80)

    x_data_path = os.path.join(gm_root_dir,  h_tag, f"X_data_{h_tag}.npy")
    y_data_path = os.path.join(tha_root_dir, h_tag, f"Y_data_{h_tag}.npy")

    if not os.path.exists(x_data_path):
        print(f"âŒ X_data Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {x_data_path} â†’ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n")
        continue
    if not os.path.exists(y_data_path):
        print(f"âŒ Y_data Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {y_data_path} â†’ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n")
        continue

    # Ø§Ø³Ú©ÛŒÙ„Ø±Ù‡Ø§
    if use_global_model:
        if is_linear:
            scaler_x_path = os.path.join(global_multi_root, "scaler_X_linear.pkl")
            scaler_y_path = os.path.join(global_multi_root, "scaler_Y_linear.pkl")
        else:
            scaler_x_path = os.path.join(global_multi_root, "scaler_X_nonlinear.pkl")
            scaler_y_path = os.path.join(global_multi_root, "scaler_Y_nonlinear.pkl")
    else:
        if is_linear:
            scaler_x_path = os.path.join(model_root_dir, h_tag, "scaler_X_linear.pkl")
            scaler_y_path = os.path.join(model_root_dir, h_tag, "scaler_Y_linear.pkl")
        else:
            scaler_x_path = os.path.join(model_root_dir, h_tag, "scaler_X_nonlinear.pkl")
            scaler_y_path = os.path.join(model_root_dir, h_tag, "scaler_Y_nonlinear.pkl")

    if not os.path.exists(scaler_x_path) or not os.path.exists(scaler_y_path):
        print(f"âŒ Ø§Ø³Ú©ÛŒÙ„Ø±Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯Ù†Ø¯ â†’ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n")
        continue

    if use_global_model:
        output_h_dir = os.path.join(output_root, "Global_training_with_height", h_tag)
    else:
        output_h_dir = os.path.join(output_root, h_tag)
    os.makedirs(output_h_dir, exist_ok=True)

    print("ğŸ”„ Loading scalers for", h_tag)
    scaler_X = joblib.load(scaler_x_path)
    scaler_Y = joblib.load(scaler_y_path)
    print("âœ… Scalers loaded.\n")

    X_data = np.load(x_data_path, allow_pickle=True).item()
    Y_data = np.load(y_data_path, allow_pickle=True).item()

    keys = sorted(X_data.keys())

    Y_list = [np.asarray(Y_data[k], dtype=np.float32).reshape(-1, 1) for k in keys]

    if use_global_model:
        h_val = np.float32(height_value_from_tag(h_tag))
        X_list = []
        for k in keys:
            x_gm = np.asarray(X_data[k], dtype=np.float32).reshape(-1, 1)
            T = x_gm.shape[0]
            h_col = np.full((T, 1), h_val, dtype=np.float32)
            x_feat = np.concatenate([x_gm, h_col], axis=1)
            X_list.append(x_feat)
    else:
        X_list = [np.asarray(X_data[k], dtype=np.float32).reshape(-1, 1) for k in keys]

    X_scaled_list = [scaler_X.transform(x) for x in X_list]

    num_to_plot = min(len(keys), len(gm_files))

    print(f"ğŸ“Œ {h_tag} â†’ ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù¾Ø§Ø³Ø®: {len(keys)}")
    print(f"ğŸ“Œ {h_tag} â†’ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ù†Ø§Ø±ÛŒÙˆ: {num_to_plot}\n")

    # ============================================================== #
    # âœ… CHANGE 2: Ù…Ø­ÙˆØ± Ù‚Ø§Ø¦Ù… ÛŒÚ©Ø³Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ (Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹)
    #   Ø±Ø§Ù‡Ú©Ø§Ø±: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ØŒ Ø§Ø¨ØªØ¯Ø§ Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…ØŒ
    #   min/max Ø±Ø§ Ø§Ø² True Ùˆ Pred Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù…Ø´ØªØ±Ú© Ø­Ø³Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ
    #   Ø¨Ø¹Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø§ ylim Ø«Ø§Ø¨Øª Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
    # ============================================================== #

    # Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÙ‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹
    if use_global_model:
        scenario_base_dir = global_multi_root
    else:
        scenario_base_dir = os.path.join(model_root_dir, h_tag)

    # Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ù…ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø¹Ø¶ÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ù†Ø¯)
    available_scen_for_height = sorted(
        d for d in os.listdir(scenario_base_dir)
        if os.path.isdir(os.path.join(scenario_base_dir, d)) and d.startswith("ep")
    )

    # ÙÛŒÙ„ØªØ±: ÙÙ‚Ø· Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù‡ Ùˆ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø§ÛŒÙ†Ø¬Ø§ Ù…ÙˆØ¬ÙˆØ¯Ù†Ø¯
    selected_scen = [s for s in selected_scen_global if s in available_scen_for_height]

    if not selected_scen:
        print(f"âŒ Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù… Ø§Ø² Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {h_tag} Ø¯Ø± Ù…Ø³ÛŒØ± {scenario_base_dir} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯. Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n")
        continue

    print("âœ… Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø±Ø§ÛŒ", h_tag, ":")
    for s in selected_scen:
        print("   -", s)
    print()

    # ---------------------------------------------------------- #
    # ğŸ“ Ø§Ú©Ø³Ù„ + Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø®Ø·Ø§
    # ---------------------------------------------------------- #
    excel_rows = []
    excel_columns = [
        "Scenario", "Earthquake", "Epochs", "Alpha", "Thresh",
        "RMSE", "CC", "PeakErr"
    ]
    scenario_errors = {}

    # ---------------------------------------------------------- #
    # âœ… PASS 1: Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ + Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ùˆ min/max
    # ---------------------------------------------------------- #
    results_by_scen = {}   # scen -> list of dict per record
    global_ymin = +np.inf
    global_ymax = -np.inf

    num_local = min(num_to_plot, len(Y_list), len(X_scaled_list), len(gm_files))

    for scen_name in selected_scen:

        if use_global_model:
            model_dir = os.path.join(global_multi_root, scen_name)
        else:
            model_dir = os.path.join(scenario_base_dir, scen_name)

        model_path = os.path.join(model_dir, "LSTM.keras")
        if not os.path.exists(model_path):
            print(f"âš ï¸ Ù…Ø¯Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {model_path} â†’ Ø§ÛŒÙ† Ø³Ù†Ø§Ø±ÛŒÙˆ Ø¨Ø±Ø§ÛŒ {h_tag} Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            continue

        # parse Ø³Ù†Ø§Ø±ÛŒÙˆ
        try:
            parts = scen_name.split("_")
            epochs_val = int(parts[0].replace("ep", ""))
            alpha_val  = float(parts[1].replace("A", ""))
            thresh_val = float(parts[2].replace("T", ""))
        except Exception:
            epochs_val = alpha_val = thresh_val = None

        model = tf.keras.models.load_model(model_path, compile=False)

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        Y_pred_list = []
        for x_sc in X_scaled_list:
            pred = model.predict(x_sc[np.newaxis, ...], verbose=0)[0]
            Y_pred_list.append(pred)

        Y_pred_list = [scaler_Y.inverse_transform(y) for y in Y_pred_list]
        Y_true_list = [y.astype(np.float32) for y in Y_list]

        # Bias correction (Ù‡Ù…Ø§Ù† Ù…Ù†Ø·Ù‚ Ø®ÙˆØ¯Øª)
        Y_true_concat = np.concatenate(Y_true_list, axis=0)
        Y_pred_concat = np.concatenate(Y_pred_list, axis=0)
        bias = np.mean(Y_true_concat) - np.mean(Y_pred_concat)
        Y_pred_bc_list = [y + bias for y in Y_pred_list]

        # Ø°Ø®ÛŒØ±Ù‡ Ø±Ú©ÙˆØ±Ø¯-Ø¨Ù‡-Ø±Ú©ÙˆØ±Ø¯ + Ø¢Ù¾Ø¯ÛŒØª ymin/ymax
        per_records = []
        for i in range(num_local):
            gm_file = gm_files[i]
            gm_name = os.path.splitext(gm_file)[0]

            y_true = Y_true_list[i].flatten()
            y_pred = Y_pred_bc_list[i].flatten()

            # Ù‡Ù…â€ŒØ·ÙˆÙ„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ GM Ø®Ø§Ù… (Ù‡Ù…Ø§Ù† Ù…Ù†Ø·Ù‚ Ø®ÙˆØ¯Øª)
            gm_path = os.path.join(gm_raw_dir, gm_file)
            gm_raw = np.loadtxt(gm_path)

            L = min(len(gm_raw), len(y_true), len(y_pred))
            y_true = y_true[:L]
            y_pred = y_pred[:L]

            # update global y-limits for THIS HEIGHT
            local_min = float(min(np.min(y_true), np.min(y_pred)))
            local_max = float(max(np.max(y_true), np.max(y_pred)))
            global_ymin = min(global_ymin, local_min)
            global_ymax = max(global_ymax, local_max)

            # metrics (Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø³Ù„)
            rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
            cc   = np.corrcoef(y_true, y_pred)[0, 1]
            peak = (
                np.abs(np.max(np.abs(y_pred)) - np.max(np.abs(y_true)))
                / (np.max(np.abs(y_true)) + 1e-12)
                * 100.0
            )

            peak_true = np.max(np.abs(y_true)) + 1e-12
            norm_err_series = (y_pred - y_true) / peak_true * 100.0

            per_records.append({
                "i": i,
                "gm_name": gm_name,
                "y_true": y_true,
                "y_pred": y_pred,
                "rmse": rmse,
                "cc": cc,
                "peak": peak,
                "norm_err_series": norm_err_series
            })

            excel_rows.append({
                "Scenario": scen_name,
                "Earthquake": gm_name,
                "Epochs": epochs_val,
                "Alpha": alpha_val,
                "Thresh": thresh_val,
                "RMSE": rmse,
                "CC": cc,
                "PeakErr": peak
            })

        # Ø®Ø·Ø§Ù‡Ø§ Ø¨Ø±Ø§ÛŒ PDF
        all_norm_errors = np.concatenate([r["norm_err_series"] for r in per_records], axis=0)
        if all_norm_errors.size > 0:
            scenario_errors[scen_name] = all_norm_errors.astype(np.float32)

        results_by_scen[scen_name] = {
            "epochs": epochs_val,
            "alpha": alpha_val,
            "thresh": thresh_val,
            "records": per_records
        }

        print(f"âœ… {h_tag} | Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªÙ…Ø§Ù… Ø´Ø¯: {scen_name}")

    if not results_by_scen:
        print(f"âŒ Ù‡ÛŒÚ† Ø³Ù†Ø§Ø±ÛŒÙˆÛŒÛŒ Ø¨Ø±Ø§ÛŒ {h_tag} Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ù†Ø¨ÙˆØ¯. Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n")
        continue

    # Ø­Ø§Ø´ÛŒÙ‡ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ
    pad = 0.05 * (global_ymax - global_ymin + 1e-12)
    global_ymin -= pad
    global_ymax += pad

    print(f"\nğŸ“Œ {h_tag} | y-limits Ù…Ø´ØªØ±Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§:")
    print(f"    ymin={global_ymin:.6g} , ymax={global_ymax:.6g}\n")

    # ---------------------------------------------------------- #
    # âœ… PASS 2: Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø¨Ø§ ylim Ù…Ø´ØªØ±Ú©
    # ---------------------------------------------------------- #
    for scen_name, payload in results_by_scen.items():

        print("\n" + "=" * 80)
        print(f"ğŸš€ {h_tag} | Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø¨Ø§ ylim Ø«Ø§Ø¨Øª: {scen_name}")
        print("=" * 80)

        scenario_output_dir = os.path.join(output_h_dir, scen_name)
        os.makedirs(scenario_output_dir, exist_ok=True)

        epochs_val = payload["epochs"]
        alpha_val  = payload["alpha"]
        thresh_val = payload["thresh"]

        print(f"ğŸ“Œ Epochs={epochs_val}, Alpha={alpha_val}, Thresh={thresh_val}")

        for rec in payload["records"]:
            i = rec["i"]
            gm_name = rec["gm_name"]
            y_true = rec["y_true"]
            y_pred = rec["y_pred"]
            rmse = rec["rmse"]
            cc = rec["cc"]
            peak = rec["peak"]

            plt.figure(figsize=(12, 6))
            plt.plot(y_true, color="black", linewidth=0.4, label="True")
            plt.plot(y_pred, color="blue",  linewidth=0.4, label="Predicted")

            # âœ… Ù…Ø­ÙˆØ± Ù‚Ø§Ø¦Ù… Ù…Ø´ØªØ±Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù‡Ù…ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹
            plt.ylim(global_ymin, global_ymax)

            txt = f"{h_tag} - {scen_name}\nRMSE={rmse:.4f}  CC={cc:.4f}  PeakErr={peak:.2f}%"
            plt.text(
                0.98, 0.02, txt,
                transform=plt.gca().transAxes,
                ha="right",
                bbox=dict(facecolor='white', alpha=0.6)
            )

            plt.xlabel("Time step")
            plt.ylabel("Response")
            plt.grid(True)
            plt.legend()

            save_path = os.path.join(scenario_output_dir, f"{i:03d}_{gm_name}.png")
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            plt.close()

            print(f"âœ” {h_tag} | {scen_name} | Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {save_path}")

        print(f"âœ… {h_tag} | Ù¾Ø§ÛŒØ§Ù† Ø³Ù†Ø§Ø±ÛŒÙˆ: {scen_name}")

    # ---------------------------------------------------------- #
    # ğŸ“ˆ PDF Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹
    # ---------------------------------------------------------- #
    if scenario_errors:
        all_vals = np.concatenate(list(scenario_errors.values()), axis=0)
        xmin = np.percentile(all_vals, 1)
        xmax = np.percentile(all_vals, 99)
        dx = (xmax - xmin) * 0.1
        xmin -= dx
        xmax += dx

        num_bins = 80
        bins = np.linspace(xmin, xmax, num_bins + 1)

        plt.rcParams['axes.prop_cycle'] = plt.cycler(
            color=['#0072B2', '#D55E00', '#009E73', '#CC79A7', '#F0E442', '#56B4E9']
        )

        plt.figure(figsize=(9, 5))
        linestyles = ['-', '--', '-.', ':']

        for idx, (scen_name, err_arr) in enumerate(scenario_errors.items()):
            hist, bin_edges = np.histogram(err_arr, bins=bins, density=True)
            bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])
            plt.plot(
                bin_centers,
                hist,
                linewidth=0.9,
                linestyle=linestyles[idx % len(linestyles)],
                label=scen_name
            )

        plt.axvline(+10.0, linestyle="--", linewidth=0.9, color="k", label="+/-10% Error")
        plt.axvline(-10.0, linestyle="--", linewidth=0.9, color="k")

        plt.xlabel("Normalized error (%)")
        plt.ylabel("PDF")
        plt.title(f"{h_tag} - Normalized Error PDF - All Scenarios")
        plt.grid(True, alpha=0.4)
        plt.legend(fontsize=7)

        pdf_all_path = os.path.join(output_h_dir, "error_pdf_all_scenarios.png")
        plt.savefig(pdf_all_path, dpi=300, bbox_inches="tight")
        plt.close()

        print("\nğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± PDF Ù‡Ù…Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
        print("   â†’", os.path.abspath(pdf_all_path))

    # ---------------------------------------------------------- #
    # ğŸ“Š Ø°Ø®ÛŒØ±Ù‡ Ø§Ú©Ø³Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹
    # ---------------------------------------------------------- #
    df = pd.DataFrame(excel_rows, columns=excel_columns)
    if not df.empty:
        df = df.sort_values(by="CC", ascending=False)

    excel_path = os.path.join(output_h_dir, "metrics_summary.xlsx")
    df.to_excel(excel_path, index=False)

    print("\nğŸ“‘ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø±ØªÙØ§Ø¹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
    print("   â†’", os.path.abspath(excel_path))
    print(f"\nğŸ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹ {h_tag} ØªÙ…Ø§Ù… Ø´Ø¯.\n")

print("ğŸ‰ ØªÙ…Ø§Ù… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ùˆ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")













