# =============================================================================
# # -*- coding: utf-8 -*-
# """
# File name      : Peak_prediction_For_ghab_GPU-Clusttering-v04.py
# Author         : pc22
# Created on     : Wed Dec 31 09:xx:xx 2025
# Last modified  : Sun Jan 04 14:xx:xx 2026
# ------------------------------------------------------------
# Purpose:
#     Train a **peak-only** surrogate model to predict the **maximum
#     absolute displacement** of the structure directly from the input
#     ground-motion (GM) acceleration time series, without predicting the
#     full displacement time-history.
# 
#     In v04, the target is defined as a scalar per record:
#         y = max(|disp(t)|)
# 
#     This version is designed as a stable and fast alternative to the
#     sequence-to-sequence LSTM (v03) when only the peak response is
#     needed.
# 
#     Key goals in v04:
#       (1) Keep your existing input pipeline unchanged:
#             - X is still loaded exactly as before from X_data_H*.npy
#             - Y is still loaded exactly as before from Y_data_H*.npy
#           (Y is used to compute the true peak target.)
#       (2) Use a robust tabular regression model:
#             - Primary: LightGBMRegressor (if installed)
#             - Fallback: strong sklearn regressor if LightGBM is unavailable
#       (3) Preserve the same training configuration logic:
#             - Linear vs Nonlinear datasets
#             - Clustered vs Non-clustered data
#             - Multi-height global model (with H feature) vs Per-height models
#       (4) Maintain experiment traceability:
#             - Version-tagged, compact output folders (e.g., V04)
#             - runinfo.txt created early + finalized
#             - saved split indices for reproducibility
# ------------------------------------------------------------
# Description:
#     The script loads preprocessed GM/THA time-series pairs generated by
#     previous pipeline steps:
# 
#       - GM inputs:
#           X_data_H*.npy (dict: record_id -> accel(t))
#       - THA responses (reference):
#           Y_data_H*.npy (dict: record_id -> disp(t))
# 
#     Target construction (per record):
#         y_true = max(abs(disp(t)))
# 
#     Feature construction (per record):
#       - A compact, stable feature vector is extracted from accel(t).
#         (Examples: PGA, RMS, energy, percentiles, duration proxies,
#          zero-crossing rate, simple spectral/FFT-derived summaries.)
#       - If training in global multi-height mode, height H is appended
#         as an additional feature:
#             X_feat = [features_from_GM, H]
# 
#     Model training:
#       - Trains either:
#           (1) Global multi-height model (USE_MULTI_HEIGHT=1):
#               One model for all selected heights (with H feature)
#           (2) Per-height independent models (USE_MULTI_HEIGHT=0):
#               Separate model per height
# 
#     Data split & reproducibility:
#       - Uses a fixed seed to generate split indices.
#       - Saves and reloads split indices under a META folder to ensure
#         identical train/val/test partitions across reruns.
# 
#     Experiment management:
#       - Outputs are organized under a short, version-tagged script folder
#         (e.g., V04) to avoid overwriting between code variants.
#       - Each run creates an isolated run folder and logs:
#           * model.pkl (trained regressor)
#           * metrics.json (MAE/RMSE/R2)
#           * runinfo.txt (human-readable run configuration; created early and updated)
#           * diagnostic plots (true vs pred, error histogram)
# ------------------------------------------------------------
# Inputs:
#     Data (from previous pipeline steps):
#       - GM time series:
#           Output/3_GM_Fixed_train_linear/H*/...
#           Output/3_GM_Fixed_train_nonlinear/H*/...
#       - THA time series:
#           Output/3_THA_Fixed_train_linear/H*/...
#           Output/3_THA_Fixed_train_nonlinear/H*/...
# 
#     Runtime user inputs (interactive):
#       - Linear vs Nonlinear mode selection
#       - Clustered vs Non-clustered dataset selection
#       - Heights to include in training (H2, H3, ...)
#       - Training mode: global multi-height vs per-height models
# 
#     (Optional / internal):
#       - Feature extraction settings (kept stable by default)
#       - Random seed for split reproducibility
# ------------------------------------------------------------
# Outputs:
#     All outputs are saved under:
# 
#       Output/
#         â”œâ”€â”€ Progress_of_MaxDisp_linear/
#         â”‚     â””â”€â”€ <script_tag>/                  (e.g., V04)
#         â”‚           â”œâ”€â”€ _META_<MODE>-<TRAIN>/    (split indices + meta)
#         â”‚           â”œâ”€â”€ <run_tag>/               (one folder per run)
#         â”‚           â”‚     â”œâ”€â”€ model.pkl
#         â”‚           â”‚     â”œâ”€â”€ metrics.json
#         â”‚           â”‚     â”œâ”€â”€ runinfo.txt
#         â”‚           â”‚     â”œâ”€â”€ y_true_vs_pred.png
#         â”‚           â”‚     â””â”€â”€ error_hist.png
#         â”‚           â””â”€â”€ ...
#         â””â”€â”€ Progress_of_MaxDisp_nonlinear/
#               â””â”€â”€ <script_tag>/
#                     ...
# 
#     where:
#       - <script_tag> is inferred from the executing file name:
#             * vXX is extracted if present (e.g., "v04" â†’ "V04")
#             * otherwise a short alphanumeric prefix is used
#       - <run_tag> is a compact code describing configuration, e.g.:
#             NC-GH__MAXABS__lightgbm
#         (cluster/noCluster + training mode + target + model type)
# ------------------------------------------------------------
# Changes since baseline / earlier code (v03):
#     1) Target changed from sequence prediction Y(t) to scalar peak:
#          y = max(|Y(t)|)
#     2) Replaced seq2seq LSTM training with a tabular regression model.
#     3) Added engineered feature extraction from GM time series.
#     4) Preserved linear/nonlinear + clustered/noCluster + GH/per-height logic.
#     5) Output management aligned with v03 philosophy:
#          - Version-tagged folder (V04)
#          - Compact run tags
#          - runinfo.txt lifecycle maintained
# ------------------------------------------------------------
# Impact of changes:
#     - Much faster training/inference when only peak response is required.
#     - Typically more stable optimization than seq2seq for peak-only tasks.
#     - Maintains reproducibility and traceability via saved split indices
#       and runinfo/metrics logging.
# ------------------------------------------------------------
# Status:
#     Stable (Training / Evaluation phase)
# ------------------------------------------------------------
# Notes:
#     - y_true is computed per record from the available THA displacement
#       time series: max(abs(disp)).
#     - Feature extraction is intentionally lightweight and stable to
#       generalize across many records and scenarios.
#     - If LightGBM is not installed, the script should automatically fall
#       back to a strong sklearn regressor while keeping the pipeline intact.
# """
# 
# 
# 
# 
# 
# 
# import sys, io
# if hasattr(sys.stdout, "buffer"):
#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')
# 
# import os, re, json, random
# import numpy as np
# import matplotlib.pyplot as plt
# 
# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# import joblib
# 
# plt.ioff()
# 
# # ==============================================================
# # ğŸ”’ Reproducibility
# # ==============================================================
# SEED = 1234
# os.environ["PYTHONHASHSEED"] = str(SEED)
# random.seed(SEED)
# np.random.seed(SEED)
# 
# # ==============================================================
# # ğŸ“ Paths + Linear / Nonlinear selection
# # ==============================================================
# base_dir = os.path.dirname(os.path.abspath(__file__))
# root_dir = os.path.abspath(os.path.join(base_dir, os.pardir))
# output_root_dir = os.path.join(root_dir, "Output")
# 
# choice = input(
#     "Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø³Ø® Ø®Ø·ÛŒ Ø¨Ø§Ø´Ø¯ ÛŒØ§ ØºÛŒØ±Ø®Ø·ÛŒØŸ "
#     "Ø¨Ø±Ø§ÛŒ Ø®Ø·ÛŒ Ø¹Ø¯Ø¯ 1 Ùˆ Ø¨Ø±Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ Ø¹Ø¯Ø¯ 0 Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†: "
# ).strip()
# is_linear = (choice == "1")
# 
# if is_linear:
#     print("ğŸ“Œ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ (THA_linear) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#     gm_root_dir  = os.path.join(output_root_dir, "3_GM_Fixed_train_linear")
#     tha_root_dir = os.path.join(output_root_dir, "3_THA_Fixed_train_linear")
#     base_model_root = os.path.join(output_root_dir, "Progress_of_MaxDisp_linear")
# else:
#     print("ğŸ“Œ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ (THA_nonlinear) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#     gm_root_dir  = os.path.join(output_root_dir, "3_GM_Fixed_train_nonlinear")
#     tha_root_dir = os.path.join(output_root_dir, "3_THA_Fixed_train_nonlinear")
#     base_model_root = os.path.join(output_root_dir, "Progress_of_MaxDisp_nonlinear")
# 
# os.makedirs(base_model_root, exist_ok=True)
# 
# print("ğŸ“‚ GM root dir :", gm_root_dir)
# print("ğŸ“‚ THA root dir:", tha_root_dir)
# print("ğŸ“‚ Base model root:", base_model_root)
# print()
# 
# # ==============================================================
# # ğŸ§¾ Script-tag isolation (like your original workflow)
# # ==============================================================
# script_name = os.path.splitext(os.path.basename(__file__))[0]
# 
# m = re.search(r"(v\d+)", script_name, flags=re.IGNORECASE)
# if m:
#     script_tag = m.group(1).upper()
# else:
#     script_tag = re.sub(r"[^A-Za-z0-9]+", "", script_name)[:12] or "SCRIPT"
# 
# # Separate by version tag first (V01/V02/...)
# base_model_root = os.path.join(base_model_root, script_tag)
# os.makedirs(base_model_root, exist_ok=True)
# 
# # ==============================================================
# # ğŸ” Clustered vs non-clustered
# # ==============================================================
# print("-------------------------------------------")
# print("Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ *Ú©Ù„Ø§Ø³ØªØ±Ø´Ø¯Ù‡* Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù…ØŸ")
# print("   1 = Ø¨Ù„Ù‡ØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ cluster_balanced_global Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†")
# print("   0 = Ø®ÛŒØ±ØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ X_data_H* Ùˆ Y_data_H* Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†")
# print("-------------------------------------------\n")
# 
# cluster_choice = input("Ø§Ù†ØªØ®Ø§Ø¨Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (1 ÛŒØ§ 0): ").strip()
# USE_CLUSTERED = (cluster_choice == "1")
# 
# cluster_label = ""
# if USE_CLUSTERED:
#     print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ØªØ±Ø´Ø¯Ù‡ (cluster_balanced_global).")
#     cluster_label = input(
#         "Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù… Ù¾ÙˆØ´Ù‡Ù” Ù…Ø¯Ù„ØŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³ØªØ±Ù‡Ø§ (K) Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ "
#         "(Ù…Ø«Ù„Ø§Ù‹ 4). Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒØŒ 'clustered' Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: "
#     ).strip()
# else:
#     print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø¯ÙˆÙ† Ú©Ù„Ø§Ø³ØªØ±ÛŒÙ†Ú¯.")
# 
# def get_mode_code():
#     if not USE_CLUSTERED:
#         return "NC"
#     if cluster_label:
#         return f"CK{cluster_label}"
#     return "CL"
# 
# MODE_CODE = get_mode_code()
# 
# print("\nğŸ“‚ Base model root for this code version:")
# print("   ", base_model_root)
# print()
# 
# # ==============================================================
# # ğŸ§­ Data folders check
# # ==============================================================
# if not os.path.isdir(gm_root_dir):
#     raise FileNotFoundError(f"âŒ GM root dir Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {gm_root_dir}")
# if not os.path.isdir(tha_root_dir):
#     raise FileNotFoundError(f"âŒ THA root dir Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {tha_root_dir}")
# 
# available_heights = sorted(
#     name for name in os.listdir(gm_root_dir)
#     if os.path.isdir(os.path.join(gm_root_dir, name)) and name.startswith("H")
# )
# if not available_heights:
#     raise ValueError(f"âŒ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ H* Ø¯Ø± {gm_root_dir} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ø±Ø­Ù„Ù‡ Û³ Ø±Ø§ Ú†Ú© Ú©Ù†.")
# 
# print("ğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
# for h in available_heights:
#     print("  -", h)
# 
# print("\n-------------------------------------------")
# print("Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŸ")
# print("Ù…Ø«Ø§Ù„: H2 H3 H4  ÛŒØ§  2 3 4  (Ø®Ø§Ù„ÛŒ = Ù‡Ù…Ù‡)")
# print("-------------------------------------------\n")
# 
# heights_raw = input("Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (Ø®Ø§Ù„ÛŒ = Ù‡Ù…Ù‡): ").strip()
# 
# if not heights_raw:
#     height_tags = available_heights[:]
#     print("\nâœ… Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù†Ø¯.")
# else:
#     tokens = heights_raw.replace(',', ' ').split()
#     selected, invalid = [], []
#     for tok in tokens:
#         tok = tok.strip()
#         if not tok:
#             continue
#         if tok.startswith("H"):
#             tag = tok
#         else:
#             try:
#                 v = float(tok)
#                 if v.is_integer():
#                     tag = f"H{int(v)}"
#                 else:
#                     tag = "H" + str(v).replace('.', 'p')
#             except Exception:
#                 invalid.append(tok)
#                 continue
#         if tag in available_heights:
#             selected.append(tag)
#         else:
#             invalid.append(tok)
#     height_tags = list(dict.fromkeys(selected))
#     if invalid:
#         print("\nâš ï¸ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ù‡Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù†Ø¯:")
#         for x in invalid:
#             print("  -", x)
#     if not height_tags:
#         print("âŒ Ù‡ÛŒÚ† Ø§Ø±ØªÙØ§Ø¹ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ø› Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#         sys.exit(1)
# 
# print("\nğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:")
# print("  " + ", ".join(height_tags))
# print()
# 
# def height_value_from_tag(h_tag: str) -> float:
#     s = h_tag[1:]
#     s = s.replace('p', '.')
#     return float(s)
# 
# height_values = {h_tag: height_value_from_tag(h_tag) for h_tag in height_tags}
# print("ğŸ”¢ Ù†Ú¯Ø§Ø´Øª Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ (tag â†’ value):")
# for h_tag, hv in height_values.items():
#     print(f"  {h_tag} â†’ {hv}")
# 
# # ==============================================================
# # â†”ï¸ Training mode selection
# # ==============================================================
# print("\n-------------------------------------------")
# print("Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ù‡Ù…Ù‡Ù” Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù… Ùˆ Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ØŸ")
# print("   1 = Ø¨Ù„Ù‡ØŒ ÛŒÚ© Ù…Ø¯Ù„ Ù…Ø´ØªØ±Ú© Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹ (Multi-height + Feature H)")
# print("   0 = Ø®ÛŒØ±ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…Ø¯Ù„ Ù…Ø³ØªÙ‚Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯")
# print("-------------------------------------------\n")
# 
# mh_choice = input("Ø§Ù†ØªØ®Ø§Ø¨Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (1 ÛŒØ§ 0): ").strip()
# USE_MULTI_HEIGHT = (mh_choice == "1")
# 
# if USE_MULTI_HEIGHT:
#     print("âœ… Ù…ÙˆØ¯ Û±: Ù…Ø¯Ù„ Ù…Ø´ØªØ±Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹")
#     TRAIN_CODE = "GH"
# else:
#     print("âœ… Ù…ÙˆØ¯ Û²: Ù…Ø¯Ù„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹")
#     TRAIN_CODE = "PH"
# 
# # ==============================================================
# # Paths helper (unchanged)
# # ==============================================================
# def get_data_paths_for_height(h_tag):
#     if USE_CLUSTERED:
#         gm_dir  = os.path.join(gm_root_dir,  h_tag, "cluster_balanced_global")
#         tha_dir = os.path.join(tha_root_dir, h_tag, "cluster_balanced_global")
#         x_name  = f"X_data_cluster_balanced_global_{h_tag}.npy"
#         y_name  = f"Y_data_cluster_balanced_global_{h_tag}.npy"
#     else:
#         gm_dir  = os.path.join(gm_root_dir,  h_tag)
#         tha_dir = os.path.join(tha_root_dir, h_tag)
#         x_name  = f"X_data_{h_tag}.npy"
#         y_name  = f"Y_data_{h_tag}.npy"
#     return os.path.join(gm_dir, x_name), os.path.join(tha_dir, y_name)
# 
# # ==============================================================
# # Feature extraction from X(t) (accel)
# # ==============================================================
# def safe_fft_features(x, n_bins=6):
#     """
#     Simple, stable FFT band-energy features.
#     Returns: [dom_norm, centroid_norm] + n_bins band ratios
#     """
#     x = np.asarray(x, dtype=np.float64).reshape(-1)
#     if x.size < 8:
#         return [0.0] * (2 + n_bins)
# 
#     x0 = x - np.mean(x)
#     spec = np.fft.rfft(x0)
#     mag = np.abs(spec)
# 
#     total = float(np.sum(mag) + 1e-12)
# 
#     dom_idx = int(np.argmax(mag))
#     dom_norm = dom_idx / max(1, (mag.size - 1))
# 
#     idxs = np.arange(mag.size, dtype=np.float64)
#     centroid = float(np.sum(idxs * mag) / (np.sum(mag) + 1e-12))
#     centroid_norm = centroid / max(1.0, (mag.size - 1))
# 
#     bands = []
#     edges = np.linspace(0, mag.size, n_bins + 1, dtype=int)
#     for i in range(n_bins):
#         a, b = edges[i], edges[i + 1]
#         if b <= a:
#             bands.append(0.0)
#         else:
#             bands.append(float(np.sum(mag[a:b]) / total))
# 
#     return [dom_norm, centroid_norm] + bands
# 
# def extract_features_from_acc(x_series):
#     """
#     x_series: shape (T,1) or (T,)
#     returns: 1D feature vector (float32)
#     """
#     x = np.asarray(x_series).reshape(-1).astype(np.float64)
#     if x.size == 0:
#         return np.zeros((1,), dtype=np.float32)
# 
#     absx = np.abs(x)
#     pga = float(np.max(absx))
#     mean = float(np.mean(x))
#     std = float(np.std(x) + 1e-12)
#     rms = float(np.sqrt(np.mean(x * x)))
#     energy = float(np.sum(x * x))
# 
#     p50 = float(np.percentile(absx, 50))
#     p75 = float(np.percentile(absx, 75))
#     p90 = float(np.percentile(absx, 90))
#     p95 = float(np.percentile(absx, 95))
#     p99 = float(np.percentile(absx, 99))
# 
#     thr = 0.10 * pga
#     dur_ratio = float(np.mean(absx >= thr)) if pga > 1e-12 else 0.0
# 
#     sgn = np.sign(x)
#     zcr = float(np.mean(sgn[1:] != sgn[:-1])) if x.size > 2 else 0.0
# 
#     fft_feats = safe_fft_features(x, n_bins=6)
# 
#     feats = [
#         pga, mean, std, rms, energy,
#         p50, p75, p90, p95, p99,
#         dur_ratio, zcr
#     ] + fft_feats
# 
#     return np.asarray(feats, dtype=np.float32)
# 
# def y_max_abs_from_disp(y_series):
#     y = np.asarray(y_series).reshape(-1).astype(np.float64)
#     if y.size == 0:
#         return 0.0
#     return float(np.max(np.abs(y)))
# 
# # ==============================================================
# # Model choice: LightGBM if available, else sklearn HGBR
# # ==============================================================
# def build_regressor():
#     try:
#         import lightgbm as lgb
#         print("âœ… Using LightGBMRegressor.")
#         return lgb.LGBMRegressor(
#             n_estimators=1200,
#             learning_rate=0.03,
#             num_leaves=63,
#             subsample=0.8,
#             colsample_bytree=0.8,
#             reg_alpha=0.0,
#             reg_lambda=1.0,
#             random_state=SEED,
#             n_jobs=-1
#         ), "lightgbm"
#     except Exception:
#         from sklearn.ensemble import HistGradientBoostingRegressor
#         print("âœ… LightGBM not found. Using HistGradientBoostingRegressor (sklearn).")
#         return HistGradientBoostingRegressor(
#             learning_rate=0.05,
#             max_depth=None,
#             max_iter=600,
#             random_state=SEED
#         ), "sklearn_hgbr"
# 
# # ==============================================================
# # I/O helpers
# # ==============================================================
# def write_runinfo(run_dir, info: dict):
#     """
#     Always creates/updates runinfo.txt in run_dir.
#     """
#     p = os.path.join(run_dir, "runinfo.txt")
#     lines = []
#     lines.append("===== RUN INFO =====")
#     for k in sorted(info.keys()):
#         lines.append(f"{k}: {info[k]}")
#     with open(p, "w", encoding="utf-8") as f:
#         f.write("\n".join(lines) + "\n")
# 
# def save_metrics_json(path, metrics: dict):
#     with open(path, "w", encoding="utf-8") as f:
#         json.dump(metrics, f, ensure_ascii=False, indent=2)
# 
# def plot_pred(y_true, y_pred, out_path, title):
#     plt.figure()
#     plt.scatter(y_true, y_pred, s=10)
#     mn = min(float(np.min(y_true)), float(np.min(y_pred)))
#     mx = max(float(np.max(y_true)), float(np.max(y_pred)))
#     plt.plot([mn, mx], [mn, mx])
#     plt.xlabel("y_true (max|disp|)")
#     plt.ylabel("y_pred")
#     plt.title(title)
#     plt.tight_layout()
#     plt.savefig(out_path, dpi=300)
#     plt.close()
# 
# def plot_err_hist(err, out_path, title):
#     plt.figure()
#     plt.hist(err, bins=50)
#     plt.xlabel("error = y_pred - y_true")
#     plt.ylabel("count")
#     plt.title(title)
#     plt.tight_layout()
#     plt.savefig(out_path, dpi=300)
#     plt.close()
# 
# # ==============================================================
# # Split helper (same behavior: fixed split saved/loaded)
# # ==============================================================
# def get_or_make_split_indices(meta_dir, split_name, num_samples):
#     os.makedirs(meta_dir, exist_ok=True)
#     split_idx_path = os.path.join(meta_dir, split_name)
#     if os.path.exists(split_idx_path):
#         idx = np.load(split_idx_path)
#         if len(idx) != num_samples:
#             idx = np.arange(num_samples)
#             rng = np.random.default_rng(SEED)
#             rng.shuffle(idx)
#             np.save(split_idx_path, idx)
#             print("âš ï¸ Ø·ÙˆÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯Ø› split Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.")
#         else:
#             print("âœ… Fixed split indices loaded:", split_idx_path)
#     else:
#         idx = np.arange(num_samples)
#         rng = np.random.default_rng(SEED)
#         rng.shuffle(idx)
#         np.save(split_idx_path, idx)
#         print("âœ… Fixed split indices saved:", split_idx_path)
#     return idx, split_idx_path
# 
# def compute_metrics(y_true, y_pred):
#     y_true = np.asarray(y_true).reshape(-1)
#     y_pred = np.asarray(y_pred).reshape(-1)
#     mae = float(mean_absolute_error(y_true, y_pred))
#     rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
#     r2 = float(r2_score(y_true, y_pred)) if y_true.size >= 2 else float("nan")
#     return {"MAE": mae, "RMSE": rmse, "R2": r2}
# 
# # ==============================================================
# # MAIN: MODE 1 (Multi-height)
# # ==============================================================
# if USE_MULTI_HEIGHT:
#     X_feat_all, y_all = [], []
# 
#     for h_tag in height_tags:
#         print("\n" + "#" * 80)
#         print(f"ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹: {h_tag}")
#         print("#" * 80)
# 
#         x_data_path, y_data_path = get_data_paths_for_height(h_tag)
#         if not os.path.exists(x_data_path):
#             print(f"âš ï¸ X Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {x_data_path} â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
#         if not os.path.exists(y_data_path):
#             print(f"âš ï¸ Y Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {y_data_path} â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         X_dict = np.load(x_data_path, allow_pickle=True).item()
#         Y_dict = np.load(y_data_path, allow_pickle=True).item()
# 
#         common_keys = sorted(set(X_dict.keys()) & set(Y_dict.keys()))
#         if not common_keys:
#             print(f"âŒ Ù‡ÛŒÚ† Ú©Ù„ÛŒØ¯ Ù…Ø´ØªØ±Ú©ÛŒ Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         h_val = float(height_values[h_tag])
# 
#         for k in common_keys:
#             x = X_dict[k].reshape(-1, 1).astype("float32")
#             y = Y_dict[k].reshape(-1, 1).astype("float32")
# 
#             feats = extract_features_from_acc(x)
#             feats = np.concatenate([feats, np.asarray([h_val], dtype=np.float32)], axis=0)
# 
#             X_feat_all.append(feats)
#             y_all.append(y_max_abs_from_disp(y))
# 
#     num_samples = len(X_feat_all)
#     if num_samples == 0:
#         raise ValueError("âŒ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
# 
#     X_feat_all = np.vstack(X_feat_all).astype(np.float32)
#     y_all = np.asarray(y_all, dtype=np.float32).reshape(-1)
# 
#     print(f"\nğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ (Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù…): {num_samples}")
#     print(f"ğŸ§¾ Feature dim: {X_feat_all.shape[1]} (includes height)")
# 
#     meta_dir = os.path.join(base_model_root, f"_META_{MODE_CODE}-{TRAIN_CODE}")
#     idx, split_idx_path = get_or_make_split_indices(meta_dir, f"split_idx_seed{SEED}.npy", num_samples)
# 
#     train_split = int(0.50 * num_samples)
#     val_split   = int(0.63 * num_samples)
# 
#     train_ids = idx[:train_split]
#     val_ids   = idx[train_split:val_split]
#     test_ids  = idx[val_split:]
# 
#     X_train, y_train = X_feat_all[train_ids], y_all[train_ids]
#     X_val,   y_val   = X_feat_all[val_ids],   y_all[val_ids]
#     X_test,  y_test  = X_feat_all[test_ids],  y_all[test_ids]
# 
#     print(f"\nğŸ“¦ Global split | Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}")
# 
#     model, model_kind = build_regressor()
# 
#     # âœ… run_tag includes script_name (so it behaves like your original workflow)
#     run_tag = f"{script_name}__{MODE_CODE}-{TRAIN_CODE}__MAXABS__{model_kind}"
#     model_dir = os.path.join(base_model_root, run_tag)
#     os.makedirs(model_dir, exist_ok=True)
# 
#     model_path = os.path.join(model_dir, "model.pkl")
#     metrics_path = os.path.join(model_dir, "metrics.json")
# 
#     runinfo = {
#         "status": "STARTED",
#         "run_tag": run_tag,
#         "script_name": script_name,
#         "script_tag": script_tag,
#         "seed": SEED,
#         "use_clustered": USE_CLUSTERED,
#         "cluster_label": cluster_label,
#         "mode_code": MODE_CODE,
#         "train_code": TRAIN_CODE,
#         "gm_root_dir": gm_root_dir,
#         "tha_root_dir": tha_root_dir,
#         "split_idx_path": split_idx_path,
#         "num_samples": int(num_samples),
#         "feature_dim": int(X_feat_all.shape[1]),
#         "target": "y = max(abs(disp(t)))",
#         "model_kind": model_kind
#     }
#     write_runinfo(model_dir, runinfo)
# 
#     try:
#         if model_kind == "lightgbm":
#             model.fit(
#                 X_train, y_train,
#                 eval_set=[(X_val, y_val)],
#                 eval_metric="l1",
#                 verbose=50
#             )
#         else:
#             model.fit(X_train, y_train)
#     except TypeError:
#         model.fit(X_train, y_train)
# 
#     y_pred_train = model.predict(X_train)
#     y_pred_val   = model.predict(X_val)
#     y_pred_test  = model.predict(X_test)
# 
#     metrics = {
#         "train": compute_metrics(y_train, y_pred_train),
#         "val":   compute_metrics(y_val,   y_pred_val),
#         "test":  compute_metrics(y_test,  y_pred_test),
#     }
# 
#     joblib.dump(model, model_path)
#     save_metrics_json(metrics_path, metrics)
# 
#     runinfo["status"] = "FINISHED"
#     runinfo["model_path"] = model_path
#     runinfo["metrics_path"] = metrics_path
#     write_runinfo(model_dir, runinfo)
# 
#     print("\nâœ… Training finished:", run_tag)
#     print("ğŸ“Œ Test metrics:", metrics["test"])
# 
#     plot_pred(y_test, y_pred_test, os.path.join(model_dir, "y_true_vs_pred.png"),
#               title=f"{run_tag} | y_true vs y_pred (TEST)")
#     plot_err_hist((y_pred_test - y_test), os.path.join(model_dir, "error_hist.png"),
#                   title=f"{run_tag} | error histogram (TEST)")
# 
# # ==============================================================
# # MAIN: MODE 2 (Per-height)
# # ==============================================================
# else:
#     for h_tag in height_tags:
#         print("\n" + "#" * 80)
#         print(f"ğŸ—ï¸ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹: {h_tag}")
#         print("#" * 80)
# 
#         x_data_path, y_data_path = get_data_paths_for_height(h_tag)
# 
#         if not os.path.exists(x_data_path):
#             print(f"âš ï¸ X_data Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {x_data_path} â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
#         if not os.path.exists(y_data_path):
#             print(f"âš ï¸ Y_data Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {y_data_path} â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         X_dict = np.load(x_data_path, allow_pickle=True).item()
#         Y_dict = np.load(y_data_path, allow_pickle=True).item()
# 
#         common_keys = sorted(set(X_dict.keys()) & set(Y_dict.keys()))
#         if not common_keys:
#             print(f"âŒ Ù‡ÛŒÚ† Ú©Ù„ÛŒØ¯ Ù…Ø´ØªØ±Ú©ÛŒ Ø¨ÛŒÙ† X Ùˆ Y Ø¨Ø±Ø§ÛŒ {h_tag} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         X_feat_all, y_all = [], []
#         for k in common_keys:
#             x = X_dict[k].reshape(-1, 1).astype("float32")
#             y = Y_dict[k].reshape(-1, 1).astype("float32")
#             X_feat_all.append(extract_features_from_acc(x))
#             y_all.append(y_max_abs_from_disp(y))
# 
#         num_samples = len(X_feat_all)
#         if num_samples == 0:
#             print(f"âš ï¸ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {h_tag} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         X_feat_all = np.vstack(X_feat_all).astype(np.float32)
#         y_all = np.asarray(y_all, dtype=np.float32).reshape(-1)
# 
#         print(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {h_tag}: {num_samples}")
#         print(f"ğŸ§¾ Feature dim: {X_feat_all.shape[1]}")
# 
#         height_meta_dir = os.path.join(base_model_root, f"_META_{MODE_CODE}-{TRAIN_CODE}_{h_tag}")
#         idx, split_idx_path = get_or_make_split_indices(height_meta_dir, f"split_idx_seed{SEED}.npy", num_samples)
# 
#         train_split = int(0.50 * num_samples)
#         val_split   = int(0.63 * num_samples)
# 
#         train_ids = idx[:train_split]
#         val_ids   = idx[train_split:val_split]
#         test_ids  = idx[val_split:]
# 
#         X_train, y_train = X_feat_all[train_ids], y_all[train_ids]
#         X_val,   y_val   = X_feat_all[val_ids],   y_all[val_ids]
#         X_test,  y_test  = X_feat_all[test_ids],  y_all[test_ids]
# 
#         model, model_kind = build_regressor()
# 
#         # âœ… run_tag includes script_name (so it behaves like your original workflow)
#         run_tag = f"{script_name}__{MODE_CODE}-{TRAIN_CODE}_{h_tag}__MAXABS__{model_kind}"
#         model_dir = os.path.join(base_model_root, run_tag)
#         os.makedirs(model_dir, exist_ok=True)
# 
#         model_path = os.path.join(model_dir, "model.pkl")
#         metrics_path = os.path.join(model_dir, "metrics.json")
# 
#         runinfo = {
#             "status": "STARTED",
#             "run_tag": run_tag,
#             "height": h_tag,
#             "script_name": script_name,
#             "script_tag": script_tag,
#             "seed": SEED,
#             "use_clustered": USE_CLUSTERED,
#             "cluster_label": cluster_label,
#             "mode_code": MODE_CODE,
#             "train_code": TRAIN_CODE,
#             "gm_root_dir": gm_root_dir,
#             "tha_root_dir": tha_root_dir,
#             "split_idx_path": split_idx_path,
#             "num_samples": int(num_samples),
#             "feature_dim": int(X_feat_all.shape[1]),
#             "target": "y = max(abs(disp(t)))",
#             "model_kind": model_kind
#         }
#         write_runinfo(model_dir, runinfo)
# 
#         try:
#             if model_kind == "lightgbm":
#                 model.fit(
#                     X_train, y_train,
#                     eval_set=[(X_val, y_val)],
#                     eval_metric="l1",
#                     verbose=50
#                 )
#             else:
#                 model.fit(X_train, y_train)
#         except TypeError:
#             model.fit(X_train, y_train)
# 
#         y_pred_train = model.predict(X_train)
#         y_pred_val   = model.predict(X_val)
#         y_pred_test  = model.predict(X_test)
# 
#         metrics = {
#             "train": compute_metrics(y_train, y_pred_train),
#             "val":   compute_metrics(y_val,   y_pred_val),
#             "test":  compute_metrics(y_test,  y_pred_test),
#         }
# 
#         joblib.dump(model, model_path)
#         save_metrics_json(metrics_path, metrics)
# 
#         runinfo["status"] = "FINISHED"
#         runinfo["model_path"] = model_path
#         runinfo["metrics_path"] = metrics_path
#         write_runinfo(model_dir, runinfo)
# 
#         print("\nâœ… Training finished:", run_tag)
#         print("ğŸ“Œ Test metrics:", metrics["test"])
# 
#         plot_pred(y_test, y_pred_test, os.path.join(model_dir, "y_true_vs_pred.png"),
#                   title=f"{run_tag} | y_true vs y_pred (TEST)")
#         plot_err_hist((y_pred_test - y_test), os.path.join(model_dir, "error_hist.png"),
#                       title=f"{run_tag} | error histogram (TEST)")
# 
#     print("ğŸ‰ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¯Ø± Ù…ÙˆØ¯ per-height ØªÙ…Ø§Ù… Ø´Ø¯.")
# 
# =============================================================================


# =============================================================================
# 
# # -*- coding: utf-8 -*-
# """
# File name      : Peak_prediction_For_ghab_GPU-Clusttering-v04_improved.py
# Based on       : Peak_prediction_For_ghab_GPU-Clusttering-v04.py
# Author         : pc22 (updated by ChatGPT)
# Created on     : Wed Dec 31 09:xx:xx 2025
# Last modified  : Sun Jan 04 2026
# ------------------------------------------------------------
# Purpose:
#     Train a **peak-only** surrogate model to predict the **maximum
#     absolute displacement** of the structure directly from the input
#     ground-motion (GM) acceleration time series.
# 
#     This improved v04 implements the user's selected upgrades:
#       (1) Log-transform target: train on log1p(y), evaluate in original units
#       (2) Peak-aware weighting + optional peak oversampling (train only)
#       (3) Support PH (per-height) and GH (global multi-height) as before
#       (4) Outlier handling: remove NaN/Inf, optional winsorize & IQR filter
#       (5) Stronger split evaluation: fixed test split + K-fold CV on train+val
#       (6) Add Arias Intensity (IA)
#       (7) Add CAV
#       (8) Add significant duration (D5-95 and D5-75 via cumulative IA)
#       (9) Add PGV/PGD (robust integration with detrend)
#       (10) Add response-spectrum features Sa(T) at multiple periods
# ------------------------------------------------------------
# Notes:
#     - This script remains interactive like your original workflow.
#     - dt is required for several features; you will be prompted.
#     - If LightGBM is installed, it is used; otherwise sklearn fallback.
#     - The pipeline for reading X/Y dict .npy files is unchanged.
# """
# 
# import sys, io
# if hasattr(sys.stdout, "buffer"):
#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')
# 
# import os, re, json, random, math
# import numpy as np
# import matplotlib.pyplot as plt
# 
# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# import joblib
# 
# plt.ioff()
# 
# # ==============================================================
# # ğŸ”’ Reproducibility
# # ==============================================================
# SEED = 1234
# os.environ["PYTHONHASHSEED"] = str(SEED)
# random.seed(SEED)
# np.random.seed(SEED)
# 
# # ==============================================================
# # âš™ï¸ Improvements toggles (defaults chosen to "ON" per your request)
# # ==============================================================
# USE_LOG_TARGET = True
# 
# # Peak weighting (sample_weight): w = 1 + W_C * (y / y_max) ** W_P
# USE_PEAK_WEIGHTING = True
# W_C = 6.0
# W_P = 2.0
# 
# # Optional oversampling of peak records in TRAIN only
# USE_PEAK_OVERSAMPLING = True
# PEAK_Q = 0.90          # oversample records above this quantile
# PEAK_MULTIPLIER = 2    # replicate each peak sample PEAK_MULTIPLIER-1 extra times
# 
# # Outlier handling
# REMOVE_NONFINITE = True
# USE_WINSORIZE = True
# WINSOR_Q_HIGH = 0.995  # clip y to this high quantile (training target construction)
# USE_IQR_FILTER = False # if True: remove extreme outliers beyond IQR fence
# IQR_K = 4.0            # fence multiplier (higher = less aggressive)
# 
# # Split evaluation: K-fold CV on Train+Val (keeps fixed TEST split)
# USE_CV_EVAL = True
# CV_FOLDS = 5
# 
# # Spectrum features (Sa(T)) settings
# DAMPING = 0.05
# SPECTRAL_PERIODS = [0.1, 0.2, 0.5, 1.0, 2.0]
# 
# # ==============================================================
# # ğŸ“ Paths + Linear / Nonlinear selection
# # ==============================================================
# base_dir = os.path.dirname(os.path.abspath(__file__))
# root_dir = os.path.abspath(os.path.join(base_dir, os.pardir))
# output_root_dir = os.path.join(root_dir, "Output")
# 
# choice = input(
#     "Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø³Ø® Ø®Ø·ÛŒ Ø¨Ø§Ø´Ø¯ ÛŒØ§ ØºÛŒØ±Ø®Ø·ÛŒØŸ "
#     "Ø¨Ø±Ø§ÛŒ Ø®Ø·ÛŒ Ø¹Ø¯Ø¯ 1 Ùˆ Ø¨Ø±Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ Ø¹Ø¯Ø¯ 0 Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†: "
# ).strip()
# is_linear = (choice == "1")
# 
# if is_linear:
#     print("ğŸ“Œ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ (THA_linear) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#     gm_root_dir  = os.path.join(output_root_dir, "3_GM_Fixed_train_linear")
#     tha_root_dir = os.path.join(output_root_dir, "3_THA_Fixed_train_linear")
#     base_model_root = os.path.join(output_root_dir, "Progress_of_MaxDisp_linear")
# else:
#     print("ğŸ“Œ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ (THA_nonlinear) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#     gm_root_dir  = os.path.join(output_root_dir, "3_GM_Fixed_train_nonlinear")
#     tha_root_dir = os.path.join(output_root_dir, "3_THA_Fixed_train_nonlinear")
#     base_model_root = os.path.join(output_root_dir, "Progress_of_MaxDisp_nonlinear")
# 
# os.makedirs(base_model_root, exist_ok=True)
# 
# print("ğŸ“‚ GM root dir :", gm_root_dir)
# print("ğŸ“‚ THA root dir:", tha_root_dir)
# print("ğŸ“‚ Base model root:", base_model_root)
# print()
# 
# # ==============================================================
# # ğŸ•’ dt needed for IA/CAV/duration/PGV/PGD/Sa(T)
# # ==============================================================
# dt_in = input("Ú¯Ø§Ù… Ø²Ù…Ø§Ù†ÛŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ (dt) Ú†Ù†Ø¯ Ø§Ø³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹ 0.01) Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒ 0.01 Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…: ").strip()
# try:
#     DT = float(dt_in) if dt_in else 0.01
#     if DT <= 0:
#         raise ValueError
# except Exception:
#     print("âš ï¸ dt Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ø› 0.01 Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
#     DT = 0.01
# 
# print(f"â±ï¸ dt = {DT}")
# 
# # ==============================================================
# # ğŸ§¾ Script-tag isolation
# # ==============================================================
# script_name = os.path.splitext(os.path.basename(__file__))[0]
# 
# m = re.search(r"(v\d+)", script_name, flags=re.IGNORECASE)
# if m:
#     script_tag = m.group(1).upper()
# else:
#     script_tag = re.sub(r"[^A-Za-z0-9]+", "", script_name)[:12] or "SCRIPT"
# 
# base_model_root = os.path.join(base_model_root, script_tag)
# os.makedirs(base_model_root, exist_ok=True)
# 
# # ==============================================================
# # ğŸ” Clustered vs non-clustered
# # ==============================================================
# print("-------------------------------------------")
# print("Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ *Ú©Ù„Ø§Ø³ØªØ±Ø´Ø¯Ù‡* Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù…ØŸ")
# print("   1 = Ø¨Ù„Ù‡ØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ cluster_balanced_global Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†")
# print("   0 = Ø®ÛŒØ±ØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ X_data_H* Ùˆ Y_data_H* Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†")
# print("-------------------------------------------\n")
# 
# cluster_choice = input("Ø§Ù†ØªØ®Ø§Ø¨Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (1 ÛŒØ§ 0): ").strip()
# USE_CLUSTERED = (cluster_choice == "1")
# 
# cluster_label = ""
# if USE_CLUSTERED:
#     print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ØªØ±Ø´Ø¯Ù‡ (cluster_balanced_global).")
#     cluster_label = input(
#         "Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù… Ù¾ÙˆØ´Ù‡Ù” Ù…Ø¯Ù„ØŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³ØªØ±Ù‡Ø§ (K) Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ "
#         "(Ù…Ø«Ù„Ø§Ù‹ 4). Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒØŒ 'clustered' Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: "
#     ).strip()
# else:
#     print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø¯ÙˆÙ† Ú©Ù„Ø§Ø³ØªØ±ÛŒÙ†Ú¯.")
# 
# def get_mode_code():
#     if not USE_CLUSTERED:
#         return "NC"
#     if cluster_label:
#         return f"CK{cluster_label}"
#     return "CL"
# 
# MODE_CODE = get_mode_code()
# 
# print("\nğŸ“‚ Base model root for this code version:")
# print("   ", base_model_root)
# print()
# 
# # ==============================================================
# # ğŸ§­ Data folders check
# # ==============================================================
# if not os.path.isdir(gm_root_dir):
#     raise FileNotFoundError(f"âŒ GM root dir Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {gm_root_dir}")
# if not os.path.isdir(tha_root_dir):
#     raise FileNotFoundError(f"âŒ THA root dir Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {tha_root_dir}")
# 
# available_heights = sorted(
#     name for name in os.listdir(gm_root_dir)
#     if os.path.isdir(os.path.join(gm_root_dir, name)) and name.startswith("H")
# )
# if not available_heights:
#     raise ValueError(f"âŒ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ H* Ø¯Ø± {gm_root_dir} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ø±Ø­Ù„Ù‡ Û³ Ø±Ø§ Ú†Ú© Ú©Ù†.")
# 
# print("ğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
# for h in available_heights:
#     print("  -", h)
# 
# print("\n-------------------------------------------")
# print("Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŸ")
# print("Ù…Ø«Ø§Ù„: H2 H3 H4  ÛŒØ§  2 3 4  (Ø®Ø§Ù„ÛŒ = Ù‡Ù…Ù‡)")
# print("-------------------------------------------\n")
# 
# heights_raw = input("Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (Ø®Ø§Ù„ÛŒ = Ù‡Ù…Ù‡): ").strip()
# 
# if not heights_raw:
#     height_tags = available_heights[:]
#     print("\nâœ… Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù†Ø¯.")
# else:
#     tokens = heights_raw.replace(',', ' ').split()
#     selected, invalid = [], []
#     for tok in tokens:
#         tok = tok.strip()
#         if not tok:
#             continue
#         if tok.startswith("H"):
#             tag = tok
#         else:
#             try:
#                 v = float(tok)
#                 if v.is_integer():
#                     tag = f"H{int(v)}"
#                 else:
#                     tag = "H" + str(v).replace('.', 'p')
#             except Exception:
#                 invalid.append(tok)
#                 continue
#         if tag in available_heights:
#             selected.append(tag)
#         else:
#             invalid.append(tok)
#     height_tags = list(dict.fromkeys(selected))
#     if invalid:
#         print("\nâš ï¸ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ù‡Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù†Ø¯:")
#         for x in invalid:
#             print("  -", x)
#     if not height_tags:
#         print("âŒ Ù‡ÛŒÚ† Ø§Ø±ØªÙØ§Ø¹ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ø› Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#         sys.exit(1)
# 
# print("\nğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:")
# print("  " + ", ".join(height_tags))
# print()
# 
# def height_value_from_tag(h_tag: str) -> float:
#     s = h_tag[1:]
#     s = s.replace('p', '.')
#     return float(s)
# 
# height_values = {h_tag: height_value_from_tag(h_tag) for h_tag in height_tags}
# print("ğŸ”¢ Ù†Ú¯Ø§Ø´Øª Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ (tag â†’ value):")
# for h_tag, hv in height_values.items():
#     print(f"  {h_tag} â†’ {hv}")
# 
# # ==============================================================
# # â†”ï¸ Training mode selection
# # ==============================================================
# print("\n-------------------------------------------")
# print("Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ù‡Ù…Ù‡Ù” Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù… Ùˆ Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ØŸ")
# print("   1 = Ø¨Ù„Ù‡ØŒ ÛŒÚ© Ù…Ø¯Ù„ Ù…Ø´ØªØ±Ú© Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹ (Multi-height + Feature H)")
# print("   0 = Ø®ÛŒØ±ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…Ø¯Ù„ Ù…Ø³ØªÙ‚Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯")
# print("-------------------------------------------\n")
# 
# mh_choice = input("Ø§Ù†ØªØ®Ø§Ø¨Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (1 ÛŒØ§ 0): ").strip()
# USE_MULTI_HEIGHT = (mh_choice == "1")
# 
# if USE_MULTI_HEIGHT:
#     print("âœ… Ù…ÙˆØ¯ Û±: Ù…Ø¯Ù„ Ù…Ø´ØªØ±Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹")
#     TRAIN_CODE = "GH"
# else:
#     print("âœ… Ù…ÙˆØ¯ Û²: Ù…Ø¯Ù„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹")
#     TRAIN_CODE = "PH"
# 
# # ==============================================================
# # Paths helper (unchanged)
# # ==============================================================
# def get_data_paths_for_height(h_tag):
#     if USE_CLUSTERED:
#         gm_dir  = os.path.join(gm_root_dir,  h_tag, "cluster_balanced_global")
#         tha_dir = os.path.join(tha_root_dir, h_tag, "cluster_balanced_global")
#         x_name  = f"X_data_cluster_balanced_global_{h_tag}.npy"
#         y_name  = f"Y_data_cluster_balanced_global_{h_tag}.npy"
#     else:
#         gm_dir  = os.path.join(gm_root_dir,  h_tag)
#         tha_dir = os.path.join(tha_root_dir, h_tag)
#         x_name  = f"X_data_{h_tag}.npy"
#         y_name  = f"Y_data_{h_tag}.npy"
#     return os.path.join(gm_dir, x_name), os.path.join(tha_dir, y_name)
# 
# # ==============================================================
# # ğŸ§  Feature extraction utilities
# # ==============================================================
# 
# G = 9.80665
# 
# def _detrend_linear(x: np.ndarray) -> np.ndarray:
#     """Remove linear trend: x - (a*t + b)."""
#     x = np.asarray(x, dtype=np.float64).reshape(-1)
#     n = x.size
#     if n < 3:
#         return x.copy()
#     t = np.arange(n, dtype=np.float64)
#     A = np.vstack([t, np.ones(n)]).T
#     # least squares
#     coef, *_ = np.linalg.lstsq(A, x, rcond=None)
#     trend = A @ coef
#     return x - trend
# 
# def integrate_trapz_cumsum(x: np.ndarray, dt: float) -> np.ndarray:
#     """Cumulative trapezoidal integral with dt, using only numpy."""
#     x = np.asarray(x, dtype=np.float64).reshape(-1)
#     if x.size == 0:
#         return x
#     y = np.zeros_like(x)
#     if x.size == 1:
#         y[0] = 0.0
#         return y
#     # trapezoid cumulative: y[i] = sum_{k=1..i} 0.5*(x[k]+x[k-1])*dt
#     y[1:] = np.cumsum(0.5 * (x[1:] + x[:-1]) * dt)
#     return y
# 
# def compute_pgv_pgd(a: np.ndarray, dt: float):
#     """Robust-ish PGV/PGD from acceleration via detrend + trapezoid integration."""
#     a = np.asarray(a, dtype=np.float64).reshape(-1)
#     if a.size < 4:
#         return 0.0, 0.0
#     a0 = a - np.mean(a)
#     v = integrate_trapz_cumsum(a0, dt)
#     v = _detrend_linear(v)
#     d = integrate_trapz_cumsum(v, dt)
#     d = _detrend_linear(d)
#     pgv = float(np.max(np.abs(v)))
#     pgd = float(np.max(np.abs(d)))
#     return pgv, pgd
# 
# def compute_arias_intensity(a: np.ndarray, dt: float):
#     """Arias intensity IA = pi/(2g) * âˆ« a(t)^2 dt."""
#     a = np.asarray(a, dtype=np.float64).reshape(-1)
#     if a.size < 2:
#         return 0.0
#     ia = (math.pi / (2.0 * G)) * float(np.sum(a * a) * dt)
#     return ia
# 
# def compute_cav(a: np.ndarray, dt: float):
#     """Cumulative Absolute Velocity CAV = âˆ« |a(t)| dt."""
#     a = np.asarray(a, dtype=np.float64).reshape(-1)
#     if a.size < 2:
#         return 0.0
#     return float(np.sum(np.abs(a)) * dt)
# 
# def significant_duration_from_ia(a: np.ndarray, dt: float, p1=0.05, p2=0.95):
#     """Compute significant duration between p1 and p2 of cumulative IA."""
#     a = np.asarray(a, dtype=np.float64).reshape(-1)
#     if a.size < 2:
#         return 0.0
#     ia_cum = np.cumsum(a * a) * dt  # proportional to IA cumulative (constant cancels)
#     total = float(ia_cum[-1])
#     if total <= 1e-18:
#         return 0.0
#     ia_norm = ia_cum / total
#     t = np.arange(a.size) * dt
#     t1 = float(np.interp(p1, ia_norm, t))
#     t2 = float(np.interp(p2, ia_norm, t))
#     return max(0.0, t2 - t1)
# 
# def safe_fft_features(x, n_bins=6):
#     """
#     Simple, stable FFT band-energy features.
#     Returns: [dom_norm, centroid_norm] + n_bins band ratios.
#     """
#     x = np.asarray(x, dtype=np.float64).reshape(-1)
#     if x.size < 8:
#         return [0.0] * (2 + n_bins)
# 
#     x0 = x - np.mean(x)
#     spec = np.fft.rfft(x0)
#     mag = np.abs(spec)
# 
#     total = float(np.sum(mag) + 1e-12)
# 
#     dom_idx = int(np.argmax(mag))
#     dom_norm = dom_idx / max(1, (mag.size - 1))
# 
#     idxs = np.arange(mag.size, dtype=np.float64)
#     centroid = float(np.sum(idxs * mag) / (np.sum(mag) + 1e-12))
#     centroid_norm = centroid / max(1.0, (mag.size - 1))
# 
#     bands = []
#     edges = np.linspace(0, mag.size, n_bins + 1, dtype=int)
#     for i in range(n_bins):
#         a, b = edges[i], edges[i + 1]
#         if b <= a:
#             bands.append(0.0)
#         else:
#             bands.append(float(np.sum(mag[a:b]) / total))
# 
#     return [dom_norm, centroid_norm] + bands
# 
# def newmark_sa(acc, dt, T, zeta=0.05):
#     """
#     Compute pseudo-spectral acceleration Sa(T) for an SDOF using
#     Newmark-beta (average acceleration) method.
#     Returns Sa in same acceleration units as input acc.
#     """
#     acc = np.asarray(acc, dtype=np.float64).reshape(-1)
#     n = acc.size
#     if n < 4 or T <= 0:
#         return 0.0
# 
#     w = 2.0 * math.pi / T
#     k = w * w
#     c = 2.0 * zeta * w
# 
#     # Newmark parameters (average acceleration)
#     beta = 1.0 / 4.0
#     gamma = 1.0 / 2.0
# 
#     u = 0.0
#     v = 0.0
#     a_rel = 0.0  # relative acceleration
# 
#     # effective stiffness
#     a0 = 1.0 / (beta * dt * dt)
#     a1 = gamma / (beta * dt)
#     a2 = 1.0 / (beta * dt)
#     a3 = 1.0 / (2.0 * beta) - 1.0
#     a4 = gamma / beta - 1.0
#     a5 = dt * (gamma / (2.0 * beta) - 1.0)
# 
#     k_eff = k + a0 + a1 * c
# 
#     umax = 0.0
# 
#     for i in range(1, n):
#         # effective load increment: p = -acc
#         p_i = -acc[i]
#         p_prev = -acc[i-1]
# 
#         dp = p_i - p_prev
# 
#         # effective force
#         p_eff = dp + (a0 * u + a2 * v + a3 * a_rel) + c * (a1 * u + a4 * v + a5 * a_rel)
# 
#         du = p_eff / k_eff
#         dv = a1 * du - a4 * v - a5 * a_rel
#         da = a0 * du - a2 * v - a3 * a_rel
# 
#         u += du
#         v += dv
#         a_rel += da
# 
#         if abs(u) > umax:
#             umax = abs(u)
# 
#     # pseudo spectral acceleration
#     sa = (w * w) * umax
#     return float(sa)
# 
# def spectral_features(acc, dt, periods, zeta=0.05):
#     feats = []
#     for T in periods:
#         feats.append(newmark_sa(acc, dt, float(T), zeta=zeta))
#     return feats
# 
# def extract_features_from_acc(x_series, dt):
#     """
#     x_series: shape (T,1) or (T,)
#     returns: 1D feature vector (float32)
#     """
#     x = np.asarray(x_series).reshape(-1).astype(np.float64)
#     if x.size == 0:
#         return np.zeros((1,), dtype=np.float32)
# 
#     absx = np.abs(x)
#     pga = float(np.max(absx))
#     mean = float(np.mean(x))
#     std = float(np.std(x) + 1e-12)
#     rms = float(np.sqrt(np.mean(x * x)))
#     energy = float(np.sum(x * x) * dt)  # now energy-like with dt
# 
#     p50 = float(np.percentile(absx, 50))
#     p75 = float(np.percentile(absx, 75))
#     p90 = float(np.percentile(absx, 90))
#     p95 = float(np.percentile(absx, 95))
#     p99 = float(np.percentile(absx, 99))
# 
#     thr = 0.10 * pga
#     dur_ratio = float(np.mean(absx >= thr)) if pga > 1e-12 else 0.0
# 
#     sgn = np.sign(x)
#     zcr = float(np.mean(sgn[1:] != sgn[:-1])) if x.size > 2 else 0.0
# 
#     # (6) IA, (7) CAV, (8) significant durations
#     ia = compute_arias_intensity(x, dt)
#     cav = compute_cav(x, dt)
#     d595 = significant_duration_from_ia(x, dt, 0.05, 0.95)
#     d575 = significant_duration_from_ia(x, dt, 0.05, 0.75)
# 
#     # (9) PGV/PGD (from integration)
#     pgv, pgd = compute_pgv_pgd(x, dt)
# 
#     # FFT summaries
#     fft_feats = safe_fft_features(x, n_bins=6)
# 
#     # (10) Sa(T) features
#     sa_feats = spectral_features(x, dt, SPECTRAL_PERIODS, zeta=DAMPING)
# 
#     feats = [
#         # basic
#         pga, mean, std, rms, energy,
#         # percentiles
#         p50, p75, p90, p95, p99,
#         # simple duration + zcr
#         dur_ratio, zcr,
#         # IA/CAV/durations
#         ia, cav, d595, d575,
#         # PGV/PGD
#         pgv, pgd,
#     ] + fft_feats + sa_feats
# 
#     # helpful ratios (scale invariance)
#     if pga > 1e-12:
#         feats += [pgv / pga, pgd / max(pgv, 1e-12), cav / pga, ia / (pga * pga * dt * x.size + 1e-12)]
#     else:
#         feats += [0.0, 0.0, 0.0, 0.0]
# 
#     return np.asarray(feats, dtype=np.float32)
# 
# def y_max_abs_from_disp(y_series):
#     y = np.asarray(y_series).reshape(-1).astype(np.float64)
#     if y.size == 0:
#         return 0.0
#     return float(np.max(np.abs(y)))
# 
# # ==============================================================
# # Model choice: LightGBM if available, else sklearn HGBR
# # ==============================================================
# def build_regressor(best_params=None):
#     try:
#         import lightgbm as lgb
#         print("âœ… Using LightGBMRegressor.")
#         params = dict(
#             n_estimators=1800,
#             learning_rate=0.02,
#             num_leaves=63,
#             subsample=0.85,
#             colsample_bytree=0.85,
#             reg_alpha=0.0,
#             reg_lambda=1.0,
#             min_child_samples=20,
#             random_state=SEED,
#             n_jobs=-1
#         )
#         if best_params:
#             params.update(best_params)
#         return lgb.LGBMRegressor(**params), "lightgbm"
#     except Exception:
#         from sklearn.ensemble import HistGradientBoostingRegressor
#         print("âœ… LightGBM not found. Using HistGradientBoostingRegressor (sklearn).")
#         # best_params mapping (subset)
#         hg_params = dict(
#             learning_rate=0.05,
#             max_depth=None,
#             max_iter=900,
#             random_state=SEED
#         )
#         if best_params:
#             for k in ["learning_rate", "max_depth", "max_iter"]:
#                 if k in best_params:
#                     hg_params[k] = best_params[k]
#         return HistGradientBoostingRegressor(**hg_params), "sklearn_hgbr"
# 
# # ==============================================================
# # I/O helpers
# # ==============================================================
# def write_runinfo(run_dir, info: dict):
#     p = os.path.join(run_dir, "runinfo.txt")
#     lines = []
#     lines.append("===== RUN INFO =====")
#     for k in sorted(info.keys()):
#         lines.append(f"{k}: {info[k]}")
#     with open(p, "w", encoding="utf-8") as f:
#         f.write("\n".join(lines) + "\n")
# 
# def save_metrics_json(path, metrics: dict):
#     with open(path, "w", encoding="utf-8") as f:
#         json.dump(metrics, f, ensure_ascii=False, indent=2)
# 
# def plot_pred(y_true, y_pred, out_path, title):
#     plt.figure()
#     plt.scatter(y_true, y_pred, s=10)
#     mn = min(float(np.min(y_true)), float(np.min(y_pred)))
#     mx = max(float(np.max(y_true)), float(np.max(y_pred)))
#     plt.plot([mn, mx], [mn, mx])
#     plt.xlabel("True max|disp|")
#     plt.ylabel("Pred max|disp|")
#     plt.title(title)
#     plt.tight_layout()
#     plt.savefig(out_path, dpi=300)
#     plt.close()
# 
# def plot_err_hist(err, out_path, title):
#     plt.figure()
#     plt.hist(err, bins=50)
#     plt.xlabel("error = y_pred - y_true")
#     plt.ylabel("count")
#     plt.title(title)
#     plt.tight_layout()
#     plt.savefig(out_path, dpi=300)
#     plt.close()
# 
# # ==============================================================
# # Split helper (fixed split saved/loaded)
# # ==============================================================
# def get_or_make_split_indices(meta_dir, split_name, num_samples):
#     os.makedirs(meta_dir, exist_ok=True)
#     split_idx_path = os.path.join(meta_dir, split_name)
#     if os.path.exists(split_idx_path):
#         idx = np.load(split_idx_path)
#         if len(idx) != num_samples:
#             idx = np.arange(num_samples)
#             rng = np.random.default_rng(SEED)
#             rng.shuffle(idx)
#             np.save(split_idx_path, idx)
#             print("âš ï¸ Ø·ÙˆÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯Ø› split Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.")
#         else:
#             print("âœ… Fixed split indices loaded:", split_idx_path)
#     else:
#         idx = np.arange(num_samples)
#         rng = np.random.default_rng(SEED)
#         rng.shuffle(idx)
#         np.save(split_idx_path, idx)
#         print("âœ… Fixed split indices saved:", split_idx_path)
#     return idx, split_idx_path
# 
# def compute_metrics(y_true, y_pred):
#     y_true = np.asarray(y_true).reshape(-1)
#     y_pred = np.asarray(y_pred).reshape(-1)
#     mae = float(mean_absolute_error(y_true, y_pred))
#     rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
#     r2 = float(r2_score(y_true, y_pred)) if y_true.size >= 2 else float("nan")
#     return {"MAE": mae, "RMSE": rmse, "R2": r2}
# 
# # ==============================================================
# # (2) sample weights + oversampling
# # ==============================================================
# def make_sample_weight(y_raw):
#     y = np.asarray(y_raw, dtype=np.float64).reshape(-1)
#     if not USE_PEAK_WEIGHTING or y.size == 0:
#         return None
#     y_max = float(np.max(y)) + 1e-12
#     w = 1.0 + W_C * (y / y_max) ** W_P
#     # guard
#     w = np.clip(w, 1.0, 1.0 + W_C)
#     return w.astype(np.float32)
# 
# def oversample_peaks(X, y, w=None, q=0.90, multiplier=2):
#     if not USE_PEAK_OVERSAMPLING or multiplier <= 1:
#         return X, y, w
#     y = np.asarray(y).reshape(-1)
#     thr = float(np.quantile(y, q))
#     mask = y >= thr
#     if np.sum(mask) == 0:
#         return X, y, w
#     X_peak = X[mask]
#     y_peak = y[mask]
#     reps = multiplier - 1
#     X_os = np.concatenate([X, np.repeat(X_peak, reps, axis=0)], axis=0)
#     y_os = np.concatenate([y, np.repeat(y_peak, reps, axis=0)], axis=0)
#     if w is not None:
#         w_peak = w[mask]
#         w_os = np.concatenate([w, np.repeat(w_peak, reps, axis=0)], axis=0)
#     else:
#         w_os = None
#     return X_os, y_os, w_os
# 
# # ==============================================================
# # (4) Outlier handling on y
# # ==============================================================
# def clean_and_clip_y(y_raw):
#     y = np.asarray(y_raw, dtype=np.float64).reshape(-1)
#     keep = np.ones_like(y, dtype=bool)
# 
#     if REMOVE_NONFINITE:
#         keep &= np.isfinite(y)
# 
#     y2 = y[keep]
#     if y2.size == 0:
#         return y2, keep, {"removed_nonfinite": True, "winsorized": False, "iqr_filtered": False}
# 
#     info = {"removed_nonfinite": bool(np.any(~np.isfinite(y))), "winsorized": False, "iqr_filtered": False}
# 
#     if USE_WINSORIZE:
#         hi = float(np.quantile(y2, WINSOR_Q_HIGH))
#         y2 = np.minimum(y2, hi)
#         info["winsorized"] = True
#         info["winsor_hi"] = hi
# 
#     if USE_IQR_FILTER:
#         q1 = float(np.quantile(y2, 0.25))
#         q3 = float(np.quantile(y2, 0.75))
#         iqr = q3 - q1
#         lo = q1 - IQR_K * iqr
#         hi = q3 + IQR_K * iqr
#         keep2 = (y2 >= lo) & (y2 <= hi)
#         # map back to original keep mask
#         idx_keep = np.where(keep)[0]
#         keep[idx_keep[~keep2]] = False
#         y2 = y[keep]
#         info["iqr_filtered"] = True
#         info["iqr_lo"] = lo
#         info["iqr_hi"] = hi
# 
#     return y2, keep, info
# 
# # ==============================================================
# # (5) CV evaluation on train+val
# # ==============================================================
# def kfold_cv_score(model_kind, X, y_raw, sample_weight=None, folds=5):
#     """
#     Returns mean/std MAE on log-scale training target (if enabled),
#     but reports MAE in original y units as well.
#     """
#     from sklearn.model_selection import KFold
# 
#     X = np.asarray(X)
#     y_raw = np.asarray(y_raw).reshape(-1)
#     kf = KFold(n_splits=folds, shuffle=True, random_state=SEED)
# 
#     maes = []
#     r2s = []
#     for tr, va in kf.split(X):
#         Xtr, Xva = X[tr], X[va]
#         ytr_raw, yva_raw = y_raw[tr], y_raw[va]
# 
#         wtr = sample_weight[tr] if sample_weight is not None else None
# 
#         # train target
#         if USE_LOG_TARGET:
#             ytr = np.log1p(ytr_raw)
#         else:
#             ytr = ytr_raw
# 
#         model, _ = build_regressor()
#         try:
#             if model_kind == "lightgbm":
#                 model.fit(Xtr, ytr, sample_weight=wtr)
#             else:
#                 model.fit(Xtr, ytr)
#         except TypeError:
#             model.fit(Xtr, ytr)
# 
#         yhat = model.predict(Xva)
#         if USE_LOG_TARGET:
#             yhat_raw = np.expm1(yhat)
#         else:
#             yhat_raw = yhat
# 
#         maes.append(mean_absolute_error(yva_raw, yhat_raw))
#         r2s.append(r2_score(yva_raw, yhat_raw) if len(yva_raw) > 1 else np.nan)
# 
#     return {
#         "cv_mae_mean": float(np.nanmean(maes)),
#         "cv_mae_std": float(np.nanstd(maes)),
#         "cv_r2_mean": float(np.nanmean(r2s)),
#         "cv_r2_std": float(np.nanstd(r2s)),
#         "folds": int(folds)
#     }
# 
# # ==============================================================
# # MAIN helpers to build dataset
# # ==============================================================
# def build_dataset_for_height(h_tag, include_height_feature=False):
#     x_data_path, y_data_path = get_data_paths_for_height(h_tag)
#     if not os.path.exists(x_data_path) or not os.path.exists(y_data_path):
#         return None, None, None
# 
#     X_dict = np.load(x_data_path, allow_pickle=True).item()
#     Y_dict = np.load(y_data_path, allow_pickle=True).item()
#     common_keys = sorted(set(X_dict.keys()) & set(Y_dict.keys()))
#     if not common_keys:
#         return None, None, None
# 
#     X_list, y_list = [], []
#     h_val = float(height_values[h_tag])
# 
#     for k in common_keys:
#         x = X_dict[k].reshape(-1, 1).astype("float32")
#         y = Y_dict[k].reshape(-1, 1).astype("float32")
# 
#         feats = extract_features_from_acc(x, DT)
#         if include_height_feature:
#             feats = np.concatenate([feats, np.asarray([h_val], dtype=np.float32)], axis=0)
# 
#         X_list.append(feats)
#         y_list.append(y_max_abs_from_disp(y))
# 
#     X = np.vstack(X_list).astype(np.float32)
#     y = np.asarray(y_list, dtype=np.float64).reshape(-1)
# 
#     return X, y, common_keys
# 
# # ==============================================================
# # MAIN: MODE 1 (Multi-height)
# # ==============================================================
# if USE_MULTI_HEIGHT:
#     X_all, y_all = [], []
# 
#     for h_tag in height_tags:
#         print("\n" + "#" * 80)
#         print(f"ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹: {h_tag}")
#         print("#" * 80)
# 
#         Xh, yh, _ = build_dataset_for_height(h_tag, include_height_feature=True)
#         if Xh is None:
#             print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {h_tag} Ú©Ø§Ù…Ù„ Ù†Ø¨ÙˆØ¯ â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         X_all.append(Xh)
#         y_all.append(yh)
# 
#     if not X_all:
#         raise ValueError("âŒ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
# 
#     X_all = np.vstack(X_all).astype(np.float32)
#     y_all = np.concatenate(y_all).astype(np.float64).reshape(-1)
# 
#     # (4) clean / winsorize / (optional) IQR filter
#     y_clean, keep_mask, out_info = clean_and_clip_y(y_all)
#     X_all = X_all[keep_mask]
#     y_all = y_clean
# 
#     num_samples = len(y_all)
#     print(f"\nğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ (Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ): {num_samples}")
#     print(f"ğŸ§¾ Feature dim: {X_all.shape[1]} (includes height)")
# 
#     meta_dir = os.path.join(base_model_root, f"_META_{MODE_CODE}-{TRAIN_CODE}")
#     idx, split_idx_path = get_or_make_split_indices(meta_dir, f"split_idx_seed{SEED}.npy", num_samples)
# 
#     train_split = int(0.50 * num_samples)
#     val_split   = int(0.63 * num_samples)
# 
#     train_ids = idx[:train_split]
#     val_ids   = idx[train_split:val_split]
#     test_ids  = idx[val_split:]
# 
#     X_train, y_train_raw = X_all[train_ids], y_all[train_ids]
#     X_val,   y_val_raw   = X_all[val_ids],   y_all[val_ids]
#     X_test,  y_test_raw  = X_all[test_ids],  y_all[test_ids]
# 
#     # (2) weights + (2) oversampling (train only)
#     w_train = make_sample_weight(y_train_raw)
#     X_train_os, y_train_os_raw, w_train_os = oversample_peaks(
#         X_train, y_train_raw, w_train, q=PEAK_Q, multiplier=PEAK_MULTIPLIER
#     )
# 
#     # train targets
#     y_train = np.log1p(y_train_os_raw) if USE_LOG_TARGET else y_train_os_raw
#     y_val   = np.log1p(y_val_raw)      if USE_LOG_TARGET else y_val_raw
#     y_test  = np.log1p(y_test_raw)     if USE_LOG_TARGET else y_test_raw
# 
#     print(f"\nğŸ“¦ Global split | Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}")
#     print(f"ğŸ“Œ After oversampling: Train = {X_train_os.shape[0]} samples (peaks q={PEAK_Q}, x{PEAK_MULTIPLIER})")
# 
#     # build model (default params; CV is evaluation/reporting)
#     model, model_kind = build_regressor()
# 
#     run_tag = f"{script_name}__{MODE_CODE}-{TRAIN_CODE}__MAXABS__{model_kind}"
#     model_dir = os.path.join(base_model_root, run_tag)
#     os.makedirs(model_dir, exist_ok=True)
# 
#     model_path = os.path.join(model_dir, "model.pkl")
#     metrics_path = os.path.join(model_dir, "metrics.json")
# 
#     runinfo = {
#         "status": "STARTED",
#         "run_tag": run_tag,
#         "script_name": script_name,
#         "script_tag": script_tag,
#         "seed": SEED,
#         "dt": DT,
#         "spectral_periods": SPECTRAL_PERIODS,
#         "damping": DAMPING,
#         "use_log_target": USE_LOG_TARGET,
#         "use_peak_weighting": USE_PEAK_WEIGHTING,
#         "use_peak_oversampling": USE_PEAK_OVERSAMPLING,
#         "peak_q": PEAK_Q,
#         "peak_multiplier": PEAK_MULTIPLIER,
#         "winsorize": USE_WINSORIZE,
#         "winsor_q_high": WINSOR_Q_HIGH,
#         "iqr_filter": USE_IQR_FILTER,
#         "iqr_k": IQR_K,
#         "outlier_info": out_info,
#         "use_clustered": USE_CLUSTERED,
#         "cluster_label": cluster_label,
#         "mode_code": MODE_CODE,
#         "train_code": TRAIN_CODE,
#         "gm_root_dir": gm_root_dir,
#         "tha_root_dir": tha_root_dir,
#         "split_idx_path": split_idx_path,
#         "num_samples": int(num_samples),
#         "feature_dim": int(X_all.shape[1]),
#         "target": "y = max(abs(disp(t)))",
#         "model_kind": model_kind
#     }
#     write_runinfo(model_dir, runinfo)
# 
#     # (5) CV evaluation on train+val (in original units)
#     cv_metrics = None
#     if USE_CV_EVAL:
#         X_tv = np.vstack([X_train, X_val])
#         y_tv = np.concatenate([y_train_raw, y_val_raw])
#         w_tv = make_sample_weight(y_tv)
#         try:
#             cv_metrics = kfold_cv_score(model_kind, X_tv, y_tv, sample_weight=w_tv, folds=CV_FOLDS)
#             print("ğŸ“Œ CV metrics (train+val):", cv_metrics)
#         except Exception as e:
#             print("âš ï¸ CV failed:", repr(e))
# 
#     # fit
#     try:
#         if model_kind == "lightgbm":
#             model.fit(
#                 X_train_os, y_train,
#                 sample_weight=w_train_os,
#                 eval_set=[(X_val, y_val)],
#                 eval_metric="l1",
#                 verbose=50
#             )
#         else:
#             model.fit(X_train_os, y_train)
#     except TypeError:
#         model.fit(X_train_os, y_train)
# 
#     # predict + inverse transform
#     y_pred_train = model.predict(X_train)
#     y_pred_val   = model.predict(X_val)
#     y_pred_test  = model.predict(X_test)
# 
#     if USE_LOG_TARGET:
#         y_pred_train_raw = np.expm1(y_pred_train)
#         y_pred_val_raw   = np.expm1(y_pred_val)
#         y_pred_test_raw  = np.expm1(y_pred_test)
#     else:
#         y_pred_train_raw = y_pred_train
#         y_pred_val_raw   = y_pred_val
#         y_pred_test_raw  = y_pred_test
# 
#     metrics = {
#         "train": compute_metrics(y_train_raw, y_pred_train_raw),
#         "val":   compute_metrics(y_val_raw,   y_pred_val_raw),
#         "test":  compute_metrics(y_test_raw,  y_pred_test_raw),
#     }
#     if cv_metrics is not None:
#         metrics["cv_trainval"] = cv_metrics
# 
#     joblib.dump(model, model_path)
#     save_metrics_json(metrics_path, metrics)
# 
#     runinfo["status"] = "FINISHED"
#     runinfo["model_path"] = model_path
#     runinfo["metrics_path"] = metrics_path
#     runinfo["final_metrics_test"] = metrics["test"]
#     if cv_metrics is not None:
#         runinfo["cv_trainval"] = cv_metrics
#     write_runinfo(model_dir, runinfo)
# 
#     print("\nâœ… Training finished:", run_tag)
#     print("ğŸ“Œ Test metrics:", metrics["test"])
# 
#     plot_pred(y_test_raw, y_pred_test_raw, os.path.join(model_dir, "y_true_vs_pred.png"),
#               title=f"{run_tag} | TEST | MAE={metrics['test']['MAE']:.5f}, RMSE={metrics['test']['RMSE']:.5f}, R2={metrics['test']['R2']:.3f}")
#     plot_err_hist((y_pred_test_raw - y_test_raw), os.path.join(model_dir, "error_hist.png"),
#                   title=f"{run_tag} | error histogram (TEST)")
# 
# # ==============================================================
# # MAIN: MODE 2 (Per-height)
# # ==============================================================
# else:
#     for h_tag in height_tags:
#         print("\n" + "#" * 80)
#         print(f"ğŸ—ï¸ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹: {h_tag}")
#         print("#" * 80)
# 
#         X_all, y_all, _ = build_dataset_for_height(h_tag, include_height_feature=False)
#         if X_all is None:
#             print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {h_tag} Ú©Ø§Ù…Ù„ Ù†Ø¨ÙˆØ¯ â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
#             continue
# 
#         # (4) clean / winsorize / (optional) IQR filter
#         y_clean, keep_mask, out_info = clean_and_clip_y(y_all)
#         X_all = X_all[keep_mask]
#         y_all = y_clean
# 
#         num_samples = len(y_all)
#         if num_samples == 0:
#             print(f"âš ï¸ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {h_tag} Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯.")
#             continue
# 
#         print(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {h_tag} (Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ): {num_samples}")
#         print(f"ğŸ§¾ Feature dim: {X_all.shape[1]}")
# 
#         height_meta_dir = os.path.join(base_model_root, f"_META_{MODE_CODE}-{TRAIN_CODE}_{h_tag}")
#         idx, split_idx_path = get_or_make_split_indices(height_meta_dir, f"split_idx_seed{SEED}.npy", num_samples)
# 
#         train_split = int(0.50 * num_samples)
#         val_split   = int(0.63 * num_samples)
# 
#         train_ids = idx[:train_split]
#         val_ids   = idx[train_split:val_split]
#         test_ids  = idx[val_split:]
# 
#         X_train, y_train_raw = X_all[train_ids], y_all[train_ids]
#         X_val,   y_val_raw   = X_all[val_ids],   y_all[val_ids]
#         X_test,  y_test_raw  = X_all[test_ids],  y_all[test_ids]
# 
#         # (2) weights + oversampling (train only)
#         w_train = make_sample_weight(y_train_raw)
#         X_train_os, y_train_os_raw, w_train_os = oversample_peaks(
#             X_train, y_train_raw, w_train, q=PEAK_Q, multiplier=PEAK_MULTIPLIER
#         )
# 
#         # train targets
#         y_train = np.log1p(y_train_os_raw) if USE_LOG_TARGET else y_train_os_raw
#         y_val   = np.log1p(y_val_raw)      if USE_LOG_TARGET else y_val_raw
#         y_test  = np.log1p(y_test_raw)     if USE_LOG_TARGET else y_test_raw
# 
#         model, model_kind = build_regressor()
# 
#         run_tag = f"{script_name}__{MODE_CODE}-{TRAIN_CODE}_{h_tag}__MAXABS__{model_kind}"
#         model_dir = os.path.join(base_model_root, run_tag)
#         os.makedirs(model_dir, exist_ok=True)
# 
#         model_path = os.path.join(model_dir, "model.pkl")
#         metrics_path = os.path.join(model_dir, "metrics.json")
# 
#         runinfo = {
#             "status": "STARTED",
#             "run_tag": run_tag,
#             "height": h_tag,
#             "script_name": script_name,
#             "script_tag": script_tag,
#             "seed": SEED,
#             "dt": DT,
#             "spectral_periods": SPECTRAL_PERIODS,
#             "damping": DAMPING,
#             "use_log_target": USE_LOG_TARGET,
#             "use_peak_weighting": USE_PEAK_WEIGHTING,
#             "use_peak_oversampling": USE_PEAK_OVERSAMPLING,
#             "peak_q": PEAK_Q,
#             "peak_multiplier": PEAK_MULTIPLIER,
#             "winsorize": USE_WINSORIZE,
#             "winsor_q_high": WINSOR_Q_HIGH,
#             "iqr_filter": USE_IQR_FILTER,
#             "iqr_k": IQR_K,
#             "outlier_info": out_info,
#             "use_clustered": USE_CLUSTERED,
#             "cluster_label": cluster_label,
#             "mode_code": MODE_CODE,
#             "train_code": TRAIN_CODE,
#             "gm_root_dir": gm_root_dir,
#             "tha_root_dir": tha_root_dir,
#             "split_idx_path": split_idx_path,
#             "num_samples": int(num_samples),
#             "feature_dim": int(X_all.shape[1]),
#             "target": "y = max(abs(disp(t)))",
#             "model_kind": model_kind
#         }
#         write_runinfo(model_dir, runinfo)
# 
#         # (5) CV evaluation on train+val (in original units)
#         cv_metrics = None
#         if USE_CV_EVAL:
#             X_tv = np.vstack([X_train, X_val])
#             y_tv = np.concatenate([y_train_raw, y_val_raw])
#             w_tv = make_sample_weight(y_tv)
#             try:
#                 cv_metrics = kfold_cv_score(model_kind, X_tv, y_tv, sample_weight=w_tv, folds=CV_FOLDS)
#                 print("ğŸ“Œ CV metrics (train+val):", cv_metrics)
#             except Exception as e:
#                 print("âš ï¸ CV failed:", repr(e))
# 
#         # fit
#         try:
#             if model_kind == "lightgbm":
#                 model.fit(
#                     X_train_os, y_train,
#                     sample_weight=w_train_os,
#                     eval_set=[(X_val, y_val)],
#                     eval_metric="l1",
#                     verbose=50
#                 )
#             else:
#                 model.fit(X_train_os, y_train)
#         except TypeError:
#             model.fit(X_train_os, y_train)
# 
#         y_pred_train = model.predict(X_train)
#         y_pred_val   = model.predict(X_val)
#         y_pred_test  = model.predict(X_test)
# 
#         if USE_LOG_TARGET:
#             y_pred_train_raw = np.expm1(y_pred_train)
#             y_pred_val_raw   = np.expm1(y_pred_val)
#             y_pred_test_raw  = np.expm1(y_pred_test)
#         else:
#             y_pred_train_raw = y_pred_train
#             y_pred_val_raw   = y_pred_val
#             y_pred_test_raw  = y_pred_test
# 
#         metrics = {
#             "train": compute_metrics(y_train_raw, y_pred_train_raw),
#             "val":   compute_metrics(y_val_raw,   y_pred_val_raw),
#             "test":  compute_metrics(y_test_raw,  y_pred_test_raw),
#         }
#         if cv_metrics is not None:
#             metrics["cv_trainval"] = cv_metrics
# 
#         joblib.dump(model, model_path)
#         save_metrics_json(metrics_path, metrics)
# 
#         runinfo["status"] = "FINISHED"
#         runinfo["model_path"] = model_path
#         runinfo["metrics_path"] = metrics_path
#         runinfo["final_metrics_test"] = metrics["test"]
#         if cv_metrics is not None:
#             runinfo["cv_trainval"] = cv_metrics
#         write_runinfo(model_dir, runinfo)
# 
#         print("\nâœ… Training finished:", run_tag)
#         print("ğŸ“Œ Test metrics:", metrics["test"])
# 
#         plot_pred(y_test_raw, y_pred_test_raw, os.path.join(model_dir, "y_true_vs_pred.png"),
#                   title=f"{run_tag} | TEST | MAE={metrics['test']['MAE']:.5f}, RMSE={metrics['test']['RMSE']:.5f}, R2={metrics['test']['R2']:.3f}")
#         plot_err_hist((y_pred_test_raw - y_test_raw), os.path.join(model_dir, "error_hist.png"),
#                       title=f"{run_tag} | error histogram (TEST)")
# 
#     print("ğŸ‰ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¯Ø± Ù…ÙˆØ¯ per-height ØªÙ…Ø§Ù… Ø´Ø¯.")
# 
# =============================================================================

import sys, io
if hasattr(sys.stdout, "buffer"):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')

import os, re, json, random, math
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

plt.ioff()

# ==============================================================
# ğŸ”’ Reproducibility
# ==============================================================
SEED = 1234
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)

# ==============================================================
# âš™ï¸ Improvements toggles
# ==============================================================
USE_LOG_TARGET = True

# Peak weighting (sample_weight): w = 1 + W_C * (y / y_max) ** W_P
USE_PEAK_WEIGHTING = True
W_C = 8.0   # âœ… (3) stronger peak weighting
W_P = 2.0

# Optional oversampling of peak records in TRAIN only
USE_PEAK_OVERSAMPLING = True
PEAK_Q = 0.95          # âœ… (3) oversample more extreme records
PEAK_MULTIPLIER = 3    # âœ… (3) replicate peaks more

# Outlier handling
REMOVE_NONFINITE = True
USE_WINSORIZE = True
WINSOR_Q_HIGH = 0.995  # (unchanged per your request)
USE_IQR_FILTER = False
IQR_K = 4.0

# Split evaluation: K-fold CV on Train+Val (keeps fixed TEST split)
USE_CV_EVAL = True
CV_FOLDS = 5

# Spectrum features (Sa(T)) settings
DAMPING = 0.05
SPECTRAL_PERIODS = [0.1, 0.2, 0.5, 1.0, 2.0]

# ==============================================================
# ğŸ“ Paths + Linear / Nonlinear selection
# ==============================================================
base_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.abspath(os.path.join(base_dir, os.pardir))
output_root_dir = os.path.join(root_dir, "Output")

choice = input(
    "Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø³Ø® Ø®Ø·ÛŒ Ø¨Ø§Ø´Ø¯ ÛŒØ§ ØºÛŒØ±Ø®Ø·ÛŒØŸ "
    "Ø¨Ø±Ø§ÛŒ Ø®Ø·ÛŒ Ø¹Ø¯Ø¯ 1 Ùˆ Ø¨Ø±Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ Ø¹Ø¯Ø¯ 0 Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†: "
).strip()
is_linear = (choice == "1")

if is_linear:
    print("ğŸ“Œ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ (THA_linear) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
    gm_root_dir  = os.path.join(output_root_dir, "3_GM_Fixed_train_linear")
    tha_root_dir = os.path.join(output_root_dir, "3_THA_Fixed_train_linear")
    base_model_root = os.path.join(output_root_dir, "Progress_of_MaxDisp_linear")
else:
    print("ğŸ“Œ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ (THA_nonlinear) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
    gm_root_dir  = os.path.join(output_root_dir, "3_GM_Fixed_train_nonlinear")
    tha_root_dir = os.path.join(output_root_dir, "3_THA_Fixed_train_nonlinear")
    base_model_root = os.path.join(output_root_dir, "Progress_of_MaxDisp_nonlinear")

os.makedirs(base_model_root, exist_ok=True)

print("ğŸ“‚ GM root dir :", gm_root_dir)
print("ğŸ“‚ THA root dir:", tha_root_dir)
print("ğŸ“‚ Base model root:", base_model_root)
print()

# ==============================================================
# ğŸ•’ dt needed for IA/CAV/duration/PGV/PGD/Sa(T)
# ==============================================================
dt_in = input("Ú¯Ø§Ù… Ø²Ù…Ø§Ù†ÛŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ (dt) Ú†Ù†Ø¯ Ø§Ø³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹ 0.01) Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒ 0.01 Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…: ").strip()
try:
    DT = float(dt_in) if dt_in else 0.01
    if DT <= 0:
        raise ValueError
except Exception:
    print("âš ï¸ dt Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ø› 0.01 Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
    DT = 0.01

print(f"â±ï¸ dt = {DT}")

# ==============================================================
# ğŸ§¾ Script-tag isolation
# ==============================================================
script_name = os.path.splitext(os.path.basename(__file__))[0]

m = re.search(r"(v\d+)", script_name, flags=re.IGNORECASE)
if m:
    script_tag = m.group(1).upper()
else:
    script_tag = re.sub(r"[^A-Za-z0-9]+", "", script_name)[:12] or "SCRIPT"

base_model_root = os.path.join(base_model_root, script_tag)
os.makedirs(base_model_root, exist_ok=True)

# ==============================================================
# ğŸ” Clustered vs non-clustered
# ==============================================================
print("-------------------------------------------")
print("Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ *Ú©Ù„Ø§Ø³ØªØ±Ø´Ø¯Ù‡* Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù…ØŸ")
print("   1 = Ø¨Ù„Ù‡ØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ cluster_balanced_global Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†")
print("   0 = Ø®ÛŒØ±ØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ X_data_H* Ùˆ Y_data_H* Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†")
print("-------------------------------------------\n")

cluster_choice = input("Ø§Ù†ØªØ®Ø§Ø¨Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (1 ÛŒØ§ 0): ").strip()
USE_CLUSTERED = (cluster_choice == "1")

cluster_label = ""
if USE_CLUSTERED:
    print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ØªØ±Ø´Ø¯Ù‡ (cluster_balanced_global).")
    cluster_label = input(
        "Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù… Ù¾ÙˆØ´Ù‡Ù” Ù…Ø¯Ù„ØŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³ØªØ±Ù‡Ø§ (K) Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ "
        "(Ù…Ø«Ù„Ø§Ù‹ 4). Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒØŒ 'clustered' Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: "
    ).strip()
else:
    print("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø¯ÙˆÙ† Ú©Ù„Ø§Ø³ØªØ±ÛŒÙ†Ú¯.")

def get_mode_code():
    if not USE_CLUSTERED:
        return "NC"
    if cluster_label:
        return f"CK{cluster_label}"
    return "CL"

MODE_CODE = get_mode_code()

print("\nğŸ“‚ Base model root for this code version:")
print("   ", base_model_root)
print()

# ==============================================================
# ğŸ§­ Data folders check
# ==============================================================
if not os.path.isdir(gm_root_dir):
    raise FileNotFoundError(f"âŒ GM root dir Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {gm_root_dir}")
if not os.path.isdir(tha_root_dir):
    raise FileNotFoundError(f"âŒ THA root dir Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {tha_root_dir}")

available_heights = sorted(
    name for name in os.listdir(gm_root_dir)
    if os.path.isdir(os.path.join(gm_root_dir, name)) and name.startswith("H")
)
if not available_heights:
    raise ValueError(f"âŒ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ H* Ø¯Ø± {gm_root_dir} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ø±Ø­Ù„Ù‡ Û³ Ø±Ø§ Ú†Ú© Ú©Ù†.")

print("ğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
for h in available_heights:
    print("  -", h)

print("\n-------------------------------------------")
print("Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŸ")
print("Ù…Ø«Ø§Ù„: H2 H3 H4  ÛŒØ§  2 3 4  (Ø®Ø§Ù„ÛŒ = Ù‡Ù…Ù‡)")
print("-------------------------------------------\n")

heights_raw = input("Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (Ø®Ø§Ù„ÛŒ = Ù‡Ù…Ù‡): ").strip()

if not heights_raw:
    height_tags = available_heights[:]
    print("\nâœ… Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù†Ø¯.")
else:
    tokens = heights_raw.replace(',', ' ').split()
    selected, invalid = [], []
    for tok in tokens:
        tok = tok.strip()
        if not tok:
            continue
        if tok.startswith("H"):
            tag = tok
        else:
            try:
                v = float(tok)
                if v.is_integer():
                    tag = f"H{int(v)}"
                else:
                    tag = "H" + str(v).replace('.', 'p')
            except Exception:
                invalid.append(tok)
                continue
        if tag in available_heights:
            selected.append(tag)
        else:
            invalid.append(tok)
    height_tags = list(dict.fromkeys(selected))
    if invalid:
        print("\nâš ï¸ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ù‡Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù†Ø¯:")
        for x in invalid:
            print("  -", x)
    if not height_tags:
        print("âŒ Ù‡ÛŒÚ† Ø§Ø±ØªÙØ§Ø¹ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ø› Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        sys.exit(1)

print("\nğŸ“ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:")
print("  " + ", ".join(height_tags))
print()

def height_value_from_tag(h_tag: str) -> float:
    s = h_tag[1:]
    s = s.replace('p', '.')
    return float(s)

height_values = {h_tag: height_value_from_tag(h_tag) for h_tag in height_tags}
print("ğŸ”¢ Ù†Ú¯Ø§Ø´Øª Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ (tag â†’ value):")
for h_tag, hv in height_values.items():
    print(f"  {h_tag} â†’ {hv}")

# ==============================================================
# â†”ï¸ Training mode selection
# ==============================================================
print("\n-------------------------------------------")
print("Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ Ù‡Ù…Ù‡Ù” Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù… Ùˆ Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ØŸ")
print("   1 = Ø¨Ù„Ù‡ØŒ ÛŒÚ© Ù…Ø¯Ù„ Ù…Ø´ØªØ±Ú© Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹ (Multi-height + Feature H)")
print("   0 = Ø®ÛŒØ±ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…Ø¯Ù„ Ù…Ø³ØªÙ‚Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯")
print("-------------------------------------------\n")

mh_choice = input("Ø§Ù†ØªØ®Ø§Ø¨Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (1 ÛŒØ§ 0): ").strip()
USE_MULTI_HEIGHT = (mh_choice == "1")

if USE_MULTI_HEIGHT:
    print("âœ… Ù…ÙˆØ¯ Û±: Ù…Ø¯Ù„ Ù…Ø´ØªØ±Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ ÙÛŒÚ†Ø± Ø§Ø±ØªÙØ§Ø¹")
    TRAIN_CODE = "GH"
else:
    print("âœ… Ù…ÙˆØ¯ Û²: Ù…Ø¯Ù„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø±ØªÙØ§Ø¹")
    TRAIN_CODE = "PH"

# ==============================================================
# Paths helper (unchanged)
# ==============================================================
def get_data_paths_for_height(h_tag):
    if USE_CLUSTERED:
        gm_dir  = os.path.join(gm_root_dir,  h_tag, "cluster_balanced_global")
        tha_dir = os.path.join(tha_root_dir, h_tag, "cluster_balanced_global")
        x_name  = f"X_data_cluster_balanced_global_{h_tag}.npy"
        y_name  = f"Y_data_cluster_balanced_global_{h_tag}.npy"
    else:
        gm_dir  = os.path.join(gm_root_dir,  h_tag)
        tha_dir = os.path.join(tha_root_dir, h_tag)
        x_name  = f"X_data_{h_tag}.npy"
        y_name  = f"Y_data_{h_tag}.npy"
    return os.path.join(gm_dir, x_name), os.path.join(tha_dir, y_name)

# ==============================================================
# ğŸ§  Feature extraction utilities
# ==============================================================
G = 9.80665

def _detrend_linear(x: np.ndarray) -> np.ndarray:
    """Remove linear trend: x - (a*t + b)."""
    x = np.asarray(x, dtype=np.float64).reshape(-1)
    n = x.size
    if n < 3:
        return x.copy()
    t = np.arange(n, dtype=np.float64)
    A = np.vstack([t, np.ones(n)]).T
    coef, *_ = np.linalg.lstsq(A, x, rcond=None)
    trend = A @ coef
    return x - trend

def integrate_trapz_cumsum(x: np.ndarray, dt: float) -> np.ndarray:
    """Cumulative trapezoidal integral with dt, using only numpy."""
    x = np.asarray(x, dtype=np.float64).reshape(-1)
    if x.size == 0:
        return x
    y = np.zeros_like(x)
    if x.size == 1:
        y[0] = 0.0
        return y
    y[1:] = np.cumsum(0.5 * (x[1:] + x[:-1]) * dt)
    return y

def compute_pgv_pgd(a: np.ndarray, dt: float):
    """Robust-ish PGV/PGD from acceleration via detrend + trapezoid integration."""
    a = np.asarray(a, dtype=np.float64).reshape(-1)
    if a.size < 4:
        return 0.0, 0.0
    a0 = a - np.mean(a)
    v = integrate_trapz_cumsum(a0, dt)
    v = _detrend_linear(v)
    d = integrate_trapz_cumsum(v, dt)
    d = _detrend_linear(d)
    pgv = float(np.max(np.abs(v)))
    pgd = float(np.max(np.abs(d)))
    return pgv, pgd

def compute_arias_intensity(a: np.ndarray, dt: float):
    """Arias intensity IA = pi/(2g) * âˆ« a(t)^2 dt."""
    a = np.asarray(a, dtype=np.float64).reshape(-1)
    if a.size < 2:
        return 0.0
    ia = (math.pi / (2.0 * G)) * float(np.sum(a * a) * dt)
    return ia

def compute_cav(a: np.ndarray, dt: float):
    """Cumulative Absolute Velocity CAV = âˆ« |a(t)| dt."""
    a = np.asarray(a, dtype=np.float64).reshape(-1)
    if a.size < 2:
        return 0.0
    return float(np.sum(np.abs(a)) * dt)

def significant_duration_from_ia(a: np.ndarray, dt: float, p1=0.05, p2=0.95):
    """Compute significant duration between p1 and p2 of cumulative IA."""
    a = np.asarray(a, dtype=np.float64).reshape(-1)
    if a.size < 2:
        return 0.0
    ia_cum = np.cumsum(a * a) * dt
    total = float(ia_cum[-1])
    if total <= 1e-18:
        return 0.0
    ia_norm = ia_cum / total
    t = np.arange(a.size) * dt
    t1 = float(np.interp(p1, ia_norm, t))
    t2 = float(np.interp(p2, ia_norm, t))
    return max(0.0, t2 - t1)

def safe_fft_features(x, n_bins=6):
    """
    Simple, stable FFT band-energy features.
    Returns: [dom_norm, centroid_norm] + n_bins band ratios.
    """
    x = np.asarray(x, dtype=np.float64).reshape(-1)
    if x.size < 8:
        return [0.0] * (2 + n_bins)

    x0 = x - np.mean(x)
    spec = np.fft.rfft(x0)
    mag = np.abs(spec)

    total = float(np.sum(mag) + 1e-12)

    dom_idx = int(np.argmax(mag))
    dom_norm = dom_idx / max(1, (mag.size - 1))

    idxs = np.arange(mag.size, dtype=np.float64)
    centroid = float(np.sum(idxs * mag) / (np.sum(mag) + 1e-12))
    centroid_norm = centroid / max(1.0, (mag.size - 1))

    bands = []
    edges = np.linspace(0, mag.size, n_bins + 1, dtype=int)
    for i in range(n_bins):
        a, b = edges[i], edges[i + 1]
        if b <= a:
            bands.append(0.0)
        else:
            bands.append(float(np.sum(mag[a:b]) / total))

    return [dom_norm, centroid_norm] + bands

def newmark_sa(acc, dt, T, zeta=0.05):
    """
    Compute pseudo-spectral acceleration Sa(T) for an SDOF using
    Newmark-beta (average acceleration) method.
    Returns Sa in same acceleration units as input acc.
    """
    acc = np.asarray(acc, dtype=np.float64).reshape(-1)
    n = acc.size
    if n < 4 or T <= 0:
        return 0.0

    w = 2.0 * math.pi / T
    k = w * w
    c = 2.0 * zeta * w

    beta = 1.0 / 4.0
    gamma = 1.0 / 2.0

    u = 0.0
    v = 0.0
    a_rel = 0.0

    a0 = 1.0 / (beta * dt * dt)
    a1 = gamma / (beta * dt)
    a2 = 1.0 / (beta * dt)
    a3 = 1.0 / (2.0 * beta) - 1.0
    a4 = gamma / beta - 1.0
    a5 = dt * (gamma / (2.0 * beta) - 1.0)

    k_eff = k + a0 + a1 * c

    umax = 0.0
    for i in range(1, n):
        p_i = -acc[i]
        p_prev = -acc[i - 1]
        dp = p_i - p_prev

        p_eff = dp + (a0 * u + a2 * v + a3 * a_rel) + c * (a1 * u + a4 * v + a5 * a_rel)

        du = p_eff / k_eff
        dv = a1 * du - a4 * v - a5 * a_rel
        da = a0 * du - a2 * v - a3 * a_rel

        u += du
        v += dv
        a_rel += da

        if abs(u) > umax:
            umax = abs(u)

    sa = (w * w) * umax
    return float(sa)

def spectral_features(acc, dt, periods, zeta=0.05):
    feats = []
    for T in periods:
        feats.append(newmark_sa(acc, dt, float(T), zeta=zeta))
    return feats

def extract_features_from_acc(x_series, dt):
    """
    x_series: shape (T,1) or (T,)
    returns: 1D feature vector (float32)
    """
    x = np.asarray(x_series).reshape(-1).astype(np.float64)
    if x.size == 0:
        return np.zeros((1,), dtype=np.float32)

    absx = np.abs(x)
    pga = float(np.max(absx))
    mean = float(np.mean(x))
    std = float(np.std(x) + 1e-12)
    rms = float(np.sqrt(np.mean(x * x)))
    energy = float(np.sum(x * x) * dt)

    p50 = float(np.percentile(absx, 50))
    p75 = float(np.percentile(absx, 75))
    p90 = float(np.percentile(absx, 90))
    p95 = float(np.percentile(absx, 95))
    p99 = float(np.percentile(absx, 99))

    thr = 0.10 * pga
    dur_ratio = float(np.mean(absx >= thr)) if pga > 1e-12 else 0.0

    sgn = np.sign(x)
    zcr = float(np.mean(sgn[1:] != sgn[:-1])) if x.size > 2 else 0.0

    ia = compute_arias_intensity(x, dt)
    cav = compute_cav(x, dt)
    d595 = significant_duration_from_ia(x, dt, 0.05, 0.95)
    d575 = significant_duration_from_ia(x, dt, 0.05, 0.75)

    pgv, pgd = compute_pgv_pgd(x, dt)

    fft_feats = safe_fft_features(x, n_bins=6)

    sa_feats = spectral_features(x, dt, SPECTRAL_PERIODS, zeta=DAMPING)

    feats = [
        pga, mean, std, rms, energy,
        p50, p75, p90, p95, p99,
        dur_ratio, zcr,
        ia, cav, d595, d575,
        pgv, pgd,
    ] + fft_feats + sa_feats

    if pga > 1e-12:
        feats += [pgv / pga, pgd / max(pgv, 1e-12), cav / pga, ia / (pga * pga * dt * x.size + 1e-12)]
    else:
        feats += [0.0, 0.0, 0.0, 0.0]

    return np.asarray(feats, dtype=np.float32)

def y_max_abs_from_disp(y_series):
    y = np.asarray(y_series).reshape(-1).astype(np.float64)
    if y.size == 0:
        return 0.0
    return float(np.max(np.abs(y)))

# ==============================================================
# Model choice: LightGBM if available, else sklearn HGBR
# ==============================================================
def build_regressor(best_params=None):
    try:
        import lightgbm as lgb
        print("âœ… Using LightGBMRegressor.")
        params = dict(
            n_estimators=8000,        # âœ… (5) allow more, rely on early stopping
            learning_rate=0.02,
            num_leaves=127,           # âœ… (5)
            subsample=0.85,
            colsample_bytree=0.85,
            reg_alpha=0.0,
            reg_lambda=1.0,
            min_child_samples=10,     # âœ… (5)
            random_state=SEED,
            n_jobs=-1
        )
        if best_params:
            params.update(best_params)
        return lgb.LGBMRegressor(**params), "lightgbm"
    except Exception:
        from sklearn.ensemble import HistGradientBoostingRegressor
        print("âœ… LightGBM not found. Using HistGradientBoostingRegressor (sklearn).")
        hg_params = dict(
            learning_rate=0.05,
            max_depth=None,
            max_iter=900,
            random_state=SEED
        )
        if best_params:
            for k in ["learning_rate", "max_depth", "max_iter"]:
                if k in best_params:
                    hg_params[k] = best_params[k]
        return HistGradientBoostingRegressor(**hg_params), "sklearn_hgbr"

# ==============================================================
# I/O helpers
# ==============================================================
def write_runinfo(run_dir, info: dict):
    p = os.path.join(run_dir, "runinfo.txt")
    lines = []
    lines.append("===== RUN INFO =====")
    for k in sorted(info.keys()):
        lines.append(f"{k}: {info[k]}")
    with open(p, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")

def save_metrics_json(path, metrics: dict):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)

def compute_metrics(y_true, y_pred):
    y_true = np.asarray(y_true).reshape(-1)
    y_pred = np.asarray(y_pred).reshape(-1)
    mae = float(mean_absolute_error(y_true, y_pred))
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
    r2 = float(r2_score(y_true, y_pred)) if y_true.size >= 2 else float("nan")
    return {"MAE": mae, "RMSE": rmse, "R2": r2}

# âœ… Plot with: small points + black y=x + metrics bottom-right
def plot_pred(y_true, y_pred, out_path, title, metrics_dict=None):
    plt.figure()
    plt.scatter(y_true, y_pred, s=6)  # âœ… smaller points
    mn = min(float(np.min(y_true)), float(np.min(y_pred)))
    mx = max(float(np.max(y_true)), float(np.max(y_pred)))
    plt.plot([mn, mx], [mn, mx], color="k", linewidth=1.5)  # âœ… black line

    if metrics_dict is not None:
        txt = f"MAE={metrics_dict['MAE']:.5f}\nRMSE={metrics_dict['RMSE']:.5f}\nRÂ²={metrics_dict['R2']:.3f}"
        plt.text(
            0.98, 0.02, txt,
            transform=plt.gca().transAxes,
            ha="right", va="bottom",
            bbox=dict(boxstyle="round", facecolor="white", alpha=0.85, edgecolor="black")
        )

    plt.xlabel("True max|disp|")
    plt.ylabel("Pred max|disp|")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()

def plot_err_hist(err, out_path, title):
    plt.figure()
    plt.hist(err, bins=50)
    plt.xlabel("error = y_pred - y_true")
    plt.ylabel("count")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()

# ==============================================================
# Split helper (fixed split saved/loaded)
# ==============================================================
def get_or_make_split_indices(meta_dir, split_name, num_samples):
    os.makedirs(meta_dir, exist_ok=True)
    split_idx_path = os.path.join(meta_dir, split_name)
    if os.path.exists(split_idx_path):
        idx = np.load(split_idx_path)
        if len(idx) != num_samples:
            idx = np.arange(num_samples)
            rng = np.random.default_rng(SEED)
            rng.shuffle(idx)
            np.save(split_idx_path, idx)
            print("âš ï¸ Ø·ÙˆÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯Ø› split Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.")
        else:
            print("âœ… Fixed split indices loaded:", split_idx_path)
    else:
        idx = np.arange(num_samples)
        rng = np.random.default_rng(SEED)
        rng.shuffle(idx)
        np.save(split_idx_path, idx)
        print("âœ… Fixed split indices saved:", split_idx_path)
    return idx, split_idx_path

# ==============================================================
# (2) sample weights + oversampling
# ==============================================================
def make_sample_weight(y_raw):
    y = np.asarray(y_raw, dtype=np.float64).reshape(-1)
    if not USE_PEAK_WEIGHTING or y.size == 0:
        return None
    y_max = float(np.max(y)) + 1e-12
    w = 1.0 + W_C * (y / y_max) ** W_P
    w = np.clip(w, 1.0, 1.0 + W_C)
    return w.astype(np.float32)

def oversample_peaks(X, y, w=None, q=0.90, multiplier=2):
    if not USE_PEAK_OVERSAMPLING or multiplier <= 1:
        return X, y, w
    y = np.asarray(y).reshape(-1)
    thr = float(np.quantile(y, q))
    mask = y >= thr
    if np.sum(mask) == 0:
        return X, y, w
    X_peak = X[mask]
    y_peak = y[mask]
    reps = multiplier - 1
    X_os = np.concatenate([X, np.repeat(X_peak, reps, axis=0)], axis=0)
    y_os = np.concatenate([y, np.repeat(y_peak, reps, axis=0)], axis=0)
    if w is not None:
        w_peak = w[mask]
        w_os = np.concatenate([w, np.repeat(w_peak, reps, axis=0)], axis=0)
    else:
        w_os = None
    return X_os, y_os, w_os

# ==============================================================
# (4) Outlier handling on y
# ==============================================================
def clean_and_clip_y(y_raw):
    y = np.asarray(y_raw, dtype=np.float64).reshape(-1)
    keep = np.ones_like(y, dtype=bool)

    if REMOVE_NONFINITE:
        keep &= np.isfinite(y)

    y2 = y[keep]
    if y2.size == 0:
        return y2, keep, {"removed_nonfinite": True, "winsorized": False, "iqr_filtered": False}

    info = {"removed_nonfinite": bool(np.any(~np.isfinite(y))), "winsorized": False, "iqr_filtered": False}

    if USE_WINSORIZE:
        hi = float(np.quantile(y2, WINSOR_Q_HIGH))
        y2 = np.minimum(y2, hi)
        info["winsorized"] = True
        info["winsor_hi"] = hi

    if USE_IQR_FILTER:
        q1 = float(np.quantile(y2, 0.25))
        q3 = float(np.quantile(y2, 0.75))
        iqr = q3 - q1
        lo = q1 - IQR_K * iqr
        hi = q3 + IQR_K * iqr
        keep2 = (y2 >= lo) & (y2 <= hi)
        idx_keep = np.where(keep)[0]
        keep[idx_keep[~keep2]] = False
        y2 = y[keep]
        info["iqr_filtered"] = True
        info["iqr_lo"] = lo
        info["iqr_hi"] = hi

    return y2, keep, info

# ==============================================================
# (5) CV evaluation on train+val
# ==============================================================
def kfold_cv_score(model_kind, X, y_raw, sample_weight=None, folds=5):
    from sklearn.model_selection import KFold

    X = np.asarray(X)
    y_raw = np.asarray(y_raw).reshape(-1)
    kf = KFold(n_splits=folds, shuffle=True, random_state=SEED)

    maes = []
    r2s = []
    for tr, va in kf.split(X):
        Xtr, Xva = X[tr], X[va]
        ytr_raw, yva_raw = y_raw[tr], y_raw[va]

        wtr = sample_weight[tr] if sample_weight is not None else None

        if USE_LOG_TARGET:
            ytr = np.log1p(ytr_raw)
        else:
            ytr = ytr_raw

        model, _ = build_regressor()
        try:
            if model_kind == "lightgbm":
                model.fit(Xtr, ytr, sample_weight=wtr)
            else:
                model.fit(Xtr, ytr)
        except TypeError:
            model.fit(Xtr, ytr)

        yhat = model.predict(Xva)
        if USE_LOG_TARGET:
            yhat_raw = np.expm1(yhat)
        else:
            yhat_raw = yhat

        maes.append(mean_absolute_error(yva_raw, yhat_raw))
        r2s.append(r2_score(yva_raw, yhat_raw) if len(yva_raw) > 1 else np.nan)

    return {
        "cv_mae_mean": float(np.nanmean(maes)),
        "cv_mae_std": float(np.nanstd(maes)),
        "cv_r2_mean": float(np.nanmean(r2s)),
        "cv_r2_std": float(np.nanstd(r2s)),
        "folds": int(folds)
    }

# ==============================================================
# MAIN helpers to build dataset
# ==============================================================
def build_dataset_for_height(h_tag, include_height_feature=False):
    x_data_path, y_data_path = get_data_paths_for_height(h_tag)
    if not os.path.exists(x_data_path) or not os.path.exists(y_data_path):
        return None, None, None

    X_dict = np.load(x_data_path, allow_pickle=True).item()
    Y_dict = np.load(y_data_path, allow_pickle=True).item()
    common_keys = sorted(set(X_dict.keys()) & set(Y_dict.keys()))
    if not common_keys:
        return None, None, None

    X_list, y_list = [], []
    h_val = float(height_values[h_tag])

    for k in common_keys:
        x = X_dict[k].reshape(-1, 1).astype("float32")
        y = Y_dict[k].reshape(-1, 1).astype("float32")

        feats = extract_features_from_acc(x, DT)
        if include_height_feature:
            feats = np.concatenate([feats, np.asarray([h_val], dtype=np.float32)], axis=0)

        X_list.append(feats)
        y_list.append(y_max_abs_from_disp(y))

    X = np.vstack(X_list).astype(np.float32)
    y = np.asarray(y_list, dtype=np.float64).reshape(-1)

    return X, y, common_keys

# ==============================================================
# MAIN: MODE 1 (Multi-height)
# ==============================================================
if USE_MULTI_HEIGHT:
    X_all, y_all = [], []

    for h_tag in height_tags:
        print("\n" + "#" * 80)
        print(f"ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹: {h_tag}")
        print("#" * 80)

        Xh, yh, _ = build_dataset_for_height(h_tag, include_height_feature=True)
        if Xh is None:
            print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {h_tag} Ú©Ø§Ù…Ù„ Ù†Ø¨ÙˆØ¯ â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            continue

        X_all.append(Xh)
        y_all.append(yh)

    if not X_all:
        raise ValueError("âŒ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")

    X_all = np.vstack(X_all).astype(np.float32)
    y_all = np.concatenate(y_all).astype(np.float64).reshape(-1)

    y_clean, keep_mask, out_info = clean_and_clip_y(y_all)
    X_all = X_all[keep_mask]
    y_all = y_clean

    num_samples = len(y_all)
    print(f"\nğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ (Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ): {num_samples}")
    print(f"ğŸ§¾ Feature dim: {X_all.shape[1]} (includes height)")

    meta_dir = os.path.join(base_model_root, f"_META_{MODE_CODE}-{TRAIN_CODE}")
    idx, split_idx_path = get_or_make_split_indices(meta_dir, f"split_idx_seed{SEED}.npy", num_samples)

    # âœ… (2) Better split: Train=70%, Val=15%, Test=15%
    train_split = int(0.70 * num_samples)
    val_split   = int(0.85 * num_samples)

    train_ids = idx[:train_split]
    val_ids   = idx[train_split:val_split]
    test_ids  = idx[val_split:]

    X_train, y_train_raw = X_all[train_ids], y_all[train_ids]
    X_val,   y_val_raw   = X_all[val_ids],   y_all[val_ids]
    X_test,  y_test_raw  = X_all[test_ids],  y_all[test_ids]

    w_train = make_sample_weight(y_train_raw)
    X_train_os, y_train_os_raw, w_train_os = oversample_peaks(
        X_train, y_train_raw, w_train, q=PEAK_Q, multiplier=PEAK_MULTIPLIER
    )

    y_train = np.log1p(y_train_os_raw) if USE_LOG_TARGET else y_train_os_raw
    y_val   = np.log1p(y_val_raw)      if USE_LOG_TARGET else y_val_raw
    y_test  = np.log1p(y_test_raw)     if USE_LOG_TARGET else y_test_raw

    print(f"\nğŸ“¦ Global split | Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}")
    print(f"ğŸ“Œ After oversampling: Train = {X_train_os.shape[0]} samples (peaks q={PEAK_Q}, x{PEAK_MULTIPLIER})")

    model, model_kind = build_regressor()

    run_tag = f"{script_name}__{MODE_CODE}-{TRAIN_CODE}__MAXABS__{model_kind}"
    model_dir = os.path.join(base_model_root, run_tag)
    os.makedirs(model_dir, exist_ok=True)

    model_path = os.path.join(model_dir, "model.pkl")
    metrics_path = os.path.join(model_dir, "metrics.json")

    runinfo = {
        "status": "STARTED",
        "run_tag": run_tag,
        "script_name": script_name,
        "script_tag": script_tag,
        "seed": SEED,
        "dt": DT,
        "spectral_periods": SPECTRAL_PERIODS,
        "damping": DAMPING,
        "use_log_target": USE_LOG_TARGET,
        "use_peak_weighting": USE_PEAK_WEIGHTING,
        "W_C": W_C,
        "W_P": W_P,
        "use_peak_oversampling": USE_PEAK_OVERSAMPLING,
        "peak_q": PEAK_Q,
        "peak_multiplier": PEAK_MULTIPLIER,
        "winsorize": USE_WINSORIZE,
        "winsor_q_high": WINSOR_Q_HIGH,
        "iqr_filter": USE_IQR_FILTER,
        "iqr_k": IQR_K,
        "outlier_info": out_info,
        "use_clustered": USE_CLUSTERED,
        "cluster_label": cluster_label,
        "mode_code": MODE_CODE,
        "train_code": TRAIN_CODE,
        "gm_root_dir": gm_root_dir,
        "tha_root_dir": tha_root_dir,
        "split_idx_path": split_idx_path,
        "num_samples": int(num_samples),
        "feature_dim": int(X_all.shape[1]),
        "target": "y = max(abs(disp(t)))",
        "model_kind": model_kind
    }
    write_runinfo(model_dir, runinfo)

    cv_metrics = None
    if USE_CV_EVAL:
        X_tv = np.vstack([X_train, X_val])
        y_tv = np.concatenate([y_train_raw, y_val_raw])
        w_tv = make_sample_weight(y_tv)
        try:
            cv_metrics = kfold_cv_score(model_kind, X_tv, y_tv, sample_weight=w_tv, folds=CV_FOLDS)
            print("ğŸ“Œ CV metrics (train+val):", cv_metrics)
        except Exception as e:
            print("âš ï¸ CV failed:", repr(e))

    # âœ… (5) Fit with early stopping if LightGBM
    try:
        if model_kind == "lightgbm":
            import lightgbm as lgb
            model.fit(
                X_train_os, y_train,
                sample_weight=w_train_os,
                eval_set=[(X_val, y_val)],
                eval_metric="l1",
                callbacks=[
                    lgb.early_stopping(stopping_rounds=200),
                    lgb.log_evaluation(50)
                ]
            )
        else:
            model.fit(X_train_os, y_train)
    except TypeError:
        model.fit(X_train_os, y_train)

    y_pred_train = model.predict(X_train)
    y_pred_val   = model.predict(X_val)
    y_pred_test  = model.predict(X_test)

    if USE_LOG_TARGET:
        y_pred_train_raw = np.expm1(y_pred_train)
        y_pred_val_raw   = np.expm1(y_pred_val)
        y_pred_test_raw  = np.expm1(y_pred_test)
    else:
        y_pred_train_raw = y_pred_train
        y_pred_val_raw   = y_pred_val
        y_pred_test_raw  = y_pred_test

    metrics = {
        "train": compute_metrics(y_train_raw, y_pred_train_raw),
        "val":   compute_metrics(y_val_raw,   y_pred_val_raw),
        "test":  compute_metrics(y_test_raw,  y_pred_test_raw),
    }
    if cv_metrics is not None:
        metrics["cv_trainval"] = cv_metrics

    joblib.dump(model, model_path)
    save_metrics_json(metrics_path, metrics)

    runinfo["status"] = "FINISHED"
    runinfo["model_path"] = model_path
    runinfo["metrics_path"] = metrics_path
    runinfo["final_metrics_test"] = metrics["test"]
    if cv_metrics is not None:
        runinfo["cv_trainval"] = cv_metrics
    write_runinfo(model_dir, runinfo)

    print("\nâœ… Training finished:", run_tag)
    print("ğŸ“Œ Test metrics:", metrics["test"])

    plot_pred(
        y_test_raw, y_pred_test_raw,
        os.path.join(model_dir, "y_true_vs_pred.png"),
        title=f"{run_tag} | TEST",
        metrics_dict=metrics["test"]   # âœ… metrics bottom-right
    )
    plot_err_hist(
        (y_pred_test_raw - y_test_raw),
        os.path.join(model_dir, "error_hist.png"),
        title=f"{run_tag} | error histogram (TEST)"
    )

# ==============================================================
# MAIN: MODE 2 (Per-height)
# ==============================================================
else:
    for h_tag in height_tags:
        print("\n" + "#" * 80)
        print(f"ğŸ—ï¸ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªÙØ§Ø¹: {h_tag}")
        print("#" * 80)

        X_all, y_all, _ = build_dataset_for_height(h_tag, include_height_feature=False)
        if X_all is None:
            print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {h_tag} Ú©Ø§Ù…Ù„ Ù†Ø¨ÙˆØ¯ â†’ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            continue

        y_clean, keep_mask, out_info = clean_and_clip_y(y_all)
        X_all = X_all[keep_mask]
        y_all = y_clean

        num_samples = len(y_all)
        if num_samples == 0:
            print(f"âš ï¸ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {h_tag} Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯.")
            continue

        print(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {h_tag} (Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ): {num_samples}")
        print(f"ğŸ§¾ Feature dim: {X_all.shape[1]}")

        height_meta_dir = os.path.join(base_model_root, f"_META_{MODE_CODE}-{TRAIN_CODE}_{h_tag}")
        idx, split_idx_path = get_or_make_split_indices(height_meta_dir, f"split_idx_seed{SEED}.npy", num_samples)

        # âœ… (2) Better split: Train=70%, Val=15%, Test=15%
        train_split = int(0.70 * num_samples)
        val_split   = int(0.85 * num_samples)

        train_ids = idx[:train_split]
        val_ids   = idx[train_split:val_split]
        test_ids  = idx[val_split:]

        X_train, y_train_raw = X_all[train_ids], y_all[train_ids]
        X_val,   y_val_raw   = X_all[val_ids],   y_all[val_ids]
        X_test,  y_test_raw  = X_all[test_ids],  y_all[test_ids]

        w_train = make_sample_weight(y_train_raw)
        X_train_os, y_train_os_raw, w_train_os = oversample_peaks(
            X_train, y_train_raw, w_train, q=PEAK_Q, multiplier=PEAK_MULTIPLIER
        )

        y_train = np.log1p(y_train_os_raw) if USE_LOG_TARGET else y_train_os_raw
        y_val   = np.log1p(y_val_raw)      if USE_LOG_TARGET else y_val_raw
        y_test  = np.log1p(y_test_raw)     if USE_LOG_TARGET else y_test_raw

        model, model_kind = build_regressor()

        run_tag = f"{script_name}__{MODE_CODE}-{TRAIN_CODE}_{h_tag}__MAXABS__{model_kind}"
        model_dir = os.path.join(base_model_root, run_tag)
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, "model.pkl")
        metrics_path = os.path.join(model_dir, "metrics.json")

        runinfo = {
            "status": "STARTED",
            "run_tag": run_tag,
            "height": h_tag,
            "script_name": script_name,
            "script_tag": script_tag,
            "seed": SEED,
            "dt": DT,
            "spectral_periods": SPECTRAL_PERIODS,
            "damping": DAMPING,
            "use_log_target": USE_LOG_TARGET,
            "use_peak_weighting": USE_PEAK_WEIGHTING,
            "W_C": W_C,
            "W_P": W_P,
            "use_peak_oversampling": USE_PEAK_OVERSAMPLING,
            "peak_q": PEAK_Q,
            "peak_multiplier": PEAK_MULTIPLIER,
            "winsorize": USE_WINSORIZE,
            "winsor_q_high": WINSOR_Q_HIGH,
            "iqr_filter": USE_IQR_FILTER,
            "iqr_k": IQR_K,
            "outlier_info": out_info,
            "use_clustered": USE_CLUSTERED,
            "cluster_label": cluster_label,
            "mode_code": MODE_CODE,
            "train_code": TRAIN_CODE,
            "gm_root_dir": gm_root_dir,
            "tha_root_dir": tha_root_dir,
            "split_idx_path": split_idx_path,
            "num_samples": int(num_samples),
            "feature_dim": int(X_all.shape[1]),
            "target": "y = max(abs(disp(t)))",
            "model_kind": model_kind
        }
        write_runinfo(model_dir, runinfo)

        cv_metrics = None
        if USE_CV_EVAL:
            X_tv = np.vstack([X_train, X_val])
            y_tv = np.concatenate([y_train_raw, y_val_raw])
            w_tv = make_sample_weight(y_tv)
            try:
                cv_metrics = kfold_cv_score(model_kind, X_tv, y_tv, sample_weight=w_tv, folds=CV_FOLDS)
                print("ğŸ“Œ CV metrics (train+val):", cv_metrics)
            except Exception as e:
                print("âš ï¸ CV failed:", repr(e))

        # âœ… (5) Fit with early stopping if LightGBM
        try:
            if model_kind == "lightgbm":
                import lightgbm as lgb
                model.fit(
                    X_train_os, y_train,
                    sample_weight=w_train_os,
                    eval_set=[(X_val, y_val)],
                    eval_metric="l1",
                    callbacks=[
                        lgb.early_stopping(stopping_rounds=200),
                        lgb.log_evaluation(50)
                    ]
                )
            else:
                model.fit(X_train_os, y_train)
        except TypeError:
            model.fit(X_train_os, y_train)

        y_pred_train = model.predict(X_train)
        y_pred_val   = model.predict(X_val)
        y_pred_test  = model.predict(X_test)

        if USE_LOG_TARGET:
            y_pred_train_raw = np.expm1(y_pred_train)
            y_pred_val_raw   = np.expm1(y_pred_val)
            y_pred_test_raw  = np.expm1(y_pred_test)
        else:
            y_pred_train_raw = y_pred_train
            y_pred_val_raw   = y_pred_val
            y_pred_test_raw  = y_pred_test

        metrics = {
            "train": compute_metrics(y_train_raw, y_pred_train_raw),
            "val":   compute_metrics(y_val_raw,   y_pred_val_raw),
            "test":  compute_metrics(y_test_raw,  y_pred_test_raw),
        }
        if cv_metrics is not None:
            metrics["cv_trainval"] = cv_metrics

        joblib.dump(model, model_path)
        save_metrics_json(metrics_path, metrics)

        runinfo["status"] = "FINISHED"
        runinfo["model_path"] = model_path
        runinfo["metrics_path"] = metrics_path
        runinfo["final_metrics_test"] = metrics["test"]
        if cv_metrics is not None:
            runinfo["cv_trainval"] = cv_metrics
        write_runinfo(model_dir, runinfo)

        print("\nâœ… Training finished:", run_tag)
        print("ğŸ“Œ Test metrics:", metrics["test"])

        plot_pred(
            y_test_raw, y_pred_test_raw,
            os.path.join(model_dir, "y_true_vs_pred.png"),
            title=f"{run_tag} | TEST",
            metrics_dict=metrics["test"]   # âœ… metrics bottom-right
        )
        plot_err_hist(
            (y_pred_test_raw - y_test_raw),
            os.path.join(model_dir, "error_hist.png"),
            title=f"{run_tag} | error histogram (TEST)"
        )

    print("ğŸ‰ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù…Ù‡ Ø§Ø±ØªÙØ§Ø¹â€ŒÙ‡Ø§ Ø¯Ø± Ù…ÙˆØ¯ per-height ØªÙ…Ø§Ù… Ø´Ø¯.")



